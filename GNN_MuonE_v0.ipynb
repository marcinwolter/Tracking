{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "GNN_MuonE_v0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcinwolter/Tracking/blob/master/GNN_MuonE_v0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_vum9p0R7yZ",
        "colab_type": "text"
      },
      "source": [
        "# Tutorial: GNNs for Particle Tracking\n",
        "\n",
        "HEP.TrkX group\n",
        "\n",
        "Steve Farrell, Daniel Murname\n",
        "\n",
        "*Feb 2020*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZh8UpRMR7yp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "e342ed46-5ac5-4f1f-d451-23d97856df3f"
      },
      "source": [
        "# System imports\n",
        "import os\n",
        "import sys\n",
        "from pprint import pprint as pp\n",
        "from time import time as tt\n",
        "\n",
        "# External imports\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCH4u73YSkuf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7efdb980-dbb9-439f-9858-b2e3c7797afa"
      },
      "source": [
        "print(\"PyTorch version:\",print(torch.__version__),\", CUDA version:\", torch.version.cuda)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.1+cu101\n",
            "PyTorch version: None , CUDA version: 10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yAAs3ZiTNGX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "76d7fbfd-d63a-4e48-a082-2a81a004ef0b"
      },
      "source": [
        "!pip install torch-scatter==latest+cu101 torch-sparse==latest+cu101 -f https://s3.eu-central-1.amazonaws.com/pytorch-geometric.com/whl/torch-1.5.0.html\n",
        "!pip install torch-geometric"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://s3.eu-central-1.amazonaws.com/pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Collecting torch-scatter==latest+cu101\n",
            "  Using cached https://pytorch-geometric.com/whl/torch-1.5.0/torch_scatter-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl\n",
            "Collecting torch-sparse==latest+cu101\n",
            "  Using cached https://pytorch-geometric.com/whl/torch-1.5.0/torch_sparse-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-sparse==latest+cu101) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-sparse==latest+cu101) (1.18.5)\n",
            "Installing collected packages: torch-scatter, torch-sparse\n",
            "  Found existing installation: torch-scatter 2.0.5\n",
            "    Uninstalling torch-scatter-2.0.5:\n",
            "      Successfully uninstalled torch-scatter-2.0.5\n",
            "  Found existing installation: torch-sparse 0.6.6\n",
            "    Uninstalling torch-sparse-0.6.6:\n",
            "      Successfully uninstalled torch-sparse-0.6.6\n",
            "Successfully installed torch-scatter-2.0.5 torch-sparse-0.6.6\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
            "Requirement already satisfied: ase in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (3.19.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.48.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.0.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.10.0)\n",
            "Requirement already satisfied: plyfile in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.7.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.5.1+cu101)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.4)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (5.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (4.41.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from ase->torch-geometric) (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (0.15.1)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (47.3.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->torch-geometric) (1.12.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.16.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.4.7)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (0.6.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ase->torch-geometric) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ase->torch-geometric) (1.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YLfHA3uSjL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch_geometric.data import Data\n",
        "from torch_geometric.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from torch_scatter import scatter_add\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interact_manual\n",
        "from IPython.display import clear_output\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Get rid of RuntimeWarnings, gross\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYwHU7mHR7zR",
        "colab_type": "text"
      },
      "source": [
        "## The Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkwvGOWWR7zY",
        "colab_type": "text"
      },
      "source": [
        "### Toy Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf6W5iVoR7zc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Circle parameters\n",
        "num_layers = 10\n",
        "height, width = 10, 10\n",
        "min_curve, max_curve =  1000, 1001 # 15, 50\n",
        "noise = False\n",
        "event_size_min, event_size_max = 2,2 # 4, 12 #I.E. The number of true particle tracks\n",
        "max_angle=(4/6)*np.pi\n",
        "feature_scale = np.array([10,10])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c-vZg1kTk-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some dumb circle calculations\n",
        "def y1(x, r, a, sign):\n",
        "    return sign*np.sqrt(r**2 - a**2) + np.sqrt(r**2 - (x-a)**2)\n",
        "def y2(x, r, a, sign):\n",
        "    return sign*np.sqrt(r**2 - a**2) - np.sqrt(r**2 - (x-a)**2)\n",
        "\n",
        "# Generate random circle / helix parameters\n",
        "def rand_pars(event_size_min, event_size_max, max_curve, min_curve):\n",
        "    event_size = int(np.floor(np.random.random(1)*(event_size_max - event_size_min) + event_size_min))\n",
        "    radii = np.random.random(event_size)*(max_curve - min_curve) + min_curve\n",
        "    dirs = np.random.random(event_size)*(radii)*2 -radii\n",
        "    sign_options = np.array([-1,1])\n",
        "    signs = sign_options[np.rint(np.random.random(event_size)).astype(int)]\n",
        "    return radii, dirs, signs, event_size\n",
        "\n",
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))\n",
        "\n",
        "# Generate event data from random parameters\n",
        "def gen_edge_class(event_size_min, event_size_max, curve_min, curve_max, height, num_layers, max_angle, feature_scale, iter, num_samples):\n",
        "    \n",
        "    \"\"\" Feed params into randomiser \"\"\"\n",
        "    while True:\n",
        "        radii, dirs, signs, event_size = rand_pars(event_size_min, event_size_max, curve_max, curve_min)\n",
        "        xys = []\n",
        "        X = np.empty([3,1])\n",
        "        x = np.arange(0 + height/num_layers,height + height/num_layers, height/num_layers)\n",
        "        i = 0\n",
        "        for r, d, s in zip(radii, dirs, signs):\n",
        "            y1test = y1(x, r, d, s)\n",
        "            y2test = y2(x, r, d, s)\n",
        "            #print(y1test,y2test, x)\n",
        "            if -2.5 < y1test[0] < 2.5 and not any(np.isnan(y1test)):\n",
        "                X = np.append(X, np.vstack((y1test, np.array([i]*len(y1test)), x )), axis=1)\n",
        "                i += 1\n",
        "            if -2.5 < y2test[0] < 2.5 and not any(np.isnan(y2test)):\n",
        "                X = np.append(X, np.vstack((y2test, np.array([i]*len(y2test)), x )), axis=1)\n",
        "                i += 1\n",
        "        print(\" X \",X)        \n",
        "        X = X[:,1:].T\n",
        "        print(\"X.T \",X)\n",
        "        np.random.shuffle(X)\n",
        "\n",
        "        e = np.array([[i,j] for layer in np.arange(num_layers-1) for i in np.argwhere(X[:,2] == layer+1) for j in np.argwhere(X[:,2] == (layer+2)) if (X[i, 0] - np.tan(max_angle/2) < X[j, 0] < X[i, 0] + np.tan(max_angle/2))]).T.squeeze()\n",
        "        \n",
        "        # This handles when no edges were constructed. In that case, the randomisation is a do-over\n",
        "        try:\n",
        "            y = np.array([int(i[1] == j[1]) for i,j in zip(X[e[0]], X[e[1]])])    \n",
        "            break\n",
        "        except:\n",
        "            pass\n",
        "    if iter is not None and num_samples is not None:\n",
        "        out.update(progress(iter, num_samples))    \n",
        "    \n",
        "    X = np.array([X[:,2], X[:,0]]).T / feature_scale\n",
        "\n",
        "    data = Data(x = torch.from_numpy(X).float(), edge_index = torch.from_numpy(e), y = torch.from_numpy(y), pid = torch.from_numpy(X[:,1]))\n",
        "    return data\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35sTNGChGhQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_toy_graph(event):\n",
        "  \n",
        "    print(event)\n",
        "    print(event.x)\n",
        "    print(event.pid)\n",
        "    print(event.y)\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    x, y = event.x[:,0].numpy(), event.x[:,1].numpy()\n",
        "    e = event.edge_index.numpy()\n",
        "#     for i, j in zip(X[e[0]], X[e[1]]):\n",
        "#         plt.plot([i[0], j[0]], [i[1], j[1]], c='b')\n",
        "    #     print(i[0], i[2], j[0], j[2])\n",
        "    plt.plot([x[e[0,:]], x[e[1,:]]], [y[e[0,:]], y[e[1,:]]], c='b')\n",
        "    plt.scatter(x, y, c='k')\n",
        "    plt.ylim(-1,1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94DIt4J6R70i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2f59ab1f-0627-4f3c-a9ed-b14558ff748e"
      },
      "source": [
        "train_size, test_size = 3,3 #1000, 1000\n",
        "out = display(progress(0, train_size), display_id=True)\n",
        "train_dataset = [gen_edge_class(event_size_min, event_size_max, max_curve, min_curve, height, num_layers, max_angle, feature_scale, i, train_size) for i in range(train_size)]\n",
        "out = display(progress(0, test_size), display_id=True)\n",
        "test_dataset = [gen_edge_class(event_size_min, event_size_max, max_curve, min_curve, height, num_layers, max_angle, feature_scale, i, test_size) for i in range(test_size)]\n",
        "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='2'\n",
              "            max='3',\n",
              "            style='width: 100%'\n",
              "        >\n",
              "            2\n",
              "        </progress>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " X  [[ 9.88131292e-324  1.99407850e+000  3.99932865e+000  6.01590202e+000\n",
            "   8.04395380e+000  1.00836430e+001  1.21351323e+001  1.41985887e+001\n",
            "   1.62741832e+001  1.83620912e+001  2.04624923e+001 -1.42709707e+000\n",
            "  -2.84892520e+000 -4.26552330e+000 -5.67692975e+000 -7.08318240e+000\n",
            "  -8.48431863e+000 -9.88037529e+000 -1.12713888e+001 -1.26573950e+001\n",
            "  -1.40384294e+001]\n",
            " [ 4.94065646e-324  0.00000000e+000  0.00000000e+000  0.00000000e+000\n",
            "   0.00000000e+000  0.00000000e+000  0.00000000e+000  0.00000000e+000\n",
            "   0.00000000e+000  0.00000000e+000  0.00000000e+000  1.00000000e+000\n",
            "   1.00000000e+000  1.00000000e+000  1.00000000e+000  1.00000000e+000\n",
            "   1.00000000e+000  1.00000000e+000  1.00000000e+000  1.00000000e+000\n",
            "   1.00000000e+000]\n",
            " [ 0.00000000e+000  1.00000000e+000  2.00000000e+000  3.00000000e+000\n",
            "   4.00000000e+000  5.00000000e+000  6.00000000e+000  7.00000000e+000\n",
            "   8.00000000e+000  9.00000000e+000  1.00000000e+001  1.00000000e+000\n",
            "   2.00000000e+000  3.00000000e+000  4.00000000e+000  5.00000000e+000\n",
            "   6.00000000e+000  7.00000000e+000  8.00000000e+000  9.00000000e+000\n",
            "   1.00000000e+001]]\n",
            "X.T  [[  1.9940785    0.           1.        ]\n",
            " [  3.99932865   0.           2.        ]\n",
            " [  6.01590202   0.           3.        ]\n",
            " [  8.0439538    0.           4.        ]\n",
            " [ 10.08364295   0.           5.        ]\n",
            " [ 12.13513231   0.           6.        ]\n",
            " [ 14.19858873   0.           7.        ]\n",
            " [ 16.27418324   0.           8.        ]\n",
            " [ 18.36209117   0.           9.        ]\n",
            " [ 20.46249231   0.          10.        ]\n",
            " [ -1.42709707   1.           1.        ]\n",
            " [ -2.8489252    1.           2.        ]\n",
            " [ -4.2655233    1.           3.        ]\n",
            " [ -5.67692975   1.           4.        ]\n",
            " [ -7.0831824    1.           5.        ]\n",
            " [ -8.48431863   1.           6.        ]\n",
            " [ -9.88037529   1.           7.        ]\n",
            " [-11.27138878   1.           8.        ]\n",
            " [-12.65739498   1.           9.        ]\n",
            " [-14.03842936   1.          10.        ]]\n",
            " X  [[ 20.46249231  -1.54006216  -3.08634106  -4.63889004  -6.19776325\n",
            "   -7.7630157   -9.33470326 -10.91288269 -12.49761166 -14.08894877\n",
            "  -15.68695358  -0.6390145   -1.27970147  -1.92206475  -2.56610818\n",
            "   -3.21183561  -3.85925093  -4.50835806  -5.15916092  -5.81166347\n",
            "   -6.46586968]\n",
            " [  0.           0.           0.           0.           0.\n",
            "    0.           0.           0.           0.           0.\n",
            "    0.           1.           1.           1.           1.\n",
            "    1.           1.           1.           1.           1.\n",
            "    1.        ]\n",
            " [ 10.           1.           2.           3.           4.\n",
            "    5.           6.           7.           8.           9.\n",
            "   10.           1.           2.           3.           4.\n",
            "    5.           6.           7.           8.           9.\n",
            "   10.        ]]\n",
            "X.T  [[ -1.54006216   0.           1.        ]\n",
            " [ -3.08634106   0.           2.        ]\n",
            " [ -4.63889004   0.           3.        ]\n",
            " [ -6.19776325   0.           4.        ]\n",
            " [ -7.7630157    0.           5.        ]\n",
            " [ -9.33470326   0.           6.        ]\n",
            " [-10.91288269   0.           7.        ]\n",
            " [-12.49761166   0.           8.        ]\n",
            " [-14.08894877   0.           9.        ]\n",
            " [-15.68695358   0.          10.        ]\n",
            " [ -0.6390145    1.           1.        ]\n",
            " [ -1.27970147   1.           2.        ]\n",
            " [ -1.92206475   1.           3.        ]\n",
            " [ -2.56610818   1.           4.        ]\n",
            " [ -3.21183561   1.           5.        ]\n",
            " [ -3.85925093   1.           6.        ]\n",
            " [ -4.50835806   1.           7.        ]\n",
            " [ -5.15916092   1.           8.        ]\n",
            " [ -5.81166347   1.           9.        ]\n",
            " [ -6.46586968   1.          10.        ]]\n",
            " X  [[-2.56610818 -0.20915948 -0.41725357 -0.62428294 -0.83024828 -1.03515025\n",
            "  -1.23898952 -1.44176675 -1.64348261 -1.84413774 -2.04373279 -0.39241444\n",
            "  -0.78359004 -1.17352835 -1.56223093 -1.94969932 -2.33593506 -2.72093968\n",
            "  -3.1047147  -3.48726164 -3.868582  ]\n",
            " [ 1.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          1.\n",
            "   1.          1.          1.          1.          1.          1.\n",
            "   1.          1.          1.        ]\n",
            " [ 4.          1.          2.          3.          4.          5.\n",
            "   6.          7.          8.          9.         10.          1.\n",
            "   2.          3.          4.          5.          6.          7.\n",
            "   8.          9.         10.        ]]\n",
            "X.T  [[-0.20915948  0.          1.        ]\n",
            " [-0.41725357  0.          2.        ]\n",
            " [-0.62428294  0.          3.        ]\n",
            " [-0.83024828  0.          4.        ]\n",
            " [-1.03515025  0.          5.        ]\n",
            " [-1.23898952  0.          6.        ]\n",
            " [-1.44176675  0.          7.        ]\n",
            " [-1.64348261  0.          8.        ]\n",
            " [-1.84413774  0.          9.        ]\n",
            " [-2.04373279  0.         10.        ]\n",
            " [-0.39241444  1.          1.        ]\n",
            " [-0.78359004  1.          2.        ]\n",
            " [-1.17352835  1.          3.        ]\n",
            " [-1.56223093  1.          4.        ]\n",
            " [-1.94969932  1.          5.        ]\n",
            " [-2.33593506  1.          6.        ]\n",
            " [-2.72093968  1.          7.        ]\n",
            " [-3.1047147   1.          8.        ]\n",
            " [-3.48726164  1.          9.        ]\n",
            " [-3.868582    1.         10.        ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='2'\n",
              "            max='3',\n",
              "            style='width: 100%'\n",
              "        >\n",
              "            2\n",
              "        </progress>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " X  [[-0.41725357 -0.30364082 -0.60614163 -0.9075035  -1.20772751 -1.50681473\n",
            "  -1.80476623 -2.10158305 -2.39726625 -2.69181689 -2.98523599  0.22205622\n",
            "   0.445187    0.66939308  0.89467519  1.12103409  1.34847052  1.57698524\n",
            "   1.806579    2.03725257  2.26900671]\n",
            " [ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          1.\n",
            "   1.          1.          1.          1.          1.          1.\n",
            "   1.          1.          1.        ]\n",
            " [ 2.          1.          2.          3.          4.          5.\n",
            "   6.          7.          8.          9.         10.          1.\n",
            "   2.          3.          4.          5.          6.          7.\n",
            "   8.          9.         10.        ]]\n",
            "X.T  [[-0.30364082  0.          1.        ]\n",
            " [-0.60614163  0.          2.        ]\n",
            " [-0.9075035   0.          3.        ]\n",
            " [-1.20772751  0.          4.        ]\n",
            " [-1.50681473  0.          5.        ]\n",
            " [-1.80476623  0.          6.        ]\n",
            " [-2.10158305  0.          7.        ]\n",
            " [-2.39726625  0.          8.        ]\n",
            " [-2.69181689  0.          9.        ]\n",
            " [-2.98523599  0.         10.        ]\n",
            " [ 0.22205622  1.          1.        ]\n",
            " [ 0.445187    1.          2.        ]\n",
            " [ 0.66939308  1.          3.        ]\n",
            " [ 0.89467519  1.          4.        ]\n",
            " [ 1.12103409  1.          5.        ]\n",
            " [ 1.34847052  1.          6.        ]\n",
            " [ 1.57698524  1.          7.        ]\n",
            " [ 1.806579    1.          8.        ]\n",
            " [ 2.03725257  1.          9.        ]\n",
            " [ 2.26900671  1.         10.        ]]\n",
            " X  [[  1.34847052  -0.13554846  -0.27007021  -0.40356568  -0.53603527\n",
            "   -0.66747941  -0.79789849  -0.92729292  -1.0556631   -1.18300943\n",
            "   -1.3093323   -1.44626538  -2.88711818  -4.32259926  -5.75274892\n",
            "   -7.1776069   -8.59721242 -10.01160414 -11.42082024 -12.82489836\n",
            "  -14.22387565]\n",
            " [  1.           0.           0.           0.           0.\n",
            "    0.           0.           0.           0.           0.\n",
            "    0.           1.           1.           1.           1.\n",
            "    1.           1.           1.           1.           1.\n",
            "    1.        ]\n",
            " [  6.           1.           2.           3.           4.\n",
            "    5.           6.           7.           8.           9.\n",
            "   10.           1.           2.           3.           4.\n",
            "    5.           6.           7.           8.           9.\n",
            "   10.        ]]\n",
            "X.T  [[ -0.13554846   0.           1.        ]\n",
            " [ -0.27007021   0.           2.        ]\n",
            " [ -0.40356568   0.           3.        ]\n",
            " [ -0.53603527   0.           4.        ]\n",
            " [ -0.66747941   0.           5.        ]\n",
            " [ -0.79789849   0.           6.        ]\n",
            " [ -0.92729292   0.           7.        ]\n",
            " [ -1.0556631    0.           8.        ]\n",
            " [ -1.18300943   0.           9.        ]\n",
            " [ -1.3093323    0.          10.        ]\n",
            " [ -1.44626538   1.           1.        ]\n",
            " [ -2.88711818   1.           2.        ]\n",
            " [ -4.32259926   1.           3.        ]\n",
            " [ -5.75274892   1.           4.        ]\n",
            " [ -7.1776069    1.           5.        ]\n",
            " [ -8.59721242   1.           6.        ]\n",
            " [-10.01160414   1.           7.        ]\n",
            " [-11.42082024   1.           8.        ]\n",
            " [-12.82489836   1.           9.        ]\n",
            " [-14.22387565   1.          10.        ]]\n",
            " X  [[ -5.75274892   0.40871063   0.81616245   1.22235711   1.62729628\n",
            "    2.0309816    2.43341469   2.83459719   3.23453072   3.63321688\n",
            "    4.03065728  -2.41906602  -4.82037132  -7.20424553  -9.57100785\n",
            "  -11.92096753 -14.25442431 -16.5716688  -18.87298286 -21.15864002\n",
            "  -23.42890574]\n",
            " [  1.           0.           0.           0.           0.\n",
            "    0.           0.           0.           0.           0.\n",
            "    0.           1.           1.           1.           1.\n",
            "    1.           1.           1.           1.           1.\n",
            "    1.        ]\n",
            " [  4.           1.           2.           3.           4.\n",
            "    5.           6.           7.           8.           9.\n",
            "   10.           1.           2.           3.           4.\n",
            "    5.           6.           7.           8.           9.\n",
            "   10.        ]]\n",
            "X.T  [[  0.40871063   0.           1.        ]\n",
            " [  0.81616245   0.           2.        ]\n",
            " [  1.22235711   0.           3.        ]\n",
            " [  1.62729628   0.           4.        ]\n",
            " [  2.0309816    0.           5.        ]\n",
            " [  2.43341469   0.           6.        ]\n",
            " [  2.83459719   0.           7.        ]\n",
            " [  3.23453072   0.           8.        ]\n",
            " [  3.63321688   0.           9.        ]\n",
            " [  4.03065728   0.          10.        ]\n",
            " [ -2.41906602   1.           1.        ]\n",
            " [ -4.82037132   1.           2.        ]\n",
            " [ -7.20424553   1.           3.        ]\n",
            " [ -9.57100785   1.           4.        ]\n",
            " [-11.92096753   1.           5.        ]\n",
            " [-14.25442431   1.           6.        ]\n",
            " [-16.5716688    1.           7.        ]\n",
            " [-18.87298286   1.           8.        ]\n",
            " [-21.15864002   1.           9.        ]\n",
            " [-23.42890574   1.          10.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsH8JVJZR71N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "outputId": "28b61eca-c372-441b-8a36-e9d17915ec8d"
      },
      "source": [
        "plot_toy_graph(test_dataset[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data(edge_index=[2, 22], pid=[20], x=[20, 2], y=[22])\n",
            "tensor([[ 0.3000, -0.0908],\n",
            "        [ 0.6000,  0.1348],\n",
            "        [ 0.2000, -0.0606],\n",
            "        [ 0.9000,  0.2037],\n",
            "        [ 0.6000, -0.1805],\n",
            "        [ 0.9000, -0.2692],\n",
            "        [ 0.1000,  0.0222],\n",
            "        [ 0.8000, -0.2397],\n",
            "        [ 0.5000,  0.1121],\n",
            "        [ 0.5000, -0.1507],\n",
            "        [ 0.4000,  0.0895],\n",
            "        [ 0.7000,  0.1577],\n",
            "        [ 0.8000,  0.1807],\n",
            "        [ 0.1000, -0.0304],\n",
            "        [ 1.0000,  0.2269],\n",
            "        [ 0.3000,  0.0669],\n",
            "        [ 1.0000, -0.2985],\n",
            "        [ 0.7000, -0.2102],\n",
            "        [ 0.2000,  0.0445],\n",
            "        [ 0.4000, -0.1208]])\n",
            "tensor([-0.0908,  0.1348, -0.0606,  0.2037, -0.1805, -0.2692,  0.0222, -0.2397,\n",
            "         0.1121, -0.1507,  0.0895,  0.1577,  0.1807, -0.0304,  0.2269,  0.0669,\n",
            "        -0.2985, -0.2102,  0.0445, -0.1208], dtype=torch.float64)\n",
            "tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB6mrc_KR71-",
        "colab_type": "text"
      },
      "source": [
        "The complexity of the graph depends on the angle cut we put on it. Try increasing max_angle to, say, `(5/6)*np.pi` and the graph should be more busy. The limit is of course `(6/6)*np.pi = pi` where each node will look at the full angle of available possible nodes to form an edge with. While playing with this number, run the next cell to see the proportion of fake edges to true edges (fake/true) on the above graph. This value will be extremely useful later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WXQu9GXR72B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fdd55240-dda2-4ed4-f64b-5a4b453cd42f"
      },
      "source": [
        "print(\"Fake / True = \", (len(train_dataset[0].y) - train_dataset[0].y.sum().item()) / train_dataset[0].y.sum().item())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fake / True =  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eusssWsdR73x",
        "colab_type": "text"
      },
      "source": [
        "## A Simple GNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1FRm4nsR730",
        "colab_type": "text"
      },
      "source": [
        "### Message Passing GNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx0VuKz1R732",
        "colab_type": "text"
      },
      "source": [
        "We can write out the full GNN as a class `MPNN_Network`. One can see its behaviour as:\n",
        "1. Encode (x,y) features as hidden features with an N-layer MLP called `node_encoder`\n",
        "2. Concatenate these along each edge, and feed the concatenated features into another MLP called `edge_network`\n",
        "3. Sum the output of `edge_classifier` at each node (that is, each node receives the sum of the \"messages\" of all connecting edges). This sum is fed into `node_network`\n",
        "4. Add the hidden features to the previous iteration (this helps to preserve information between messages)\n",
        "5. Repeat (2) --> (4) n_graph_iters times\n",
        "6. After the message passing loop, pass the features of each edge through an output classifier network called `edge_classifier`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEPycV6mR734",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_mlp(input_size, sizes,\n",
        "             hidden_activation='ReLU',\n",
        "             output_activation='ReLU',\n",
        "             layer_norm=False):\n",
        "    \"\"\"Construct an MLP with specified fully-connected layers.\"\"\"\n",
        "    hidden_activation = getattr(nn, hidden_activation)\n",
        "    if output_activation is not None:\n",
        "        output_activation = getattr(nn, output_activation)\n",
        "    layers = []\n",
        "    n_layers = len(sizes)\n",
        "    sizes = [input_size] + sizes\n",
        "    # Hidden layers\n",
        "    for i in range(n_layers-1):\n",
        "        layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
        "        if layer_norm:\n",
        "            layers.append(nn.LayerNorm(sizes[i+1]))\n",
        "        layers.append(hidden_activation())\n",
        "    # Final layer\n",
        "    layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
        "    if output_activation is not None:\n",
        "        if layer_norm:\n",
        "            layers.append(nn.LayerNorm(sizes[-1]))\n",
        "        layers.append(output_activation())\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class MPNN_Network(nn.Module):\n",
        "    \"\"\"\n",
        "    A message-passing graph network which takes a graph with:\n",
        "    - bi-directional edges\n",
        "    - node features, no edge features\n",
        "\n",
        "    and applies the following modules:\n",
        "    - a graph encoder (no message passing)\n",
        "    - recurrent edge and node networks\n",
        "    - an edge classifier\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_node_dim, hidden_edge_dim, in_layers, node_layers, edge_layers,\n",
        "                 n_graph_iters=1, layer_norm=True):\n",
        "        super(MPNN_Network, self).__init__()\n",
        "        self.n_graph_iters = n_graph_iters\n",
        "\n",
        "        # The node encoder transforms input node features to the hidden space\n",
        "        self.node_encoder = make_mlp(input_dim, [hidden_node_dim]*in_layers)\n",
        "\n",
        "        # The edge network computes new edge features from connected nodes\n",
        "        self.edge_network = make_mlp(2*hidden_node_dim,\n",
        "                                     [hidden_edge_dim]*edge_layers,\n",
        "                                     layer_norm=layer_norm)\n",
        "\n",
        "        # The node network computes new node features\n",
        "        self.node_network = make_mlp(hidden_node_dim + hidden_edge_dim,\n",
        "                                     [hidden_node_dim]*node_layers,\n",
        "                                     layer_norm=layer_norm)\n",
        "\n",
        "        # The edge classifier computes final edge scores\n",
        "        self.edge_classifier = make_mlp(2*hidden_node_dim,\n",
        "                                        [hidden_edge_dim, 1],\n",
        "                                        output_activation=None)\n",
        "\n",
        "    def forward(self, data):\n",
        "        # Make every edge bi-directional\n",
        "        send_idx = torch.cat([data.edge_index[0], data.edge_index[1]], dim=0)\n",
        "        recv_idx = torch.cat([data.edge_index[1], data.edge_index[0]], dim=0)\n",
        "\n",
        "        # Encode the graph features into the hidden space\n",
        "        x = self.node_encoder(data.x)\n",
        "\n",
        "        # Loop over graph iterations\n",
        "        for i in range(self.n_graph_iters):\n",
        "\n",
        "            # Previous hidden state\n",
        "            x0 = x\n",
        "\n",
        "            # Compute new edge features\n",
        "            edge_inputs = torch.cat([x[send_idx], x[recv_idx]], dim=1)\n",
        "            e = self.edge_network(edge_inputs)\n",
        "\n",
        "            # Sum edge features coming into each node\n",
        "            aggr_messages = scatter_add(e, recv_idx, dim=0, dim_size=x.shape[0])\n",
        "\n",
        "            # Compute new node features\n",
        "            node_inputs = torch.cat([x, aggr_messages], dim=1)\n",
        "            x = self.node_network(node_inputs)\n",
        "\n",
        "            # Residual connection\n",
        "            x = x + x0\n",
        "\n",
        "        # Compute final edge scores; use original edge directions only\n",
        "        start_idx, end_idx = data.edge_index\n",
        "        clf_inputs = torch.cat([x[start_idx], x[end_idx]], dim=1)\n",
        "        return self.edge_classifier(clf_inputs).squeeze(-1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOsNvEEpR74O",
        "colab_type": "text"
      },
      "source": [
        "Build a version of the model and print it out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fPSN18uR74Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "outputId": "2c4f19bb-c2be-4b5a-9185-d20cff8c3864"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "m_configs = {\"input_dim\": 2, \"hidden_node_dim\": 64, \"hidden_edge_dim\": 64, \"in_layers\": 2, \"node_layers\": 4, \"edge_layers\": 4, \"n_graph_iters\": 1, \"layer_norm\": True}\n",
        "model = MPNN_Network(**m_configs).to(device)\n",
        "model"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MPNN_Network(\n",
              "  (node_encoder): Sequential(\n",
              "    (0): Linear(in_features=2, out_features=64, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (3): ReLU()\n",
              "  )\n",
              "  (edge_network): Sequential(\n",
              "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    (8): ReLU()\n",
              "    (9): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (10): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    (11): ReLU()\n",
              "  )\n",
              "  (node_network): Sequential(\n",
              "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    (8): ReLU()\n",
              "    (9): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (10): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    (11): ReLU()\n",
              "  )\n",
              "  (edge_classifier): Sequential(\n",
              "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSM30cK2R746",
        "colab_type": "text"
      },
      "source": [
        "### Training on Toy Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQrAmWD23j0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_loader, optimizer):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        data = batch.to(device)\n",
        "        pred = model(data)\n",
        "        loss = F.binary_cross_entropy_with_logits(pred.float(), data.y.float(), pos_weight=torch.tensor(weight))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        correct += ((pred > 0.5) == (data.y > 0.5)).sum().item()\n",
        "        total += len(pred)\n",
        "    acc = correct/total\n",
        "    return acc, total_loss\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0\n",
        "    for batch in test_loader:\n",
        "        data = batch.to(device)\n",
        "        pred = model(data)\n",
        "        loss = F.binary_cross_entropy_with_logits(pred.float(), data.y.float(), pos_weight=torch.tensor(weight))\n",
        "        total_loss += loss.item()\n",
        "        correct += ((pred > 0.5) == (data.y > 0.5)).sum().item()\n",
        "        total += len(pred)\n",
        "    acc = correct/total\n",
        "    return acc, total_loss"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzAsqpGuR75G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_loss_v = []\n",
        "t_acc_v = []\n",
        "v_loss_v = []\n",
        "v_acc_v = []\n",
        "ep = 0"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wbNjVo3R75N",
        "colab_type": "text"
      },
      "source": [
        "We set a weight value that is more or less the (fake / true) ratio found above. This forces the loss function to punish incorrectly classified true edges more severely. It rebalances the distribution as if there was a 1:1 true:fake ratio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBHqYJPaR75P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "accd4c72-d368-4ead-bc50-03c8aa553add"
      },
      "source": [
        "weight = 2\n",
        "\n",
        "m_configs = {\"input_dim\": 2, \"hidden_node_dim\": 16, \"hidden_edge_dim\": 16, \"in_layers\": 1, \"node_layers\": 2, \"edge_layers\": 2, \"n_graph_iters\": 6, \"layer_norm\": True}\n",
        "model = MPNN_Network(**m_configs).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3, amsgrad=True)\n",
        "\n",
        "for epoch in range(200):\n",
        "    ep += 1  \n",
        "    model.train()\n",
        "    acc, total_loss = train(model, train_loader, optimizer)\n",
        "    t_loss_v.append(total_loss)\n",
        "    t_acc_v.append(acc)\n",
        "\n",
        "    model.eval()\n",
        "    acc, total_loss = evaluate(model, test_loader)\n",
        "    v_loss_v.append(total_loss)\n",
        "    v_acc_v.append(acc)\n",
        "\n",
        "    print('Epoch: {}, Accuracy: {:.4f}'.format(ep, acc))\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Accuracy: 0.1000\n",
            "Epoch: 2, Accuracy: 0.1000\n",
            "Epoch: 3, Accuracy: 0.1000\n",
            "Epoch: 4, Accuracy: 0.1000\n",
            "Epoch: 5, Accuracy: 0.1000\n",
            "Epoch: 6, Accuracy: 0.1000\n",
            "Epoch: 7, Accuracy: 0.1000\n",
            "Epoch: 8, Accuracy: 0.1000\n",
            "Epoch: 9, Accuracy: 0.1000\n",
            "Epoch: 10, Accuracy: 0.1000\n",
            "Epoch: 11, Accuracy: 0.1000\n",
            "Epoch: 12, Accuracy: 0.1000\n",
            "Epoch: 13, Accuracy: 0.1000\n",
            "Epoch: 14, Accuracy: 0.1800\n",
            "Epoch: 15, Accuracy: 0.9000\n",
            "Epoch: 16, Accuracy: 0.9000\n",
            "Epoch: 17, Accuracy: 0.9000\n",
            "Epoch: 18, Accuracy: 0.9000\n",
            "Epoch: 19, Accuracy: 0.9000\n",
            "Epoch: 20, Accuracy: 0.9000\n",
            "Epoch: 21, Accuracy: 0.9000\n",
            "Epoch: 22, Accuracy: 0.9000\n",
            "Epoch: 23, Accuracy: 0.9000\n",
            "Epoch: 24, Accuracy: 0.9000\n",
            "Epoch: 25, Accuracy: 0.9000\n",
            "Epoch: 26, Accuracy: 0.9000\n",
            "Epoch: 27, Accuracy: 0.9000\n",
            "Epoch: 28, Accuracy: 0.9000\n",
            "Epoch: 29, Accuracy: 0.9000\n",
            "Epoch: 30, Accuracy: 0.9000\n",
            "Epoch: 31, Accuracy: 0.9000\n",
            "Epoch: 32, Accuracy: 0.9000\n",
            "Epoch: 33, Accuracy: 0.9000\n",
            "Epoch: 34, Accuracy: 0.9000\n",
            "Epoch: 35, Accuracy: 0.9000\n",
            "Epoch: 36, Accuracy: 0.9000\n",
            "Epoch: 37, Accuracy: 0.9000\n",
            "Epoch: 38, Accuracy: 0.9000\n",
            "Epoch: 39, Accuracy: 0.9000\n",
            "Epoch: 40, Accuracy: 0.9000\n",
            "Epoch: 41, Accuracy: 0.9000\n",
            "Epoch: 42, Accuracy: 0.9000\n",
            "Epoch: 43, Accuracy: 0.9000\n",
            "Epoch: 44, Accuracy: 0.9000\n",
            "Epoch: 45, Accuracy: 0.9000\n",
            "Epoch: 46, Accuracy: 0.9000\n",
            "Epoch: 47, Accuracy: 0.9000\n",
            "Epoch: 48, Accuracy: 0.9000\n",
            "Epoch: 49, Accuracy: 0.9000\n",
            "Epoch: 50, Accuracy: 0.9000\n",
            "Epoch: 51, Accuracy: 0.9000\n",
            "Epoch: 52, Accuracy: 0.9000\n",
            "Epoch: 53, Accuracy: 0.9000\n",
            "Epoch: 54, Accuracy: 0.9000\n",
            "Epoch: 55, Accuracy: 0.9000\n",
            "Epoch: 56, Accuracy: 0.9000\n",
            "Epoch: 57, Accuracy: 0.9000\n",
            "Epoch: 58, Accuracy: 0.9000\n",
            "Epoch: 59, Accuracy: 0.9000\n",
            "Epoch: 60, Accuracy: 0.9000\n",
            "Epoch: 61, Accuracy: 0.9000\n",
            "Epoch: 62, Accuracy: 0.9000\n",
            "Epoch: 63, Accuracy: 0.9000\n",
            "Epoch: 64, Accuracy: 0.9000\n",
            "Epoch: 65, Accuracy: 0.9000\n",
            "Epoch: 66, Accuracy: 0.9000\n",
            "Epoch: 67, Accuracy: 0.9000\n",
            "Epoch: 68, Accuracy: 0.9000\n",
            "Epoch: 69, Accuracy: 0.9000\n",
            "Epoch: 70, Accuracy: 0.9000\n",
            "Epoch: 71, Accuracy: 0.9000\n",
            "Epoch: 72, Accuracy: 0.9000\n",
            "Epoch: 73, Accuracy: 0.9000\n",
            "Epoch: 74, Accuracy: 0.9000\n",
            "Epoch: 75, Accuracy: 0.9000\n",
            "Epoch: 76, Accuracy: 0.9000\n",
            "Epoch: 77, Accuracy: 0.9000\n",
            "Epoch: 78, Accuracy: 0.9000\n",
            "Epoch: 79, Accuracy: 0.9000\n",
            "Epoch: 80, Accuracy: 0.9000\n",
            "Epoch: 81, Accuracy: 0.9000\n",
            "Epoch: 82, Accuracy: 0.9000\n",
            "Epoch: 83, Accuracy: 0.9000\n",
            "Epoch: 84, Accuracy: 0.9000\n",
            "Epoch: 85, Accuracy: 0.9000\n",
            "Epoch: 86, Accuracy: 0.9000\n",
            "Epoch: 87, Accuracy: 0.9000\n",
            "Epoch: 88, Accuracy: 0.9000\n",
            "Epoch: 89, Accuracy: 0.9000\n",
            "Epoch: 90, Accuracy: 0.9000\n",
            "Epoch: 91, Accuracy: 0.9000\n",
            "Epoch: 92, Accuracy: 0.9000\n",
            "Epoch: 93, Accuracy: 0.9000\n",
            "Epoch: 94, Accuracy: 0.9000\n",
            "Epoch: 95, Accuracy: 0.9000\n",
            "Epoch: 96, Accuracy: 0.9000\n",
            "Epoch: 97, Accuracy: 0.9000\n",
            "Epoch: 98, Accuracy: 0.9000\n",
            "Epoch: 99, Accuracy: 0.9000\n",
            "Epoch: 100, Accuracy: 0.9000\n",
            "Epoch: 101, Accuracy: 0.9000\n",
            "Epoch: 102, Accuracy: 0.9000\n",
            "Epoch: 103, Accuracy: 0.9000\n",
            "Epoch: 104, Accuracy: 0.9000\n",
            "Epoch: 105, Accuracy: 0.9000\n",
            "Epoch: 106, Accuracy: 0.9000\n",
            "Epoch: 107, Accuracy: 0.9000\n",
            "Epoch: 108, Accuracy: 0.9000\n",
            "Epoch: 109, Accuracy: 0.9000\n",
            "Epoch: 110, Accuracy: 0.9000\n",
            "Epoch: 111, Accuracy: 0.9000\n",
            "Epoch: 112, Accuracy: 0.9000\n",
            "Epoch: 113, Accuracy: 0.9000\n",
            "Epoch: 114, Accuracy: 0.9000\n",
            "Epoch: 115, Accuracy: 0.9000\n",
            "Epoch: 116, Accuracy: 0.9000\n",
            "Epoch: 117, Accuracy: 0.9000\n",
            "Epoch: 118, Accuracy: 0.9000\n",
            "Epoch: 119, Accuracy: 0.9000\n",
            "Epoch: 120, Accuracy: 0.9000\n",
            "Epoch: 121, Accuracy: 0.9000\n",
            "Epoch: 122, Accuracy: 0.9000\n",
            "Epoch: 123, Accuracy: 0.9000\n",
            "Epoch: 124, Accuracy: 0.9000\n",
            "Epoch: 125, Accuracy: 0.9000\n",
            "Epoch: 126, Accuracy: 0.9000\n",
            "Epoch: 127, Accuracy: 0.9000\n",
            "Epoch: 128, Accuracy: 0.9000\n",
            "Epoch: 129, Accuracy: 0.9000\n",
            "Epoch: 130, Accuracy: 0.9000\n",
            "Epoch: 131, Accuracy: 0.9000\n",
            "Epoch: 132, Accuracy: 0.9000\n",
            "Epoch: 133, Accuracy: 0.9000\n",
            "Epoch: 134, Accuracy: 0.9000\n",
            "Epoch: 135, Accuracy: 0.9000\n",
            "Epoch: 136, Accuracy: 0.9000\n",
            "Epoch: 137, Accuracy: 0.9000\n",
            "Epoch: 138, Accuracy: 0.9000\n",
            "Epoch: 139, Accuracy: 0.9000\n",
            "Epoch: 140, Accuracy: 0.9000\n",
            "Epoch: 141, Accuracy: 0.9000\n",
            "Epoch: 142, Accuracy: 0.9000\n",
            "Epoch: 143, Accuracy: 0.9000\n",
            "Epoch: 144, Accuracy: 0.9000\n",
            "Epoch: 145, Accuracy: 0.9000\n",
            "Epoch: 146, Accuracy: 0.9000\n",
            "Epoch: 147, Accuracy: 0.9000\n",
            "Epoch: 148, Accuracy: 0.9000\n",
            "Epoch: 149, Accuracy: 0.9000\n",
            "Epoch: 150, Accuracy: 0.9000\n",
            "Epoch: 151, Accuracy: 0.9000\n",
            "Epoch: 152, Accuracy: 0.9000\n",
            "Epoch: 153, Accuracy: 0.9000\n",
            "Epoch: 154, Accuracy: 0.9000\n",
            "Epoch: 155, Accuracy: 0.9000\n",
            "Epoch: 156, Accuracy: 0.9000\n",
            "Epoch: 157, Accuracy: 0.9000\n",
            "Epoch: 158, Accuracy: 0.9000\n",
            "Epoch: 159, Accuracy: 0.9000\n",
            "Epoch: 160, Accuracy: 0.9000\n",
            "Epoch: 161, Accuracy: 0.9000\n",
            "Epoch: 162, Accuracy: 0.9000\n",
            "Epoch: 163, Accuracy: 0.9000\n",
            "Epoch: 164, Accuracy: 0.9000\n",
            "Epoch: 165, Accuracy: 0.9000\n",
            "Epoch: 166, Accuracy: 0.9000\n",
            "Epoch: 167, Accuracy: 0.9000\n",
            "Epoch: 168, Accuracy: 0.9000\n",
            "Epoch: 169, Accuracy: 0.9000\n",
            "Epoch: 170, Accuracy: 0.9000\n",
            "Epoch: 171, Accuracy: 0.9000\n",
            "Epoch: 172, Accuracy: 0.9000\n",
            "Epoch: 173, Accuracy: 0.9000\n",
            "Epoch: 174, Accuracy: 0.9000\n",
            "Epoch: 175, Accuracy: 0.9000\n",
            "Epoch: 176, Accuracy: 0.9000\n",
            "Epoch: 177, Accuracy: 0.9000\n",
            "Epoch: 178, Accuracy: 0.9000\n",
            "Epoch: 179, Accuracy: 0.9000\n",
            "Epoch: 180, Accuracy: 0.9000\n",
            "Epoch: 181, Accuracy: 0.9000\n",
            "Epoch: 182, Accuracy: 0.9000\n",
            "Epoch: 183, Accuracy: 0.9000\n",
            "Epoch: 184, Accuracy: 0.9200\n",
            "Epoch: 185, Accuracy: 0.9200\n",
            "Epoch: 186, Accuracy: 0.9200\n",
            "Epoch: 187, Accuracy: 0.9200\n",
            "Epoch: 188, Accuracy: 0.9200\n",
            "Epoch: 189, Accuracy: 0.9000\n",
            "Epoch: 190, Accuracy: 0.9000\n",
            "Epoch: 191, Accuracy: 0.9000\n",
            "Epoch: 192, Accuracy: 0.9000\n",
            "Epoch: 193, Accuracy: 0.9000\n",
            "Epoch: 194, Accuracy: 0.9000\n",
            "Epoch: 195, Accuracy: 0.9000\n",
            "Epoch: 196, Accuracy: 0.9000\n",
            "Epoch: 197, Accuracy: 0.9000\n",
            "Epoch: 198, Accuracy: 0.9000\n",
            "Epoch: 199, Accuracy: 0.9000\n",
            "Epoch: 200, Accuracy: 0.9200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEjmOmtuR75e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "9590715b-61a2-4e8e-cb85-8c610819adb1"
      },
      "source": [
        "fig, axs = plt.subplots(1, 2, constrained_layout=True, figsize=(10, 5))\n",
        "axs[0].plot(np.arange(len(t_loss_v)), t_loss_v, np.arange(len(t_acc_v)), t_acc_v)\n",
        "axs[0].set_title(\"Training loss and accuracy\")\n",
        "axs[0].set_yscale(\"log\")\n",
        "axs[1].plot(np.arange(len(v_loss_v)), v_loss_v, np.arange(len(v_acc_v)), v_acc_v)\n",
        "axs[1].set_title(\"Validation loss and accuracy\")\n",
        "axs[1].set_yscale(\"log\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEDdFqVnR75x",
        "colab_type": "text"
      },
      "source": [
        "### Did it work?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fTa4sHsR75z",
        "colab_type": "text"
      },
      "source": [
        "Running the above with 1 graph iteration gives me about 90% accuracy in 200 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT5s0J70R751",
        "colab_type": "text"
      },
      "source": [
        "The best performance that I can get with some simple manual tuning is around 95% accuracy. We can improve the efficiency at the cost of purity by raising the weight on true edges, but the accuracy won't significantly improve. In general, the biggest changes were from increasing the width (i.e. the number of dimensions) of the hidden layers. We can visualise the performance on a particular graph, colouring true positives black, false positives red, true negatives a transparent black, and false negatives in blue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6CR0cLcR752",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_toy_classified(event, preds, cut=0.5):\n",
        "    \n",
        "      \n",
        "    print(event)\n",
        "    print(event.x)\n",
        "    print(event.pid)\n",
        "    print(event.y)\n",
        "    print(preds)\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    x, y = event.x[:,0].numpy(), event.x[:,1].numpy()\n",
        "    edges = event.edge_index.numpy()\n",
        "    labels = event.y\n",
        "    plt.scatter(x, y, c='k')\n",
        "    \n",
        "    preds = preds.detach().numpy()\n",
        "    \n",
        "    for j in range(len(labels)):\n",
        "        \n",
        "        # False negatives\n",
        "        if preds[j] < cut and labels[j].item() > cut:\n",
        "            plt.plot([x[edges[0,j]], x[edges[1,j]]],\n",
        "                     [y[edges[0,j]], y[edges[1,j]]],\n",
        "                     '--', c='b')\n",
        "\n",
        "        # False positives\n",
        "        if preds[j] > cut and labels[j].item() < cut:\n",
        "            plt.plot([x[edges[0,j]], x[edges[1,j]]],\n",
        "                     [y[edges[0,j]], y[edges[1,j]]],\n",
        "                     '-', c='r', alpha=preds[j])\n",
        "\n",
        "        # True positives\n",
        "        if preds[j] > cut and labels[j].item() > cut:\n",
        "            plt.plot([x[edges[0,j]], x[edges[1,j]]],\n",
        "                     [y[edges[0,j]], y[edges[1,j]]],\n",
        "                     '-', c='k', alpha=preds[j])\n",
        "                \n",
        "        # True negatives\n",
        "        if preds[j] < cut and labels[j].item() < cut:\n",
        "            plt.plot([x[edges[0,j]], x[edges[1,j]]],\n",
        "                     [y[edges[0,j]], y[edges[1,j]]],\n",
        "                     '-', c='k', alpha=preds[j])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIv5Ka7RR76B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        },
        "outputId": "9191994b-7659-4cf4-80e4-1266a04ad341"
      },
      "source": [
        "data = test_loader.dataset[0].to(device)\n",
        "preds = torch.sigmoid(model(data)).to('cpu')\n",
        "plot_toy_classified(data.to('cpu'), preds, cut = 0.6)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data(edge_index=[2, 22], pid=[20], x=[20, 2], y=[22])\n",
            "tensor([[ 0.3000, -0.0908],\n",
            "        [ 0.6000,  0.1348],\n",
            "        [ 0.2000, -0.0606],\n",
            "        [ 0.9000,  0.2037],\n",
            "        [ 0.6000, -0.1805],\n",
            "        [ 0.9000, -0.2692],\n",
            "        [ 0.1000,  0.0222],\n",
            "        [ 0.8000, -0.2397],\n",
            "        [ 0.5000,  0.1121],\n",
            "        [ 0.5000, -0.1507],\n",
            "        [ 0.4000,  0.0895],\n",
            "        [ 0.7000,  0.1577],\n",
            "        [ 0.8000,  0.1807],\n",
            "        [ 0.1000, -0.0304],\n",
            "        [ 1.0000,  0.2269],\n",
            "        [ 0.3000,  0.0669],\n",
            "        [ 1.0000, -0.2985],\n",
            "        [ 0.7000, -0.2102],\n",
            "        [ 0.2000,  0.0445],\n",
            "        [ 0.4000, -0.1208]])\n",
            "tensor([-0.0908,  0.1348, -0.0606,  0.2037, -0.1805, -0.2692,  0.0222, -0.2397,\n",
            "         0.1121, -0.1507,  0.0895,  0.1577,  0.1807, -0.0304,  0.2269,  0.0669,\n",
            "        -0.2985, -0.2102,  0.0445, -0.1208], dtype=torch.float64)\n",
            "tensor([0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "tensor([0.6518, 0.6491, 0.6516, 0.6489, 0.6770, 0.6686, 0.6772, 0.6688, 0.7304,\n",
            "        0.6958, 0.6756, 0.7354, 0.6720, 0.8181, 0.6704, 0.9247, 0.6730, 0.9754,\n",
            "        0.9973, 0.6756, 0.9740, 0.9997], grad_fn=<CopyBackwards>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIo7EeyJR76L",
        "colab_type": "text"
      },
      "source": [
        "So we can see that it's working quite well. Few missed true edges, and few misclassified fake edges. The ratio of false positives to false negatives (which can be defined with efficiency and purity) is controlled by the cut we put on the prediction score. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CBDsYlxR76N",
        "colab_type": "text"
      },
      "source": [
        "### The effect of Message Passing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSZzzAtDR76P",
        "colab_type": "text"
      },
      "source": [
        "In this simple example, the message passing does not do a huge amount. Going from 1 iteration to 6 iterations improves the accuracy to around 93% (from 90%). This improvement is washed out with more hidden dimensions, as one can see from the below set of tests. Try playing with more hidden node and edge dimensions (e.g. 64) and see if the message passing iterations can improve the performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9vg_JclR76S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b1a843e5-153d-417f-dcee-a89d89524105"
      },
      "source": [
        "t_loss_v = []\n",
        "t_acc_v = []\n",
        "v_loss_v = []\n",
        "v_acc_v = []\n",
        "ep = 0\n",
        "best_acc = np.zeros(10)\n",
        "for i in range(1,11):\n",
        "    m_configs = {\"input_dim\": 2, \"hidden_node_dim\": 16, \"hidden_edge_dim\": 16, \"in_layers\": 1, \"node_layers\": 2, \"edge_layers\": 2, \"n_graph_iters\": i, \"layer_norm\": True}\n",
        "    model = MPNN_Network(**m_configs).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3, amsgrad=True)\n",
        "    for epoch in range(200):\n",
        "        ep += 1  \n",
        "        model.train()\n",
        "        acc, total_loss = train(model, train_loader, optimizer)\n",
        "        t_loss_v.append(total_loss)\n",
        "        t_acc_v.append(acc)\n",
        "\n",
        "        model.eval()\n",
        "        acc, total_loss = evaluate(model, test_loader)\n",
        "        if acc > best_acc[i-1]: best_acc[i-1] = acc\n",
        "        v_loss_v.append(total_loss)\n",
        "        v_acc_v.append(acc)\n",
        "\n",
        "        print('Epoch: {}, Accuracy: {:.4f}'.format(ep, acc))\n",
        "plt.plot(np.arange(len(best_acc)), best_acc)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Accuracy: 0.1000\n",
            "Epoch: 2, Accuracy: 0.1000\n",
            "Epoch: 3, Accuracy: 0.1000\n",
            "Epoch: 4, Accuracy: 0.1000\n",
            "Epoch: 5, Accuracy: 0.1000\n",
            "Epoch: 6, Accuracy: 0.1000\n",
            "Epoch: 7, Accuracy: 0.2600\n",
            "Epoch: 8, Accuracy: 0.5800\n",
            "Epoch: 9, Accuracy: 0.8600\n",
            "Epoch: 10, Accuracy: 0.8800\n",
            "Epoch: 11, Accuracy: 0.9000\n",
            "Epoch: 12, Accuracy: 0.9000\n",
            "Epoch: 13, Accuracy: 0.9000\n",
            "Epoch: 14, Accuracy: 0.9000\n",
            "Epoch: 15, Accuracy: 0.9000\n",
            "Epoch: 16, Accuracy: 0.9000\n",
            "Epoch: 17, Accuracy: 0.9000\n",
            "Epoch: 18, Accuracy: 0.9000\n",
            "Epoch: 19, Accuracy: 0.9000\n",
            "Epoch: 20, Accuracy: 0.9000\n",
            "Epoch: 21, Accuracy: 0.9000\n",
            "Epoch: 22, Accuracy: 0.9000\n",
            "Epoch: 23, Accuracy: 0.9000\n",
            "Epoch: 24, Accuracy: 0.9000\n",
            "Epoch: 25, Accuracy: 0.9000\n",
            "Epoch: 26, Accuracy: 0.9000\n",
            "Epoch: 27, Accuracy: 0.9000\n",
            "Epoch: 28, Accuracy: 0.9000\n",
            "Epoch: 29, Accuracy: 0.9000\n",
            "Epoch: 30, Accuracy: 0.9000\n",
            "Epoch: 31, Accuracy: 0.9000\n",
            "Epoch: 32, Accuracy: 0.9000\n",
            "Epoch: 33, Accuracy: 0.9000\n",
            "Epoch: 34, Accuracy: 0.9000\n",
            "Epoch: 35, Accuracy: 0.9000\n",
            "Epoch: 36, Accuracy: 0.9000\n",
            "Epoch: 37, Accuracy: 0.9000\n",
            "Epoch: 38, Accuracy: 0.9000\n",
            "Epoch: 39, Accuracy: 0.9000\n",
            "Epoch: 40, Accuracy: 0.9000\n",
            "Epoch: 41, Accuracy: 0.9000\n",
            "Epoch: 42, Accuracy: 0.9000\n",
            "Epoch: 43, Accuracy: 0.9000\n",
            "Epoch: 44, Accuracy: 0.9000\n",
            "Epoch: 45, Accuracy: 0.9000\n",
            "Epoch: 46, Accuracy: 0.9000\n",
            "Epoch: 47, Accuracy: 0.9000\n",
            "Epoch: 48, Accuracy: 0.9000\n",
            "Epoch: 49, Accuracy: 0.9000\n",
            "Epoch: 50, Accuracy: 0.9000\n",
            "Epoch: 51, Accuracy: 0.9000\n",
            "Epoch: 52, Accuracy: 0.9000\n",
            "Epoch: 53, Accuracy: 0.9000\n",
            "Epoch: 54, Accuracy: 0.9000\n",
            "Epoch: 55, Accuracy: 0.9000\n",
            "Epoch: 56, Accuracy: 0.9000\n",
            "Epoch: 57, Accuracy: 0.9000\n",
            "Epoch: 58, Accuracy: 0.9000\n",
            "Epoch: 59, Accuracy: 0.9000\n",
            "Epoch: 60, Accuracy: 0.9000\n",
            "Epoch: 61, Accuracy: 0.9000\n",
            "Epoch: 62, Accuracy: 0.9000\n",
            "Epoch: 63, Accuracy: 0.9000\n",
            "Epoch: 64, Accuracy: 0.9000\n",
            "Epoch: 65, Accuracy: 0.9000\n",
            "Epoch: 66, Accuracy: 0.9000\n",
            "Epoch: 67, Accuracy: 0.9000\n",
            "Epoch: 68, Accuracy: 0.9000\n",
            "Epoch: 69, Accuracy: 0.9000\n",
            "Epoch: 70, Accuracy: 0.9000\n",
            "Epoch: 71, Accuracy: 0.9000\n",
            "Epoch: 72, Accuracy: 0.9000\n",
            "Epoch: 73, Accuracy: 0.9000\n",
            "Epoch: 74, Accuracy: 0.9000\n",
            "Epoch: 75, Accuracy: 0.9000\n",
            "Epoch: 76, Accuracy: 0.9000\n",
            "Epoch: 77, Accuracy: 0.9000\n",
            "Epoch: 78, Accuracy: 0.9000\n",
            "Epoch: 79, Accuracy: 0.9000\n",
            "Epoch: 80, Accuracy: 0.9000\n",
            "Epoch: 81, Accuracy: 0.9000\n",
            "Epoch: 82, Accuracy: 0.9000\n",
            "Epoch: 83, Accuracy: 0.9000\n",
            "Epoch: 84, Accuracy: 0.9000\n",
            "Epoch: 85, Accuracy: 0.9000\n",
            "Epoch: 86, Accuracy: 0.9000\n",
            "Epoch: 87, Accuracy: 0.9000\n",
            "Epoch: 88, Accuracy: 0.9000\n",
            "Epoch: 89, Accuracy: 0.9000\n",
            "Epoch: 90, Accuracy: 0.9000\n",
            "Epoch: 91, Accuracy: 0.9000\n",
            "Epoch: 92, Accuracy: 0.9000\n",
            "Epoch: 93, Accuracy: 0.9000\n",
            "Epoch: 94, Accuracy: 0.9000\n",
            "Epoch: 95, Accuracy: 0.9000\n",
            "Epoch: 96, Accuracy: 0.9000\n",
            "Epoch: 97, Accuracy: 0.9000\n",
            "Epoch: 98, Accuracy: 0.9000\n",
            "Epoch: 99, Accuracy: 0.9000\n",
            "Epoch: 100, Accuracy: 0.9000\n",
            "Epoch: 101, Accuracy: 0.9000\n",
            "Epoch: 102, Accuracy: 0.9000\n",
            "Epoch: 103, Accuracy: 0.9000\n",
            "Epoch: 104, Accuracy: 0.8800\n",
            "Epoch: 105, Accuracy: 0.8600\n",
            "Epoch: 106, Accuracy: 0.8200\n",
            "Epoch: 107, Accuracy: 0.8000\n",
            "Epoch: 108, Accuracy: 0.7800\n",
            "Epoch: 109, Accuracy: 0.7600\n",
            "Epoch: 110, Accuracy: 0.7400\n",
            "Epoch: 111, Accuracy: 0.7200\n",
            "Epoch: 112, Accuracy: 0.7000\n",
            "Epoch: 113, Accuracy: 0.7200\n",
            "Epoch: 114, Accuracy: 0.7200\n",
            "Epoch: 115, Accuracy: 0.6800\n",
            "Epoch: 116, Accuracy: 0.6800\n",
            "Epoch: 117, Accuracy: 0.7000\n",
            "Epoch: 118, Accuracy: 0.7000\n",
            "Epoch: 119, Accuracy: 0.7000\n",
            "Epoch: 120, Accuracy: 0.7000\n",
            "Epoch: 121, Accuracy: 0.7000\n",
            "Epoch: 122, Accuracy: 0.7200\n",
            "Epoch: 123, Accuracy: 0.7200\n",
            "Epoch: 124, Accuracy: 0.7000\n",
            "Epoch: 125, Accuracy: 0.7000\n",
            "Epoch: 126, Accuracy: 0.6800\n",
            "Epoch: 127, Accuracy: 0.6600\n",
            "Epoch: 128, Accuracy: 0.6600\n",
            "Epoch: 129, Accuracy: 0.6400\n",
            "Epoch: 130, Accuracy: 0.6400\n",
            "Epoch: 131, Accuracy: 0.6400\n",
            "Epoch: 132, Accuracy: 0.6400\n",
            "Epoch: 133, Accuracy: 0.6400\n",
            "Epoch: 134, Accuracy: 0.6400\n",
            "Epoch: 135, Accuracy: 0.6400\n",
            "Epoch: 136, Accuracy: 0.6400\n",
            "Epoch: 137, Accuracy: 0.6400\n",
            "Epoch: 138, Accuracy: 0.6400\n",
            "Epoch: 139, Accuracy: 0.6600\n",
            "Epoch: 140, Accuracy: 0.6600\n",
            "Epoch: 141, Accuracy: 0.6600\n",
            "Epoch: 142, Accuracy: 0.6600\n",
            "Epoch: 143, Accuracy: 0.6600\n",
            "Epoch: 144, Accuracy: 0.6600\n",
            "Epoch: 145, Accuracy: 0.6600\n",
            "Epoch: 146, Accuracy: 0.6600\n",
            "Epoch: 147, Accuracy: 0.6600\n",
            "Epoch: 148, Accuracy: 0.6600\n",
            "Epoch: 149, Accuracy: 0.6600\n",
            "Epoch: 150, Accuracy: 0.6600\n",
            "Epoch: 151, Accuracy: 0.6400\n",
            "Epoch: 152, Accuracy: 0.6600\n",
            "Epoch: 153, Accuracy: 0.6600\n",
            "Epoch: 154, Accuracy: 0.6600\n",
            "Epoch: 155, Accuracy: 0.6600\n",
            "Epoch: 156, Accuracy: 0.6600\n",
            "Epoch: 157, Accuracy: 0.6600\n",
            "Epoch: 158, Accuracy: 0.6600\n",
            "Epoch: 159, Accuracy: 0.6600\n",
            "Epoch: 160, Accuracy: 0.6800\n",
            "Epoch: 161, Accuracy: 0.6800\n",
            "Epoch: 162, Accuracy: 0.6600\n",
            "Epoch: 163, Accuracy: 0.6400\n",
            "Epoch: 164, Accuracy: 0.6400\n",
            "Epoch: 165, Accuracy: 0.6600\n",
            "Epoch: 166, Accuracy: 0.6600\n",
            "Epoch: 167, Accuracy: 0.6600\n",
            "Epoch: 168, Accuracy: 0.6600\n",
            "Epoch: 169, Accuracy: 0.6800\n",
            "Epoch: 170, Accuracy: 0.6600\n",
            "Epoch: 171, Accuracy: 0.6600\n",
            "Epoch: 172, Accuracy: 0.6800\n",
            "Epoch: 173, Accuracy: 0.6800\n",
            "Epoch: 174, Accuracy: 0.6800\n",
            "Epoch: 175, Accuracy: 0.6800\n",
            "Epoch: 176, Accuracy: 0.6800\n",
            "Epoch: 177, Accuracy: 0.6800\n",
            "Epoch: 178, Accuracy: 0.6800\n",
            "Epoch: 179, Accuracy: 0.6800\n",
            "Epoch: 180, Accuracy: 0.6800\n",
            "Epoch: 181, Accuracy: 0.6800\n",
            "Epoch: 182, Accuracy: 0.7000\n",
            "Epoch: 183, Accuracy: 0.7200\n",
            "Epoch: 184, Accuracy: 0.7000\n",
            "Epoch: 185, Accuracy: 0.7000\n",
            "Epoch: 186, Accuracy: 0.7400\n",
            "Epoch: 187, Accuracy: 0.7800\n",
            "Epoch: 188, Accuracy: 0.7800\n",
            "Epoch: 189, Accuracy: 0.7400\n",
            "Epoch: 190, Accuracy: 0.7600\n",
            "Epoch: 191, Accuracy: 0.8200\n",
            "Epoch: 192, Accuracy: 0.8000\n",
            "Epoch: 193, Accuracy: 0.8000\n",
            "Epoch: 194, Accuracy: 0.8200\n",
            "Epoch: 195, Accuracy: 0.8800\n",
            "Epoch: 196, Accuracy: 0.8800\n",
            "Epoch: 197, Accuracy: 0.8400\n",
            "Epoch: 198, Accuracy: 0.8800\n",
            "Epoch: 199, Accuracy: 0.8800\n",
            "Epoch: 200, Accuracy: 0.8800\n",
            "Epoch: 201, Accuracy: 0.1000\n",
            "Epoch: 202, Accuracy: 0.1000\n",
            "Epoch: 203, Accuracy: 0.1000\n",
            "Epoch: 204, Accuracy: 0.1600\n",
            "Epoch: 205, Accuracy: 0.2600\n",
            "Epoch: 206, Accuracy: 0.8200\n",
            "Epoch: 207, Accuracy: 0.9000\n",
            "Epoch: 208, Accuracy: 0.9000\n",
            "Epoch: 209, Accuracy: 0.9000\n",
            "Epoch: 210, Accuracy: 0.9000\n",
            "Epoch: 211, Accuracy: 0.9000\n",
            "Epoch: 212, Accuracy: 0.9000\n",
            "Epoch: 213, Accuracy: 0.9000\n",
            "Epoch: 214, Accuracy: 0.9000\n",
            "Epoch: 215, Accuracy: 0.9000\n",
            "Epoch: 216, Accuracy: 0.9000\n",
            "Epoch: 217, Accuracy: 0.9000\n",
            "Epoch: 218, Accuracy: 0.9000\n",
            "Epoch: 219, Accuracy: 0.9000\n",
            "Epoch: 220, Accuracy: 0.9000\n",
            "Epoch: 221, Accuracy: 0.9000\n",
            "Epoch: 222, Accuracy: 0.9000\n",
            "Epoch: 223, Accuracy: 0.9000\n",
            "Epoch: 224, Accuracy: 0.9000\n",
            "Epoch: 225, Accuracy: 0.9000\n",
            "Epoch: 226, Accuracy: 0.9000\n",
            "Epoch: 227, Accuracy: 0.9000\n",
            "Epoch: 228, Accuracy: 0.9000\n",
            "Epoch: 229, Accuracy: 0.9000\n",
            "Epoch: 230, Accuracy: 0.9000\n",
            "Epoch: 231, Accuracy: 0.9000\n",
            "Epoch: 232, Accuracy: 0.9000\n",
            "Epoch: 233, Accuracy: 0.9000\n",
            "Epoch: 234, Accuracy: 0.9000\n",
            "Epoch: 235, Accuracy: 0.9000\n",
            "Epoch: 236, Accuracy: 0.9000\n",
            "Epoch: 237, Accuracy: 0.9000\n",
            "Epoch: 238, Accuracy: 0.9000\n",
            "Epoch: 239, Accuracy: 0.9000\n",
            "Epoch: 240, Accuracy: 0.9000\n",
            "Epoch: 241, Accuracy: 0.9000\n",
            "Epoch: 242, Accuracy: 0.9000\n",
            "Epoch: 243, Accuracy: 0.9000\n",
            "Epoch: 244, Accuracy: 0.9000\n",
            "Epoch: 245, Accuracy: 0.9000\n",
            "Epoch: 246, Accuracy: 0.9000\n",
            "Epoch: 247, Accuracy: 0.9000\n",
            "Epoch: 248, Accuracy: 0.9000\n",
            "Epoch: 249, Accuracy: 0.9000\n",
            "Epoch: 250, Accuracy: 0.9000\n",
            "Epoch: 251, Accuracy: 0.9000\n",
            "Epoch: 252, Accuracy: 0.9000\n",
            "Epoch: 253, Accuracy: 0.9000\n",
            "Epoch: 254, Accuracy: 0.9000\n",
            "Epoch: 255, Accuracy: 0.9000\n",
            "Epoch: 256, Accuracy: 0.9000\n",
            "Epoch: 257, Accuracy: 0.9000\n",
            "Epoch: 258, Accuracy: 0.9000\n",
            "Epoch: 259, Accuracy: 0.9000\n",
            "Epoch: 260, Accuracy: 0.9000\n",
            "Epoch: 261, Accuracy: 0.9000\n",
            "Epoch: 262, Accuracy: 0.9000\n",
            "Epoch: 263, Accuracy: 0.9000\n",
            "Epoch: 264, Accuracy: 0.9000\n",
            "Epoch: 265, Accuracy: 0.9000\n",
            "Epoch: 266, Accuracy: 0.9000\n",
            "Epoch: 267, Accuracy: 0.9000\n",
            "Epoch: 268, Accuracy: 0.9000\n",
            "Epoch: 269, Accuracy: 0.9000\n",
            "Epoch: 270, Accuracy: 0.9000\n",
            "Epoch: 271, Accuracy: 0.9000\n",
            "Epoch: 272, Accuracy: 0.9000\n",
            "Epoch: 273, Accuracy: 0.9000\n",
            "Epoch: 274, Accuracy: 0.9000\n",
            "Epoch: 275, Accuracy: 0.9000\n",
            "Epoch: 276, Accuracy: 0.9000\n",
            "Epoch: 277, Accuracy: 0.9000\n",
            "Epoch: 278, Accuracy: 0.9000\n",
            "Epoch: 279, Accuracy: 0.9000\n",
            "Epoch: 280, Accuracy: 0.9000\n",
            "Epoch: 281, Accuracy: 0.9000\n",
            "Epoch: 282, Accuracy: 0.9000\n",
            "Epoch: 283, Accuracy: 0.9000\n",
            "Epoch: 284, Accuracy: 0.9000\n",
            "Epoch: 285, Accuracy: 0.9000\n",
            "Epoch: 286, Accuracy: 0.9000\n",
            "Epoch: 287, Accuracy: 0.9000\n",
            "Epoch: 288, Accuracy: 0.9000\n",
            "Epoch: 289, Accuracy: 0.9000\n",
            "Epoch: 290, Accuracy: 0.9000\n",
            "Epoch: 291, Accuracy: 0.9000\n",
            "Epoch: 292, Accuracy: 0.9000\n",
            "Epoch: 293, Accuracy: 0.9000\n",
            "Epoch: 294, Accuracy: 0.9000\n",
            "Epoch: 295, Accuracy: 0.9000\n",
            "Epoch: 296, Accuracy: 0.9000\n",
            "Epoch: 297, Accuracy: 0.9000\n",
            "Epoch: 298, Accuracy: 0.9000\n",
            "Epoch: 299, Accuracy: 0.9000\n",
            "Epoch: 300, Accuracy: 0.9000\n",
            "Epoch: 301, Accuracy: 0.9000\n",
            "Epoch: 302, Accuracy: 0.9000\n",
            "Epoch: 303, Accuracy: 0.9000\n",
            "Epoch: 304, Accuracy: 0.9000\n",
            "Epoch: 305, Accuracy: 0.8800\n",
            "Epoch: 306, Accuracy: 0.8800\n",
            "Epoch: 307, Accuracy: 0.8600\n",
            "Epoch: 308, Accuracy: 0.8600\n",
            "Epoch: 309, Accuracy: 0.8800\n",
            "Epoch: 310, Accuracy: 0.9000\n",
            "Epoch: 311, Accuracy: 0.9000\n",
            "Epoch: 312, Accuracy: 0.9000\n",
            "Epoch: 313, Accuracy: 0.8800\n",
            "Epoch: 314, Accuracy: 0.8200\n",
            "Epoch: 315, Accuracy: 0.7800\n",
            "Epoch: 316, Accuracy: 0.7800\n",
            "Epoch: 317, Accuracy: 0.8400\n",
            "Epoch: 318, Accuracy: 0.9000\n",
            "Epoch: 319, Accuracy: 0.8800\n",
            "Epoch: 320, Accuracy: 0.8800\n",
            "Epoch: 321, Accuracy: 0.9000\n",
            "Epoch: 322, Accuracy: 0.8600\n",
            "Epoch: 323, Accuracy: 0.8600\n",
            "Epoch: 324, Accuracy: 0.8400\n",
            "Epoch: 325, Accuracy: 0.8400\n",
            "Epoch: 326, Accuracy: 0.8400\n",
            "Epoch: 327, Accuracy: 0.8200\n",
            "Epoch: 328, Accuracy: 0.8200\n",
            "Epoch: 329, Accuracy: 0.8200\n",
            "Epoch: 330, Accuracy: 0.8400\n",
            "Epoch: 331, Accuracy: 0.8600\n",
            "Epoch: 332, Accuracy: 0.8600\n",
            "Epoch: 333, Accuracy: 0.8600\n",
            "Epoch: 334, Accuracy: 0.8600\n",
            "Epoch: 335, Accuracy: 0.8600\n",
            "Epoch: 336, Accuracy: 0.8800\n",
            "Epoch: 337, Accuracy: 0.8800\n",
            "Epoch: 338, Accuracy: 0.8800\n",
            "Epoch: 339, Accuracy: 0.8800\n",
            "Epoch: 340, Accuracy: 0.8800\n",
            "Epoch: 341, Accuracy: 0.8600\n",
            "Epoch: 342, Accuracy: 0.8800\n",
            "Epoch: 343, Accuracy: 0.8800\n",
            "Epoch: 344, Accuracy: 0.8800\n",
            "Epoch: 345, Accuracy: 0.8600\n",
            "Epoch: 346, Accuracy: 0.9000\n",
            "Epoch: 347, Accuracy: 0.9200\n",
            "Epoch: 348, Accuracy: 0.9200\n",
            "Epoch: 349, Accuracy: 0.9200\n",
            "Epoch: 350, Accuracy: 0.9200\n",
            "Epoch: 351, Accuracy: 0.9200\n",
            "Epoch: 352, Accuracy: 0.9200\n",
            "Epoch: 353, Accuracy: 0.9000\n",
            "Epoch: 354, Accuracy: 0.9000\n",
            "Epoch: 355, Accuracy: 0.9200\n",
            "Epoch: 356, Accuracy: 0.9200\n",
            "Epoch: 357, Accuracy: 0.9200\n",
            "Epoch: 358, Accuracy: 0.9200\n",
            "Epoch: 359, Accuracy: 0.9200\n",
            "Epoch: 360, Accuracy: 0.9200\n",
            "Epoch: 361, Accuracy: 0.9000\n",
            "Epoch: 362, Accuracy: 0.9400\n",
            "Epoch: 363, Accuracy: 0.9400\n",
            "Epoch: 364, Accuracy: 0.9000\n",
            "Epoch: 365, Accuracy: 0.9200\n",
            "Epoch: 366, Accuracy: 0.9200\n",
            "Epoch: 367, Accuracy: 0.9200\n",
            "Epoch: 368, Accuracy: 0.9200\n",
            "Epoch: 369, Accuracy: 0.9200\n",
            "Epoch: 370, Accuracy: 0.9200\n",
            "Epoch: 371, Accuracy: 0.9200\n",
            "Epoch: 372, Accuracy: 0.9200\n",
            "Epoch: 373, Accuracy: 0.9200\n",
            "Epoch: 374, Accuracy: 0.9200\n",
            "Epoch: 375, Accuracy: 0.9200\n",
            "Epoch: 376, Accuracy: 0.9200\n",
            "Epoch: 377, Accuracy: 0.9200\n",
            "Epoch: 378, Accuracy: 0.9200\n",
            "Epoch: 379, Accuracy: 0.9200\n",
            "Epoch: 380, Accuracy: 0.9200\n",
            "Epoch: 381, Accuracy: 0.9200\n",
            "Epoch: 382, Accuracy: 0.9200\n",
            "Epoch: 383, Accuracy: 0.9200\n",
            "Epoch: 384, Accuracy: 0.9200\n",
            "Epoch: 385, Accuracy: 0.9200\n",
            "Epoch: 386, Accuracy: 0.9200\n",
            "Epoch: 387, Accuracy: 0.9200\n",
            "Epoch: 388, Accuracy: 0.9200\n",
            "Epoch: 389, Accuracy: 0.9200\n",
            "Epoch: 390, Accuracy: 0.9200\n",
            "Epoch: 391, Accuracy: 0.9200\n",
            "Epoch: 392, Accuracy: 0.9200\n",
            "Epoch: 393, Accuracy: 0.9200\n",
            "Epoch: 394, Accuracy: 0.9200\n",
            "Epoch: 395, Accuracy: 0.9200\n",
            "Epoch: 396, Accuracy: 0.9200\n",
            "Epoch: 397, Accuracy: 0.9200\n",
            "Epoch: 398, Accuracy: 0.9200\n",
            "Epoch: 399, Accuracy: 0.9200\n",
            "Epoch: 400, Accuracy: 0.9200\n",
            "Epoch: 401, Accuracy: 0.1000\n",
            "Epoch: 402, Accuracy: 0.1000\n",
            "Epoch: 403, Accuracy: 0.1000\n",
            "Epoch: 404, Accuracy: 0.1000\n",
            "Epoch: 405, Accuracy: 0.1000\n",
            "Epoch: 406, Accuracy: 0.1000\n",
            "Epoch: 407, Accuracy: 0.1000\n",
            "Epoch: 408, Accuracy: 0.1000\n",
            "Epoch: 409, Accuracy: 0.1600\n",
            "Epoch: 410, Accuracy: 0.7600\n",
            "Epoch: 411, Accuracy: 0.8600\n",
            "Epoch: 412, Accuracy: 0.9000\n",
            "Epoch: 413, Accuracy: 0.9000\n",
            "Epoch: 414, Accuracy: 0.9000\n",
            "Epoch: 415, Accuracy: 0.9000\n",
            "Epoch: 416, Accuracy: 0.9000\n",
            "Epoch: 417, Accuracy: 0.9000\n",
            "Epoch: 418, Accuracy: 0.9000\n",
            "Epoch: 419, Accuracy: 0.9000\n",
            "Epoch: 420, Accuracy: 0.9000\n",
            "Epoch: 421, Accuracy: 0.9000\n",
            "Epoch: 422, Accuracy: 0.9000\n",
            "Epoch: 423, Accuracy: 0.9000\n",
            "Epoch: 424, Accuracy: 0.9000\n",
            "Epoch: 425, Accuracy: 0.9000\n",
            "Epoch: 426, Accuracy: 0.9000\n",
            "Epoch: 427, Accuracy: 0.9000\n",
            "Epoch: 428, Accuracy: 0.9000\n",
            "Epoch: 429, Accuracy: 0.9000\n",
            "Epoch: 430, Accuracy: 0.9000\n",
            "Epoch: 431, Accuracy: 0.9000\n",
            "Epoch: 432, Accuracy: 0.9000\n",
            "Epoch: 433, Accuracy: 0.9000\n",
            "Epoch: 434, Accuracy: 0.9000\n",
            "Epoch: 435, Accuracy: 0.9000\n",
            "Epoch: 436, Accuracy: 0.9000\n",
            "Epoch: 437, Accuracy: 0.9000\n",
            "Epoch: 438, Accuracy: 0.9000\n",
            "Epoch: 439, Accuracy: 0.9000\n",
            "Epoch: 440, Accuracy: 0.9000\n",
            "Epoch: 441, Accuracy: 0.9000\n",
            "Epoch: 442, Accuracy: 0.9000\n",
            "Epoch: 443, Accuracy: 0.9000\n",
            "Epoch: 444, Accuracy: 0.9000\n",
            "Epoch: 445, Accuracy: 0.9000\n",
            "Epoch: 446, Accuracy: 0.9000\n",
            "Epoch: 447, Accuracy: 0.9000\n",
            "Epoch: 448, Accuracy: 0.9000\n",
            "Epoch: 449, Accuracy: 0.9000\n",
            "Epoch: 450, Accuracy: 0.9000\n",
            "Epoch: 451, Accuracy: 0.9000\n",
            "Epoch: 452, Accuracy: 0.9000\n",
            "Epoch: 453, Accuracy: 0.9000\n",
            "Epoch: 454, Accuracy: 0.9000\n",
            "Epoch: 455, Accuracy: 0.9000\n",
            "Epoch: 456, Accuracy: 0.9000\n",
            "Epoch: 457, Accuracy: 0.9000\n",
            "Epoch: 458, Accuracy: 0.9000\n",
            "Epoch: 459, Accuracy: 0.9000\n",
            "Epoch: 460, Accuracy: 0.9000\n",
            "Epoch: 461, Accuracy: 0.9000\n",
            "Epoch: 462, Accuracy: 0.9000\n",
            "Epoch: 463, Accuracy: 0.9000\n",
            "Epoch: 464, Accuracy: 0.9000\n",
            "Epoch: 465, Accuracy: 0.9000\n",
            "Epoch: 466, Accuracy: 0.9000\n",
            "Epoch: 467, Accuracy: 0.9000\n",
            "Epoch: 468, Accuracy: 0.9000\n",
            "Epoch: 469, Accuracy: 0.9000\n",
            "Epoch: 470, Accuracy: 0.9000\n",
            "Epoch: 471, Accuracy: 0.9000\n",
            "Epoch: 472, Accuracy: 0.9000\n",
            "Epoch: 473, Accuracy: 0.9000\n",
            "Epoch: 474, Accuracy: 0.9000\n",
            "Epoch: 475, Accuracy: 0.9000\n",
            "Epoch: 476, Accuracy: 0.9000\n",
            "Epoch: 477, Accuracy: 0.9000\n",
            "Epoch: 478, Accuracy: 0.9000\n",
            "Epoch: 479, Accuracy: 0.9000\n",
            "Epoch: 480, Accuracy: 0.8800\n",
            "Epoch: 481, Accuracy: 0.8200\n",
            "Epoch: 482, Accuracy: 0.7400\n",
            "Epoch: 483, Accuracy: 0.7000\n",
            "Epoch: 484, Accuracy: 0.7200\n",
            "Epoch: 485, Accuracy: 0.8000\n",
            "Epoch: 486, Accuracy: 0.9000\n",
            "Epoch: 487, Accuracy: 0.9000\n",
            "Epoch: 488, Accuracy: 0.9000\n",
            "Epoch: 489, Accuracy: 0.9000\n",
            "Epoch: 490, Accuracy: 0.9000\n",
            "Epoch: 491, Accuracy: 0.9000\n",
            "Epoch: 492, Accuracy: 0.9000\n",
            "Epoch: 493, Accuracy: 0.9000\n",
            "Epoch: 494, Accuracy: 0.9000\n",
            "Epoch: 495, Accuracy: 0.9000\n",
            "Epoch: 496, Accuracy: 0.9000\n",
            "Epoch: 497, Accuracy: 0.9000\n",
            "Epoch: 498, Accuracy: 0.9000\n",
            "Epoch: 499, Accuracy: 0.9000\n",
            "Epoch: 500, Accuracy: 0.9000\n",
            "Epoch: 501, Accuracy: 0.9000\n",
            "Epoch: 502, Accuracy: 0.9000\n",
            "Epoch: 503, Accuracy: 0.9000\n",
            "Epoch: 504, Accuracy: 0.9000\n",
            "Epoch: 505, Accuracy: 0.9000\n",
            "Epoch: 506, Accuracy: 0.9000\n",
            "Epoch: 507, Accuracy: 0.9000\n",
            "Epoch: 508, Accuracy: 0.9000\n",
            "Epoch: 509, Accuracy: 0.9000\n",
            "Epoch: 510, Accuracy: 0.9000\n",
            "Epoch: 511, Accuracy: 0.9000\n",
            "Epoch: 512, Accuracy: 0.9000\n",
            "Epoch: 513, Accuracy: 0.9000\n",
            "Epoch: 514, Accuracy: 0.9000\n",
            "Epoch: 515, Accuracy: 0.9000\n",
            "Epoch: 516, Accuracy: 0.9000\n",
            "Epoch: 517, Accuracy: 0.9000\n",
            "Epoch: 518, Accuracy: 0.9000\n",
            "Epoch: 519, Accuracy: 0.9000\n",
            "Epoch: 520, Accuracy: 0.9000\n",
            "Epoch: 521, Accuracy: 0.9000\n",
            "Epoch: 522, Accuracy: 0.9000\n",
            "Epoch: 523, Accuracy: 0.9000\n",
            "Epoch: 524, Accuracy: 0.9000\n",
            "Epoch: 525, Accuracy: 0.9000\n",
            "Epoch: 526, Accuracy: 0.9000\n",
            "Epoch: 527, Accuracy: 0.9000\n",
            "Epoch: 528, Accuracy: 0.9000\n",
            "Epoch: 529, Accuracy: 0.9000\n",
            "Epoch: 530, Accuracy: 0.9000\n",
            "Epoch: 531, Accuracy: 0.9000\n",
            "Epoch: 532, Accuracy: 0.9000\n",
            "Epoch: 533, Accuracy: 0.9000\n",
            "Epoch: 534, Accuracy: 0.9000\n",
            "Epoch: 535, Accuracy: 0.9000\n",
            "Epoch: 536, Accuracy: 0.9200\n",
            "Epoch: 537, Accuracy: 0.9200\n",
            "Epoch: 538, Accuracy: 0.9000\n",
            "Epoch: 539, Accuracy: 0.9000\n",
            "Epoch: 540, Accuracy: 0.9000\n",
            "Epoch: 541, Accuracy: 0.9200\n",
            "Epoch: 542, Accuracy: 0.9200\n",
            "Epoch: 543, Accuracy: 0.9200\n",
            "Epoch: 544, Accuracy: 0.9200\n",
            "Epoch: 545, Accuracy: 0.9000\n",
            "Epoch: 546, Accuracy: 0.9000\n",
            "Epoch: 547, Accuracy: 0.9000\n",
            "Epoch: 548, Accuracy: 0.9200\n",
            "Epoch: 549, Accuracy: 0.9200\n",
            "Epoch: 550, Accuracy: 0.9200\n",
            "Epoch: 551, Accuracy: 0.9200\n",
            "Epoch: 552, Accuracy: 0.9200\n",
            "Epoch: 553, Accuracy: 0.9000\n",
            "Epoch: 554, Accuracy: 0.9000\n",
            "Epoch: 555, Accuracy: 0.9200\n",
            "Epoch: 556, Accuracy: 0.9200\n",
            "Epoch: 557, Accuracy: 0.9200\n",
            "Epoch: 558, Accuracy: 0.9200\n",
            "Epoch: 559, Accuracy: 0.9200\n",
            "Epoch: 560, Accuracy: 0.9200\n",
            "Epoch: 561, Accuracy: 0.9200\n",
            "Epoch: 562, Accuracy: 0.9200\n",
            "Epoch: 563, Accuracy: 0.9200\n",
            "Epoch: 564, Accuracy: 0.9000\n",
            "Epoch: 565, Accuracy: 0.9200\n",
            "Epoch: 566, Accuracy: 0.9200\n",
            "Epoch: 567, Accuracy: 0.9200\n",
            "Epoch: 568, Accuracy: 0.9200\n",
            "Epoch: 569, Accuracy: 0.9200\n",
            "Epoch: 570, Accuracy: 0.9000\n",
            "Epoch: 571, Accuracy: 0.9200\n",
            "Epoch: 572, Accuracy: 0.9200\n",
            "Epoch: 573, Accuracy: 0.9000\n",
            "Epoch: 574, Accuracy: 0.9200\n",
            "Epoch: 575, Accuracy: 0.9200\n",
            "Epoch: 576, Accuracy: 0.9200\n",
            "Epoch: 577, Accuracy: 0.9200\n",
            "Epoch: 578, Accuracy: 0.9200\n",
            "Epoch: 579, Accuracy: 0.9200\n",
            "Epoch: 580, Accuracy: 0.9200\n",
            "Epoch: 581, Accuracy: 0.9200\n",
            "Epoch: 582, Accuracy: 0.9200\n",
            "Epoch: 583, Accuracy: 0.9200\n",
            "Epoch: 584, Accuracy: 0.9000\n",
            "Epoch: 585, Accuracy: 0.9000\n",
            "Epoch: 586, Accuracy: 0.9200\n",
            "Epoch: 587, Accuracy: 0.9200\n",
            "Epoch: 588, Accuracy: 0.9000\n",
            "Epoch: 589, Accuracy: 0.9000\n",
            "Epoch: 590, Accuracy: 0.9200\n",
            "Epoch: 591, Accuracy: 0.9400\n",
            "Epoch: 592, Accuracy: 0.9200\n",
            "Epoch: 593, Accuracy: 0.9200\n",
            "Epoch: 594, Accuracy: 0.9200\n",
            "Epoch: 595, Accuracy: 0.9200\n",
            "Epoch: 596, Accuracy: 0.9200\n",
            "Epoch: 597, Accuracy: 0.9000\n",
            "Epoch: 598, Accuracy: 0.9000\n",
            "Epoch: 599, Accuracy: 0.9000\n",
            "Epoch: 600, Accuracy: 0.9200\n",
            "Epoch: 601, Accuracy: 0.1000\n",
            "Epoch: 602, Accuracy: 0.1000\n",
            "Epoch: 603, Accuracy: 0.1400\n",
            "Epoch: 604, Accuracy: 0.6800\n",
            "Epoch: 605, Accuracy: 0.9000\n",
            "Epoch: 606, Accuracy: 0.9000\n",
            "Epoch: 607, Accuracy: 0.9000\n",
            "Epoch: 608, Accuracy: 0.9000\n",
            "Epoch: 609, Accuracy: 0.9000\n",
            "Epoch: 610, Accuracy: 0.9000\n",
            "Epoch: 611, Accuracy: 0.9000\n",
            "Epoch: 612, Accuracy: 0.9000\n",
            "Epoch: 613, Accuracy: 0.9000\n",
            "Epoch: 614, Accuracy: 0.9000\n",
            "Epoch: 615, Accuracy: 0.9000\n",
            "Epoch: 616, Accuracy: 0.9000\n",
            "Epoch: 617, Accuracy: 0.9000\n",
            "Epoch: 618, Accuracy: 0.9000\n",
            "Epoch: 619, Accuracy: 0.9000\n",
            "Epoch: 620, Accuracy: 0.9000\n",
            "Epoch: 621, Accuracy: 0.9000\n",
            "Epoch: 622, Accuracy: 0.9000\n",
            "Epoch: 623, Accuracy: 0.9000\n",
            "Epoch: 624, Accuracy: 0.9000\n",
            "Epoch: 625, Accuracy: 0.9000\n",
            "Epoch: 626, Accuracy: 0.9000\n",
            "Epoch: 627, Accuracy: 0.9000\n",
            "Epoch: 628, Accuracy: 0.9000\n",
            "Epoch: 629, Accuracy: 0.9000\n",
            "Epoch: 630, Accuracy: 0.9000\n",
            "Epoch: 631, Accuracy: 0.9000\n",
            "Epoch: 632, Accuracy: 0.9000\n",
            "Epoch: 633, Accuracy: 0.9000\n",
            "Epoch: 634, Accuracy: 0.9000\n",
            "Epoch: 635, Accuracy: 0.9000\n",
            "Epoch: 636, Accuracy: 0.9000\n",
            "Epoch: 637, Accuracy: 0.9000\n",
            "Epoch: 638, Accuracy: 0.9000\n",
            "Epoch: 639, Accuracy: 0.9000\n",
            "Epoch: 640, Accuracy: 0.9000\n",
            "Epoch: 641, Accuracy: 0.9000\n",
            "Epoch: 642, Accuracy: 0.9000\n",
            "Epoch: 643, Accuracy: 0.9000\n",
            "Epoch: 644, Accuracy: 0.9000\n",
            "Epoch: 645, Accuracy: 0.9000\n",
            "Epoch: 646, Accuracy: 0.9000\n",
            "Epoch: 647, Accuracy: 0.9000\n",
            "Epoch: 648, Accuracy: 0.9000\n",
            "Epoch: 649, Accuracy: 0.9000\n",
            "Epoch: 650, Accuracy: 0.9000\n",
            "Epoch: 651, Accuracy: 0.9000\n",
            "Epoch: 652, Accuracy: 0.9000\n",
            "Epoch: 653, Accuracy: 0.9000\n",
            "Epoch: 654, Accuracy: 0.9000\n",
            "Epoch: 655, Accuracy: 0.9000\n",
            "Epoch: 656, Accuracy: 0.9000\n",
            "Epoch: 657, Accuracy: 0.9000\n",
            "Epoch: 658, Accuracy: 0.9000\n",
            "Epoch: 659, Accuracy: 0.9000\n",
            "Epoch: 660, Accuracy: 0.9000\n",
            "Epoch: 661, Accuracy: 0.9000\n",
            "Epoch: 662, Accuracy: 0.9000\n",
            "Epoch: 663, Accuracy: 0.9000\n",
            "Epoch: 664, Accuracy: 0.9000\n",
            "Epoch: 665, Accuracy: 0.9000\n",
            "Epoch: 666, Accuracy: 0.9000\n",
            "Epoch: 667, Accuracy: 0.9000\n",
            "Epoch: 668, Accuracy: 0.9000\n",
            "Epoch: 669, Accuracy: 0.9000\n",
            "Epoch: 670, Accuracy: 0.9000\n",
            "Epoch: 671, Accuracy: 0.9000\n",
            "Epoch: 672, Accuracy: 0.9000\n",
            "Epoch: 673, Accuracy: 0.9000\n",
            "Epoch: 674, Accuracy: 0.9000\n",
            "Epoch: 675, Accuracy: 0.9000\n",
            "Epoch: 676, Accuracy: 0.9000\n",
            "Epoch: 677, Accuracy: 0.9000\n",
            "Epoch: 678, Accuracy: 0.9000\n",
            "Epoch: 679, Accuracy: 0.9000\n",
            "Epoch: 680, Accuracy: 0.9000\n",
            "Epoch: 681, Accuracy: 0.9000\n",
            "Epoch: 682, Accuracy: 0.9000\n",
            "Epoch: 683, Accuracy: 0.9000\n",
            "Epoch: 684, Accuracy: 0.9000\n",
            "Epoch: 685, Accuracy: 0.9000\n",
            "Epoch: 686, Accuracy: 0.9000\n",
            "Epoch: 687, Accuracy: 0.9000\n",
            "Epoch: 688, Accuracy: 0.9000\n",
            "Epoch: 689, Accuracy: 0.9000\n",
            "Epoch: 690, Accuracy: 0.9000\n",
            "Epoch: 691, Accuracy: 0.9000\n",
            "Epoch: 692, Accuracy: 0.9000\n",
            "Epoch: 693, Accuracy: 0.9000\n",
            "Epoch: 694, Accuracy: 0.9000\n",
            "Epoch: 695, Accuracy: 0.9000\n",
            "Epoch: 696, Accuracy: 0.9000\n",
            "Epoch: 697, Accuracy: 0.9000\n",
            "Epoch: 698, Accuracy: 0.9000\n",
            "Epoch: 699, Accuracy: 0.9000\n",
            "Epoch: 700, Accuracy: 0.9000\n",
            "Epoch: 701, Accuracy: 0.9000\n",
            "Epoch: 702, Accuracy: 0.9000\n",
            "Epoch: 703, Accuracy: 0.9000\n",
            "Epoch: 704, Accuracy: 0.9000\n",
            "Epoch: 705, Accuracy: 0.9000\n",
            "Epoch: 706, Accuracy: 0.9000\n",
            "Epoch: 707, Accuracy: 0.9000\n",
            "Epoch: 708, Accuracy: 0.9000\n",
            "Epoch: 709, Accuracy: 0.9000\n",
            "Epoch: 710, Accuracy: 0.9000\n",
            "Epoch: 711, Accuracy: 0.9000\n",
            "Epoch: 712, Accuracy: 0.9000\n",
            "Epoch: 713, Accuracy: 0.9000\n",
            "Epoch: 714, Accuracy: 0.9000\n",
            "Epoch: 715, Accuracy: 0.9000\n",
            "Epoch: 716, Accuracy: 0.9000\n",
            "Epoch: 717, Accuracy: 0.9000\n",
            "Epoch: 718, Accuracy: 0.9000\n",
            "Epoch: 719, Accuracy: 0.9200\n",
            "Epoch: 720, Accuracy: 0.9000\n",
            "Epoch: 721, Accuracy: 0.9000\n",
            "Epoch: 722, Accuracy: 0.9000\n",
            "Epoch: 723, Accuracy: 0.9000\n",
            "Epoch: 724, Accuracy: 0.9200\n",
            "Epoch: 725, Accuracy: 0.9200\n",
            "Epoch: 726, Accuracy: 0.9200\n",
            "Epoch: 727, Accuracy: 0.9200\n",
            "Epoch: 728, Accuracy: 0.9200\n",
            "Epoch: 729, Accuracy: 0.9000\n",
            "Epoch: 730, Accuracy: 0.9000\n",
            "Epoch: 731, Accuracy: 0.9000\n",
            "Epoch: 732, Accuracy: 0.9200\n",
            "Epoch: 733, Accuracy: 0.9200\n",
            "Epoch: 734, Accuracy: 0.9200\n",
            "Epoch: 735, Accuracy: 0.9200\n",
            "Epoch: 736, Accuracy: 0.9200\n",
            "Epoch: 737, Accuracy: 0.9200\n",
            "Epoch: 738, Accuracy: 0.9200\n",
            "Epoch: 739, Accuracy: 0.9000\n",
            "Epoch: 740, Accuracy: 0.9200\n",
            "Epoch: 741, Accuracy: 0.9200\n",
            "Epoch: 742, Accuracy: 0.9200\n",
            "Epoch: 743, Accuracy: 0.9200\n",
            "Epoch: 744, Accuracy: 0.9200\n",
            "Epoch: 745, Accuracy: 0.9200\n",
            "Epoch: 746, Accuracy: 0.9200\n",
            "Epoch: 747, Accuracy: 0.9200\n",
            "Epoch: 748, Accuracy: 0.9200\n",
            "Epoch: 749, Accuracy: 0.9200\n",
            "Epoch: 750, Accuracy: 0.9200\n",
            "Epoch: 751, Accuracy: 0.9200\n",
            "Epoch: 752, Accuracy: 0.9200\n",
            "Epoch: 753, Accuracy: 0.9200\n",
            "Epoch: 754, Accuracy: 0.9200\n",
            "Epoch: 755, Accuracy: 0.9200\n",
            "Epoch: 756, Accuracy: 0.9200\n",
            "Epoch: 757, Accuracy: 0.9200\n",
            "Epoch: 758, Accuracy: 0.9200\n",
            "Epoch: 759, Accuracy: 0.9200\n",
            "Epoch: 760, Accuracy: 0.9200\n",
            "Epoch: 761, Accuracy: 0.9200\n",
            "Epoch: 762, Accuracy: 0.9200\n",
            "Epoch: 763, Accuracy: 0.9200\n",
            "Epoch: 764, Accuracy: 0.9200\n",
            "Epoch: 765, Accuracy: 0.9200\n",
            "Epoch: 766, Accuracy: 0.9200\n",
            "Epoch: 767, Accuracy: 0.9200\n",
            "Epoch: 768, Accuracy: 0.9200\n",
            "Epoch: 769, Accuracy: 0.9200\n",
            "Epoch: 770, Accuracy: 0.9200\n",
            "Epoch: 771, Accuracy: 0.9200\n",
            "Epoch: 772, Accuracy: 0.9200\n",
            "Epoch: 773, Accuracy: 0.9000\n",
            "Epoch: 774, Accuracy: 0.9200\n",
            "Epoch: 775, Accuracy: 0.9000\n",
            "Epoch: 776, Accuracy: 0.9000\n",
            "Epoch: 777, Accuracy: 0.9000\n",
            "Epoch: 778, Accuracy: 0.9200\n",
            "Epoch: 779, Accuracy: 0.9200\n",
            "Epoch: 780, Accuracy: 0.9200\n",
            "Epoch: 781, Accuracy: 0.9200\n",
            "Epoch: 782, Accuracy: 0.9200\n",
            "Epoch: 783, Accuracy: 0.9200\n",
            "Epoch: 784, Accuracy: 0.9200\n",
            "Epoch: 785, Accuracy: 0.9000\n",
            "Epoch: 786, Accuracy: 0.9200\n",
            "Epoch: 787, Accuracy: 0.9200\n",
            "Epoch: 788, Accuracy: 0.9000\n",
            "Epoch: 789, Accuracy: 0.8800\n",
            "Epoch: 790, Accuracy: 0.8800\n",
            "Epoch: 791, Accuracy: 0.9200\n",
            "Epoch: 792, Accuracy: 0.9200\n",
            "Epoch: 793, Accuracy: 0.9200\n",
            "Epoch: 794, Accuracy: 0.9200\n",
            "Epoch: 795, Accuracy: 0.9000\n",
            "Epoch: 796, Accuracy: 0.9200\n",
            "Epoch: 797, Accuracy: 0.9000\n",
            "Epoch: 798, Accuracy: 0.9000\n",
            "Epoch: 799, Accuracy: 0.9200\n",
            "Epoch: 800, Accuracy: 0.9200\n",
            "Epoch: 801, Accuracy: 0.1000\n",
            "Epoch: 802, Accuracy: 0.1000\n",
            "Epoch: 803, Accuracy: 0.1000\n",
            "Epoch: 804, Accuracy: 0.1200\n",
            "Epoch: 805, Accuracy: 0.1600\n",
            "Epoch: 806, Accuracy: 0.2800\n",
            "Epoch: 807, Accuracy: 0.8800\n",
            "Epoch: 808, Accuracy: 0.8800\n",
            "Epoch: 809, Accuracy: 0.9000\n",
            "Epoch: 810, Accuracy: 0.9000\n",
            "Epoch: 811, Accuracy: 0.9000\n",
            "Epoch: 812, Accuracy: 0.9000\n",
            "Epoch: 813, Accuracy: 0.9000\n",
            "Epoch: 814, Accuracy: 0.9000\n",
            "Epoch: 815, Accuracy: 0.9000\n",
            "Epoch: 816, Accuracy: 0.9000\n",
            "Epoch: 817, Accuracy: 0.9000\n",
            "Epoch: 818, Accuracy: 0.9000\n",
            "Epoch: 819, Accuracy: 0.9000\n",
            "Epoch: 820, Accuracy: 0.9000\n",
            "Epoch: 821, Accuracy: 0.9000\n",
            "Epoch: 822, Accuracy: 0.9000\n",
            "Epoch: 823, Accuracy: 0.9000\n",
            "Epoch: 824, Accuracy: 0.9000\n",
            "Epoch: 825, Accuracy: 0.9000\n",
            "Epoch: 826, Accuracy: 0.9000\n",
            "Epoch: 827, Accuracy: 0.9000\n",
            "Epoch: 828, Accuracy: 0.9000\n",
            "Epoch: 829, Accuracy: 0.9000\n",
            "Epoch: 830, Accuracy: 0.9000\n",
            "Epoch: 831, Accuracy: 0.9000\n",
            "Epoch: 832, Accuracy: 0.9000\n",
            "Epoch: 833, Accuracy: 0.9000\n",
            "Epoch: 834, Accuracy: 0.9000\n",
            "Epoch: 835, Accuracy: 0.9000\n",
            "Epoch: 836, Accuracy: 0.9000\n",
            "Epoch: 837, Accuracy: 0.9000\n",
            "Epoch: 838, Accuracy: 0.9000\n",
            "Epoch: 839, Accuracy: 0.9000\n",
            "Epoch: 840, Accuracy: 0.9000\n",
            "Epoch: 841, Accuracy: 0.9000\n",
            "Epoch: 842, Accuracy: 0.9000\n",
            "Epoch: 843, Accuracy: 0.9000\n",
            "Epoch: 844, Accuracy: 0.9000\n",
            "Epoch: 845, Accuracy: 0.9000\n",
            "Epoch: 846, Accuracy: 0.9000\n",
            "Epoch: 847, Accuracy: 0.9000\n",
            "Epoch: 848, Accuracy: 0.9000\n",
            "Epoch: 849, Accuracy: 0.9000\n",
            "Epoch: 850, Accuracy: 0.9000\n",
            "Epoch: 851, Accuracy: 0.9000\n",
            "Epoch: 852, Accuracy: 0.9000\n",
            "Epoch: 853, Accuracy: 0.9000\n",
            "Epoch: 854, Accuracy: 0.9000\n",
            "Epoch: 855, Accuracy: 0.9000\n",
            "Epoch: 856, Accuracy: 0.9000\n",
            "Epoch: 857, Accuracy: 0.9000\n",
            "Epoch: 858, Accuracy: 0.9000\n",
            "Epoch: 859, Accuracy: 0.9000\n",
            "Epoch: 860, Accuracy: 0.9000\n",
            "Epoch: 861, Accuracy: 0.9000\n",
            "Epoch: 862, Accuracy: 0.9000\n",
            "Epoch: 863, Accuracy: 0.9000\n",
            "Epoch: 864, Accuracy: 0.9000\n",
            "Epoch: 865, Accuracy: 0.9000\n",
            "Epoch: 866, Accuracy: 0.9000\n",
            "Epoch: 867, Accuracy: 0.9000\n",
            "Epoch: 868, Accuracy: 0.9000\n",
            "Epoch: 869, Accuracy: 0.9000\n",
            "Epoch: 870, Accuracy: 0.9000\n",
            "Epoch: 871, Accuracy: 0.9000\n",
            "Epoch: 872, Accuracy: 0.9000\n",
            "Epoch: 873, Accuracy: 0.9000\n",
            "Epoch: 874, Accuracy: 0.9000\n",
            "Epoch: 875, Accuracy: 0.9000\n",
            "Epoch: 876, Accuracy: 0.9000\n",
            "Epoch: 877, Accuracy: 0.9000\n",
            "Epoch: 878, Accuracy: 0.9000\n",
            "Epoch: 879, Accuracy: 0.9000\n",
            "Epoch: 880, Accuracy: 0.9000\n",
            "Epoch: 881, Accuracy: 0.9000\n",
            "Epoch: 882, Accuracy: 0.9000\n",
            "Epoch: 883, Accuracy: 0.9000\n",
            "Epoch: 884, Accuracy: 0.9000\n",
            "Epoch: 885, Accuracy: 0.9000\n",
            "Epoch: 886, Accuracy: 0.9000\n",
            "Epoch: 887, Accuracy: 0.9000\n",
            "Epoch: 888, Accuracy: 0.9000\n",
            "Epoch: 889, Accuracy: 0.9000\n",
            "Epoch: 890, Accuracy: 0.9000\n",
            "Epoch: 891, Accuracy: 0.9000\n",
            "Epoch: 892, Accuracy: 0.9000\n",
            "Epoch: 893, Accuracy: 0.9000\n",
            "Epoch: 894, Accuracy: 0.9000\n",
            "Epoch: 895, Accuracy: 0.9000\n",
            "Epoch: 896, Accuracy: 0.9000\n",
            "Epoch: 897, Accuracy: 0.9000\n",
            "Epoch: 898, Accuracy: 0.9000\n",
            "Epoch: 899, Accuracy: 0.9000\n",
            "Epoch: 900, Accuracy: 0.9000\n",
            "Epoch: 901, Accuracy: 0.9000\n",
            "Epoch: 902, Accuracy: 0.9000\n",
            "Epoch: 903, Accuracy: 0.9000\n",
            "Epoch: 904, Accuracy: 0.9000\n",
            "Epoch: 905, Accuracy: 0.9000\n",
            "Epoch: 906, Accuracy: 0.9000\n",
            "Epoch: 907, Accuracy: 0.9000\n",
            "Epoch: 908, Accuracy: 0.9000\n",
            "Epoch: 909, Accuracy: 0.9000\n",
            "Epoch: 910, Accuracy: 0.9000\n",
            "Epoch: 911, Accuracy: 0.9000\n",
            "Epoch: 912, Accuracy: 0.9000\n",
            "Epoch: 913, Accuracy: 0.9000\n",
            "Epoch: 914, Accuracy: 0.9000\n",
            "Epoch: 915, Accuracy: 0.9000\n",
            "Epoch: 916, Accuracy: 0.9000\n",
            "Epoch: 917, Accuracy: 0.9000\n",
            "Epoch: 918, Accuracy: 0.9000\n",
            "Epoch: 919, Accuracy: 0.9000\n",
            "Epoch: 920, Accuracy: 0.9000\n",
            "Epoch: 921, Accuracy: 0.9000\n",
            "Epoch: 922, Accuracy: 0.9000\n",
            "Epoch: 923, Accuracy: 0.9000\n",
            "Epoch: 924, Accuracy: 0.9000\n",
            "Epoch: 925, Accuracy: 0.9000\n",
            "Epoch: 926, Accuracy: 0.9000\n",
            "Epoch: 927, Accuracy: 0.9000\n",
            "Epoch: 928, Accuracy: 0.9000\n",
            "Epoch: 929, Accuracy: 0.9000\n",
            "Epoch: 930, Accuracy: 0.9000\n",
            "Epoch: 931, Accuracy: 0.9000\n",
            "Epoch: 932, Accuracy: 0.9000\n",
            "Epoch: 933, Accuracy: 0.9000\n",
            "Epoch: 934, Accuracy: 0.9000\n",
            "Epoch: 935, Accuracy: 0.9000\n",
            "Epoch: 936, Accuracy: 0.9000\n",
            "Epoch: 937, Accuracy: 0.9000\n",
            "Epoch: 938, Accuracy: 0.9000\n",
            "Epoch: 939, Accuracy: 0.9000\n",
            "Epoch: 940, Accuracy: 0.9000\n",
            "Epoch: 941, Accuracy: 0.9000\n",
            "Epoch: 942, Accuracy: 0.9000\n",
            "Epoch: 943, Accuracy: 0.9000\n",
            "Epoch: 944, Accuracy: 0.9000\n",
            "Epoch: 945, Accuracy: 0.9000\n",
            "Epoch: 946, Accuracy: 0.9000\n",
            "Epoch: 947, Accuracy: 0.9000\n",
            "Epoch: 948, Accuracy: 0.9000\n",
            "Epoch: 949, Accuracy: 0.9000\n",
            "Epoch: 950, Accuracy: 0.9000\n",
            "Epoch: 951, Accuracy: 0.9000\n",
            "Epoch: 952, Accuracy: 0.9000\n",
            "Epoch: 953, Accuracy: 0.9000\n",
            "Epoch: 954, Accuracy: 0.9000\n",
            "Epoch: 955, Accuracy: 0.9000\n",
            "Epoch: 956, Accuracy: 0.9000\n",
            "Epoch: 957, Accuracy: 0.9000\n",
            "Epoch: 958, Accuracy: 0.9000\n",
            "Epoch: 959, Accuracy: 0.9000\n",
            "Epoch: 960, Accuracy: 0.9000\n",
            "Epoch: 961, Accuracy: 0.9000\n",
            "Epoch: 962, Accuracy: 0.9000\n",
            "Epoch: 963, Accuracy: 0.9000\n",
            "Epoch: 964, Accuracy: 0.9000\n",
            "Epoch: 965, Accuracy: 0.9000\n",
            "Epoch: 966, Accuracy: 0.9000\n",
            "Epoch: 967, Accuracy: 0.9000\n",
            "Epoch: 968, Accuracy: 0.9000\n",
            "Epoch: 969, Accuracy: 0.9000\n",
            "Epoch: 970, Accuracy: 0.9000\n",
            "Epoch: 971, Accuracy: 0.9000\n",
            "Epoch: 972, Accuracy: 0.9000\n",
            "Epoch: 973, Accuracy: 0.9000\n",
            "Epoch: 974, Accuracy: 0.9000\n",
            "Epoch: 975, Accuracy: 0.9000\n",
            "Epoch: 976, Accuracy: 0.9000\n",
            "Epoch: 977, Accuracy: 0.9000\n",
            "Epoch: 978, Accuracy: 0.9000\n",
            "Epoch: 979, Accuracy: 0.9000\n",
            "Epoch: 980, Accuracy: 0.9000\n",
            "Epoch: 981, Accuracy: 0.9000\n",
            "Epoch: 982, Accuracy: 0.9000\n",
            "Epoch: 983, Accuracy: 0.9000\n",
            "Epoch: 984, Accuracy: 0.9000\n",
            "Epoch: 985, Accuracy: 0.9000\n",
            "Epoch: 986, Accuracy: 0.9000\n",
            "Epoch: 987, Accuracy: 0.9000\n",
            "Epoch: 988, Accuracy: 0.9000\n",
            "Epoch: 989, Accuracy: 0.9200\n",
            "Epoch: 990, Accuracy: 0.9200\n",
            "Epoch: 991, Accuracy: 0.9000\n",
            "Epoch: 992, Accuracy: 0.9000\n",
            "Epoch: 993, Accuracy: 0.9000\n",
            "Epoch: 994, Accuracy: 0.9000\n",
            "Epoch: 995, Accuracy: 0.9200\n",
            "Epoch: 996, Accuracy: 0.9200\n",
            "Epoch: 997, Accuracy: 0.9200\n",
            "Epoch: 998, Accuracy: 0.9200\n",
            "Epoch: 999, Accuracy: 0.9200\n",
            "Epoch: 1000, Accuracy: 0.9200\n",
            "Epoch: 1001, Accuracy: 0.1800\n",
            "Epoch: 1002, Accuracy: 0.3000\n",
            "Epoch: 1003, Accuracy: 0.8200\n",
            "Epoch: 1004, Accuracy: 0.8800\n",
            "Epoch: 1005, Accuracy: 0.9000\n",
            "Epoch: 1006, Accuracy: 0.9000\n",
            "Epoch: 1007, Accuracy: 0.9000\n",
            "Epoch: 1008, Accuracy: 0.9000\n",
            "Epoch: 1009, Accuracy: 0.9000\n",
            "Epoch: 1010, Accuracy: 0.9000\n",
            "Epoch: 1011, Accuracy: 0.9000\n",
            "Epoch: 1012, Accuracy: 0.9000\n",
            "Epoch: 1013, Accuracy: 0.9000\n",
            "Epoch: 1014, Accuracy: 0.9000\n",
            "Epoch: 1015, Accuracy: 0.9000\n",
            "Epoch: 1016, Accuracy: 0.9000\n",
            "Epoch: 1017, Accuracy: 0.9000\n",
            "Epoch: 1018, Accuracy: 0.9000\n",
            "Epoch: 1019, Accuracy: 0.9000\n",
            "Epoch: 1020, Accuracy: 0.9000\n",
            "Epoch: 1021, Accuracy: 0.9000\n",
            "Epoch: 1022, Accuracy: 0.9000\n",
            "Epoch: 1023, Accuracy: 0.9000\n",
            "Epoch: 1024, Accuracy: 0.9000\n",
            "Epoch: 1025, Accuracy: 0.9000\n",
            "Epoch: 1026, Accuracy: 0.9000\n",
            "Epoch: 1027, Accuracy: 0.9000\n",
            "Epoch: 1028, Accuracy: 0.9000\n",
            "Epoch: 1029, Accuracy: 0.9000\n",
            "Epoch: 1030, Accuracy: 0.9000\n",
            "Epoch: 1031, Accuracy: 0.9000\n",
            "Epoch: 1032, Accuracy: 0.9000\n",
            "Epoch: 1033, Accuracy: 0.9000\n",
            "Epoch: 1034, Accuracy: 0.9000\n",
            "Epoch: 1035, Accuracy: 0.9000\n",
            "Epoch: 1036, Accuracy: 0.9000\n",
            "Epoch: 1037, Accuracy: 0.9000\n",
            "Epoch: 1038, Accuracy: 0.9000\n",
            "Epoch: 1039, Accuracy: 0.9000\n",
            "Epoch: 1040, Accuracy: 0.9000\n",
            "Epoch: 1041, Accuracy: 0.9000\n",
            "Epoch: 1042, Accuracy: 0.9000\n",
            "Epoch: 1043, Accuracy: 0.9000\n",
            "Epoch: 1044, Accuracy: 0.9000\n",
            "Epoch: 1045, Accuracy: 0.9000\n",
            "Epoch: 1046, Accuracy: 0.9000\n",
            "Epoch: 1047, Accuracy: 0.9000\n",
            "Epoch: 1048, Accuracy: 0.9000\n",
            "Epoch: 1049, Accuracy: 0.9000\n",
            "Epoch: 1050, Accuracy: 0.9000\n",
            "Epoch: 1051, Accuracy: 0.9000\n",
            "Epoch: 1052, Accuracy: 0.9000\n",
            "Epoch: 1053, Accuracy: 0.9000\n",
            "Epoch: 1054, Accuracy: 0.9000\n",
            "Epoch: 1055, Accuracy: 0.9000\n",
            "Epoch: 1056, Accuracy: 0.9000\n",
            "Epoch: 1057, Accuracy: 0.9000\n",
            "Epoch: 1058, Accuracy: 0.9000\n",
            "Epoch: 1059, Accuracy: 0.9000\n",
            "Epoch: 1060, Accuracy: 0.9000\n",
            "Epoch: 1061, Accuracy: 0.9000\n",
            "Epoch: 1062, Accuracy: 0.9000\n",
            "Epoch: 1063, Accuracy: 0.9000\n",
            "Epoch: 1064, Accuracy: 0.9000\n",
            "Epoch: 1065, Accuracy: 0.9000\n",
            "Epoch: 1066, Accuracy: 0.9000\n",
            "Epoch: 1067, Accuracy: 0.9000\n",
            "Epoch: 1068, Accuracy: 0.9000\n",
            "Epoch: 1069, Accuracy: 0.9000\n",
            "Epoch: 1070, Accuracy: 0.9000\n",
            "Epoch: 1071, Accuracy: 0.9000\n",
            "Epoch: 1072, Accuracy: 0.9000\n",
            "Epoch: 1073, Accuracy: 0.9000\n",
            "Epoch: 1074, Accuracy: 0.9000\n",
            "Epoch: 1075, Accuracy: 0.9000\n",
            "Epoch: 1076, Accuracy: 0.9000\n",
            "Epoch: 1077, Accuracy: 0.9000\n",
            "Epoch: 1078, Accuracy: 0.9000\n",
            "Epoch: 1079, Accuracy: 0.9000\n",
            "Epoch: 1080, Accuracy: 0.9000\n",
            "Epoch: 1081, Accuracy: 0.9000\n",
            "Epoch: 1082, Accuracy: 0.9000\n",
            "Epoch: 1083, Accuracy: 0.9000\n",
            "Epoch: 1084, Accuracy: 0.9000\n",
            "Epoch: 1085, Accuracy: 0.9000\n",
            "Epoch: 1086, Accuracy: 0.9000\n",
            "Epoch: 1087, Accuracy: 0.9000\n",
            "Epoch: 1088, Accuracy: 0.9000\n",
            "Epoch: 1089, Accuracy: 0.9000\n",
            "Epoch: 1090, Accuracy: 0.9000\n",
            "Epoch: 1091, Accuracy: 0.9000\n",
            "Epoch: 1092, Accuracy: 0.9000\n",
            "Epoch: 1093, Accuracy: 0.9000\n",
            "Epoch: 1094, Accuracy: 0.9000\n",
            "Epoch: 1095, Accuracy: 0.9000\n",
            "Epoch: 1096, Accuracy: 0.9000\n",
            "Epoch: 1097, Accuracy: 0.9000\n",
            "Epoch: 1098, Accuracy: 0.9000\n",
            "Epoch: 1099, Accuracy: 0.9000\n",
            "Epoch: 1100, Accuracy: 0.9000\n",
            "Epoch: 1101, Accuracy: 0.9000\n",
            "Epoch: 1102, Accuracy: 0.9000\n",
            "Epoch: 1103, Accuracy: 0.9000\n",
            "Epoch: 1104, Accuracy: 0.9000\n",
            "Epoch: 1105, Accuracy: 0.9000\n",
            "Epoch: 1106, Accuracy: 0.9000\n",
            "Epoch: 1107, Accuracy: 0.9000\n",
            "Epoch: 1108, Accuracy: 0.9000\n",
            "Epoch: 1109, Accuracy: 0.9000\n",
            "Epoch: 1110, Accuracy: 0.9000\n",
            "Epoch: 1111, Accuracy: 0.9000\n",
            "Epoch: 1112, Accuracy: 0.9000\n",
            "Epoch: 1113, Accuracy: 0.9000\n",
            "Epoch: 1114, Accuracy: 0.9000\n",
            "Epoch: 1115, Accuracy: 0.9000\n",
            "Epoch: 1116, Accuracy: 0.9000\n",
            "Epoch: 1117, Accuracy: 0.9000\n",
            "Epoch: 1118, Accuracy: 0.9000\n",
            "Epoch: 1119, Accuracy: 0.8800\n",
            "Epoch: 1120, Accuracy: 0.8800\n",
            "Epoch: 1121, Accuracy: 0.9000\n",
            "Epoch: 1122, Accuracy: 0.9000\n",
            "Epoch: 1123, Accuracy: 0.8800\n",
            "Epoch: 1124, Accuracy: 0.9200\n",
            "Epoch: 1125, Accuracy: 0.9000\n",
            "Epoch: 1126, Accuracy: 0.8800\n",
            "Epoch: 1127, Accuracy: 0.9000\n",
            "Epoch: 1128, Accuracy: 0.9000\n",
            "Epoch: 1129, Accuracy: 0.9000\n",
            "Epoch: 1130, Accuracy: 0.9000\n",
            "Epoch: 1131, Accuracy: 0.9000\n",
            "Epoch: 1132, Accuracy: 0.9000\n",
            "Epoch: 1133, Accuracy: 0.9000\n",
            "Epoch: 1134, Accuracy: 0.8800\n",
            "Epoch: 1135, Accuracy: 0.8800\n",
            "Epoch: 1136, Accuracy: 0.8800\n",
            "Epoch: 1137, Accuracy: 0.9000\n",
            "Epoch: 1138, Accuracy: 0.9000\n",
            "Epoch: 1139, Accuracy: 0.9000\n",
            "Epoch: 1140, Accuracy: 0.9000\n",
            "Epoch: 1141, Accuracy: 0.9000\n",
            "Epoch: 1142, Accuracy: 0.8800\n",
            "Epoch: 1143, Accuracy: 0.8800\n",
            "Epoch: 1144, Accuracy: 0.9000\n",
            "Epoch: 1145, Accuracy: 0.9000\n",
            "Epoch: 1146, Accuracy: 0.9000\n",
            "Epoch: 1147, Accuracy: 0.8800\n",
            "Epoch: 1148, Accuracy: 0.8600\n",
            "Epoch: 1149, Accuracy: 0.8800\n",
            "Epoch: 1150, Accuracy: 0.9000\n",
            "Epoch: 1151, Accuracy: 0.9000\n",
            "Epoch: 1152, Accuracy: 0.9000\n",
            "Epoch: 1153, Accuracy: 0.8400\n",
            "Epoch: 1154, Accuracy: 0.8800\n",
            "Epoch: 1155, Accuracy: 0.9000\n",
            "Epoch: 1156, Accuracy: 0.9000\n",
            "Epoch: 1157, Accuracy: 0.9000\n",
            "Epoch: 1158, Accuracy: 0.9000\n",
            "Epoch: 1159, Accuracy: 0.8600\n",
            "Epoch: 1160, Accuracy: 0.8800\n",
            "Epoch: 1161, Accuracy: 0.8800\n",
            "Epoch: 1162, Accuracy: 0.8800\n",
            "Epoch: 1163, Accuracy: 0.9000\n",
            "Epoch: 1164, Accuracy: 0.8800\n",
            "Epoch: 1165, Accuracy: 0.8800\n",
            "Epoch: 1166, Accuracy: 0.9000\n",
            "Epoch: 1167, Accuracy: 0.9000\n",
            "Epoch: 1168, Accuracy: 0.9000\n",
            "Epoch: 1169, Accuracy: 0.8800\n",
            "Epoch: 1170, Accuracy: 0.8800\n",
            "Epoch: 1171, Accuracy: 0.9200\n",
            "Epoch: 1172, Accuracy: 0.8600\n",
            "Epoch: 1173, Accuracy: 0.8800\n",
            "Epoch: 1174, Accuracy: 0.8800\n",
            "Epoch: 1175, Accuracy: 0.8800\n",
            "Epoch: 1176, Accuracy: 0.8600\n",
            "Epoch: 1177, Accuracy: 0.8600\n",
            "Epoch: 1178, Accuracy: 0.8600\n",
            "Epoch: 1179, Accuracy: 0.8600\n",
            "Epoch: 1180, Accuracy: 0.8800\n",
            "Epoch: 1181, Accuracy: 0.8600\n",
            "Epoch: 1182, Accuracy: 0.9000\n",
            "Epoch: 1183, Accuracy: 0.8400\n",
            "Epoch: 1184, Accuracy: 0.8600\n",
            "Epoch: 1185, Accuracy: 0.8600\n",
            "Epoch: 1186, Accuracy: 0.8800\n",
            "Epoch: 1187, Accuracy: 0.8400\n",
            "Epoch: 1188, Accuracy: 0.8600\n",
            "Epoch: 1189, Accuracy: 0.8600\n",
            "Epoch: 1190, Accuracy: 0.8800\n",
            "Epoch: 1191, Accuracy: 0.8200\n",
            "Epoch: 1192, Accuracy: 0.8800\n",
            "Epoch: 1193, Accuracy: 0.8400\n",
            "Epoch: 1194, Accuracy: 0.8800\n",
            "Epoch: 1195, Accuracy: 0.8200\n",
            "Epoch: 1196, Accuracy: 0.8600\n",
            "Epoch: 1197, Accuracy: 0.8400\n",
            "Epoch: 1198, Accuracy: 0.8400\n",
            "Epoch: 1199, Accuracy: 0.8400\n",
            "Epoch: 1200, Accuracy: 0.8400\n",
            "Epoch: 1201, Accuracy: 0.1000\n",
            "Epoch: 1202, Accuracy: 0.1000\n",
            "Epoch: 1203, Accuracy: 0.1200\n",
            "Epoch: 1204, Accuracy: 0.3400\n",
            "Epoch: 1205, Accuracy: 0.9000\n",
            "Epoch: 1206, Accuracy: 0.9000\n",
            "Epoch: 1207, Accuracy: 0.9000\n",
            "Epoch: 1208, Accuracy: 0.9000\n",
            "Epoch: 1209, Accuracy: 0.9000\n",
            "Epoch: 1210, Accuracy: 0.9000\n",
            "Epoch: 1211, Accuracy: 0.9000\n",
            "Epoch: 1212, Accuracy: 0.9000\n",
            "Epoch: 1213, Accuracy: 0.9000\n",
            "Epoch: 1214, Accuracy: 0.9000\n",
            "Epoch: 1215, Accuracy: 0.9000\n",
            "Epoch: 1216, Accuracy: 0.9000\n",
            "Epoch: 1217, Accuracy: 0.9000\n",
            "Epoch: 1218, Accuracy: 0.9000\n",
            "Epoch: 1219, Accuracy: 0.9000\n",
            "Epoch: 1220, Accuracy: 0.9000\n",
            "Epoch: 1221, Accuracy: 0.9000\n",
            "Epoch: 1222, Accuracy: 0.9000\n",
            "Epoch: 1223, Accuracy: 0.9000\n",
            "Epoch: 1224, Accuracy: 0.9000\n",
            "Epoch: 1225, Accuracy: 0.9000\n",
            "Epoch: 1226, Accuracy: 0.9000\n",
            "Epoch: 1227, Accuracy: 0.9000\n",
            "Epoch: 1228, Accuracy: 0.9000\n",
            "Epoch: 1229, Accuracy: 0.9000\n",
            "Epoch: 1230, Accuracy: 0.9000\n",
            "Epoch: 1231, Accuracy: 0.9000\n",
            "Epoch: 1232, Accuracy: 0.9000\n",
            "Epoch: 1233, Accuracy: 0.9000\n",
            "Epoch: 1234, Accuracy: 0.9000\n",
            "Epoch: 1235, Accuracy: 0.9000\n",
            "Epoch: 1236, Accuracy: 0.9000\n",
            "Epoch: 1237, Accuracy: 0.9000\n",
            "Epoch: 1238, Accuracy: 0.9000\n",
            "Epoch: 1239, Accuracy: 0.9000\n",
            "Epoch: 1240, Accuracy: 0.9000\n",
            "Epoch: 1241, Accuracy: 0.9000\n",
            "Epoch: 1242, Accuracy: 0.9000\n",
            "Epoch: 1243, Accuracy: 0.9000\n",
            "Epoch: 1244, Accuracy: 0.9000\n",
            "Epoch: 1245, Accuracy: 0.9000\n",
            "Epoch: 1246, Accuracy: 0.9000\n",
            "Epoch: 1247, Accuracy: 0.9000\n",
            "Epoch: 1248, Accuracy: 0.9000\n",
            "Epoch: 1249, Accuracy: 0.9000\n",
            "Epoch: 1250, Accuracy: 0.9000\n",
            "Epoch: 1251, Accuracy: 0.9000\n",
            "Epoch: 1252, Accuracy: 0.9000\n",
            "Epoch: 1253, Accuracy: 0.9000\n",
            "Epoch: 1254, Accuracy: 0.9000\n",
            "Epoch: 1255, Accuracy: 0.9000\n",
            "Epoch: 1256, Accuracy: 0.9000\n",
            "Epoch: 1257, Accuracy: 0.9000\n",
            "Epoch: 1258, Accuracy: 0.9000\n",
            "Epoch: 1259, Accuracy: 0.9000\n",
            "Epoch: 1260, Accuracy: 0.9000\n",
            "Epoch: 1261, Accuracy: 0.9000\n",
            "Epoch: 1262, Accuracy: 0.9000\n",
            "Epoch: 1263, Accuracy: 0.9000\n",
            "Epoch: 1264, Accuracy: 0.9000\n",
            "Epoch: 1265, Accuracy: 0.9000\n",
            "Epoch: 1266, Accuracy: 0.9000\n",
            "Epoch: 1267, Accuracy: 0.9000\n",
            "Epoch: 1268, Accuracy: 0.9000\n",
            "Epoch: 1269, Accuracy: 0.9000\n",
            "Epoch: 1270, Accuracy: 0.9000\n",
            "Epoch: 1271, Accuracy: 0.9000\n",
            "Epoch: 1272, Accuracy: 0.9000\n",
            "Epoch: 1273, Accuracy: 0.9000\n",
            "Epoch: 1274, Accuracy: 0.9000\n",
            "Epoch: 1275, Accuracy: 0.9000\n",
            "Epoch: 1276, Accuracy: 0.9000\n",
            "Epoch: 1277, Accuracy: 0.9000\n",
            "Epoch: 1278, Accuracy: 0.9000\n",
            "Epoch: 1279, Accuracy: 0.9000\n",
            "Epoch: 1280, Accuracy: 0.9000\n",
            "Epoch: 1281, Accuracy: 0.9000\n",
            "Epoch: 1282, Accuracy: 0.9200\n",
            "Epoch: 1283, Accuracy: 0.9200\n",
            "Epoch: 1284, Accuracy: 0.9000\n",
            "Epoch: 1285, Accuracy: 0.9000\n",
            "Epoch: 1286, Accuracy: 0.9200\n",
            "Epoch: 1287, Accuracy: 0.9200\n",
            "Epoch: 1288, Accuracy: 0.9000\n",
            "Epoch: 1289, Accuracy: 0.9000\n",
            "Epoch: 1290, Accuracy: 0.9000\n",
            "Epoch: 1291, Accuracy: 0.9200\n",
            "Epoch: 1292, Accuracy: 0.9000\n",
            "Epoch: 1293, Accuracy: 0.9200\n",
            "Epoch: 1294, Accuracy: 0.9200\n",
            "Epoch: 1295, Accuracy: 0.9200\n",
            "Epoch: 1296, Accuracy: 0.9200\n",
            "Epoch: 1297, Accuracy: 0.9000\n",
            "Epoch: 1298, Accuracy: 0.9000\n",
            "Epoch: 1299, Accuracy: 0.9200\n",
            "Epoch: 1300, Accuracy: 0.9000\n",
            "Epoch: 1301, Accuracy: 0.9000\n",
            "Epoch: 1302, Accuracy: 0.9000\n",
            "Epoch: 1303, Accuracy: 0.9200\n",
            "Epoch: 1304, Accuracy: 0.9200\n",
            "Epoch: 1305, Accuracy: 0.9200\n",
            "Epoch: 1306, Accuracy: 0.9200\n",
            "Epoch: 1307, Accuracy: 0.9400\n",
            "Epoch: 1308, Accuracy: 0.9200\n",
            "Epoch: 1309, Accuracy: 0.9000\n",
            "Epoch: 1310, Accuracy: 0.8800\n",
            "Epoch: 1311, Accuracy: 0.9200\n",
            "Epoch: 1312, Accuracy: 0.9000\n",
            "Epoch: 1313, Accuracy: 0.9200\n",
            "Epoch: 1314, Accuracy: 0.9200\n",
            "Epoch: 1315, Accuracy: 0.9200\n",
            "Epoch: 1316, Accuracy: 0.9200\n",
            "Epoch: 1317, Accuracy: 0.9000\n",
            "Epoch: 1318, Accuracy: 0.9000\n",
            "Epoch: 1319, Accuracy: 0.9000\n",
            "Epoch: 1320, Accuracy: 0.9000\n",
            "Epoch: 1321, Accuracy: 0.9000\n",
            "Epoch: 1322, Accuracy: 0.9000\n",
            "Epoch: 1323, Accuracy: 0.9000\n",
            "Epoch: 1324, Accuracy: 0.9000\n",
            "Epoch: 1325, Accuracy: 0.9000\n",
            "Epoch: 1326, Accuracy: 0.8800\n",
            "Epoch: 1327, Accuracy: 0.8800\n",
            "Epoch: 1328, Accuracy: 0.8800\n",
            "Epoch: 1329, Accuracy: 0.8800\n",
            "Epoch: 1330, Accuracy: 0.9000\n",
            "Epoch: 1331, Accuracy: 0.9000\n",
            "Epoch: 1332, Accuracy: 0.9000\n",
            "Epoch: 1333, Accuracy: 0.9000\n",
            "Epoch: 1334, Accuracy: 0.9000\n",
            "Epoch: 1335, Accuracy: 0.9000\n",
            "Epoch: 1336, Accuracy: 0.9000\n",
            "Epoch: 1337, Accuracy: 0.9000\n",
            "Epoch: 1338, Accuracy: 0.9000\n",
            "Epoch: 1339, Accuracy: 0.9200\n",
            "Epoch: 1340, Accuracy: 0.9000\n",
            "Epoch: 1341, Accuracy: 0.9000\n",
            "Epoch: 1342, Accuracy: 0.9200\n",
            "Epoch: 1343, Accuracy: 0.9200\n",
            "Epoch: 1344, Accuracy: 0.9200\n",
            "Epoch: 1345, Accuracy: 0.9000\n",
            "Epoch: 1346, Accuracy: 0.9200\n",
            "Epoch: 1347, Accuracy: 0.9200\n",
            "Epoch: 1348, Accuracy: 0.9200\n",
            "Epoch: 1349, Accuracy: 0.9200\n",
            "Epoch: 1350, Accuracy: 0.9200\n",
            "Epoch: 1351, Accuracy: 0.9200\n",
            "Epoch: 1352, Accuracy: 0.9200\n",
            "Epoch: 1353, Accuracy: 0.9200\n",
            "Epoch: 1354, Accuracy: 0.9200\n",
            "Epoch: 1355, Accuracy: 0.9200\n",
            "Epoch: 1356, Accuracy: 0.9200\n",
            "Epoch: 1357, Accuracy: 0.9200\n",
            "Epoch: 1358, Accuracy: 0.9200\n",
            "Epoch: 1359, Accuracy: 0.9200\n",
            "Epoch: 1360, Accuracy: 0.9200\n",
            "Epoch: 1361, Accuracy: 0.9200\n",
            "Epoch: 1362, Accuracy: 0.9400\n",
            "Epoch: 1363, Accuracy: 0.9200\n",
            "Epoch: 1364, Accuracy: 0.9200\n",
            "Epoch: 1365, Accuracy: 0.9200\n",
            "Epoch: 1366, Accuracy: 0.9200\n",
            "Epoch: 1367, Accuracy: 0.9000\n",
            "Epoch: 1368, Accuracy: 0.9200\n",
            "Epoch: 1369, Accuracy: 0.9200\n",
            "Epoch: 1370, Accuracy: 0.9200\n",
            "Epoch: 1371, Accuracy: 0.9200\n",
            "Epoch: 1372, Accuracy: 0.9200\n",
            "Epoch: 1373, Accuracy: 0.9200\n",
            "Epoch: 1374, Accuracy: 0.9200\n",
            "Epoch: 1375, Accuracy: 0.9000\n",
            "Epoch: 1376, Accuracy: 0.9200\n",
            "Epoch: 1377, Accuracy: 0.9200\n",
            "Epoch: 1378, Accuracy: 0.9200\n",
            "Epoch: 1379, Accuracy: 0.9200\n",
            "Epoch: 1380, Accuracy: 0.9200\n",
            "Epoch: 1381, Accuracy: 0.9200\n",
            "Epoch: 1382, Accuracy: 0.9200\n",
            "Epoch: 1383, Accuracy: 0.9200\n",
            "Epoch: 1384, Accuracy: 0.9200\n",
            "Epoch: 1385, Accuracy: 0.9200\n",
            "Epoch: 1386, Accuracy: 0.9200\n",
            "Epoch: 1387, Accuracy: 0.9200\n",
            "Epoch: 1388, Accuracy: 0.9200\n",
            "Epoch: 1389, Accuracy: 0.9200\n",
            "Epoch: 1390, Accuracy: 0.9200\n",
            "Epoch: 1391, Accuracy: 0.9200\n",
            "Epoch: 1392, Accuracy: 0.9200\n",
            "Epoch: 1393, Accuracy: 0.9200\n",
            "Epoch: 1394, Accuracy: 0.9200\n",
            "Epoch: 1395, Accuracy: 0.9200\n",
            "Epoch: 1396, Accuracy: 0.9200\n",
            "Epoch: 1397, Accuracy: 0.9400\n",
            "Epoch: 1398, Accuracy: 0.9200\n",
            "Epoch: 1399, Accuracy: 0.9200\n",
            "Epoch: 1400, Accuracy: 0.9200\n",
            "Epoch: 1401, Accuracy: 0.9000\n",
            "Epoch: 1402, Accuracy: 0.9000\n",
            "Epoch: 1403, Accuracy: 0.9000\n",
            "Epoch: 1404, Accuracy: 0.9000\n",
            "Epoch: 1405, Accuracy: 0.9000\n",
            "Epoch: 1406, Accuracy: 0.9000\n",
            "Epoch: 1407, Accuracy: 0.9000\n",
            "Epoch: 1408, Accuracy: 0.9000\n",
            "Epoch: 1409, Accuracy: 0.9000\n",
            "Epoch: 1410, Accuracy: 0.9000\n",
            "Epoch: 1411, Accuracy: 0.9000\n",
            "Epoch: 1412, Accuracy: 0.9000\n",
            "Epoch: 1413, Accuracy: 0.9000\n",
            "Epoch: 1414, Accuracy: 0.9000\n",
            "Epoch: 1415, Accuracy: 0.9000\n",
            "Epoch: 1416, Accuracy: 0.9000\n",
            "Epoch: 1417, Accuracy: 0.9000\n",
            "Epoch: 1418, Accuracy: 0.9000\n",
            "Epoch: 1419, Accuracy: 0.9000\n",
            "Epoch: 1420, Accuracy: 0.9000\n",
            "Epoch: 1421, Accuracy: 0.9000\n",
            "Epoch: 1422, Accuracy: 0.9000\n",
            "Epoch: 1423, Accuracy: 0.9000\n",
            "Epoch: 1424, Accuracy: 0.9000\n",
            "Epoch: 1425, Accuracy: 0.9000\n",
            "Epoch: 1426, Accuracy: 0.9000\n",
            "Epoch: 1427, Accuracy: 0.9000\n",
            "Epoch: 1428, Accuracy: 0.9000\n",
            "Epoch: 1429, Accuracy: 0.9000\n",
            "Epoch: 1430, Accuracy: 0.9000\n",
            "Epoch: 1431, Accuracy: 0.9000\n",
            "Epoch: 1432, Accuracy: 0.9000\n",
            "Epoch: 1433, Accuracy: 0.9000\n",
            "Epoch: 1434, Accuracy: 0.9000\n",
            "Epoch: 1435, Accuracy: 0.9000\n",
            "Epoch: 1436, Accuracy: 0.9000\n",
            "Epoch: 1437, Accuracy: 0.9000\n",
            "Epoch: 1438, Accuracy: 0.9000\n",
            "Epoch: 1439, Accuracy: 0.9000\n",
            "Epoch: 1440, Accuracy: 0.9000\n",
            "Epoch: 1441, Accuracy: 0.9000\n",
            "Epoch: 1442, Accuracy: 0.9000\n",
            "Epoch: 1443, Accuracy: 0.9000\n",
            "Epoch: 1444, Accuracy: 0.9000\n",
            "Epoch: 1445, Accuracy: 0.9000\n",
            "Epoch: 1446, Accuracy: 0.9000\n",
            "Epoch: 1447, Accuracy: 0.9000\n",
            "Epoch: 1448, Accuracy: 0.9000\n",
            "Epoch: 1449, Accuracy: 0.9000\n",
            "Epoch: 1450, Accuracy: 0.9000\n",
            "Epoch: 1451, Accuracy: 0.9000\n",
            "Epoch: 1452, Accuracy: 0.9000\n",
            "Epoch: 1453, Accuracy: 0.9000\n",
            "Epoch: 1454, Accuracy: 0.9000\n",
            "Epoch: 1455, Accuracy: 0.9000\n",
            "Epoch: 1456, Accuracy: 0.9000\n",
            "Epoch: 1457, Accuracy: 0.9000\n",
            "Epoch: 1458, Accuracy: 0.9000\n",
            "Epoch: 1459, Accuracy: 0.9000\n",
            "Epoch: 1460, Accuracy: 0.9000\n",
            "Epoch: 1461, Accuracy: 0.9000\n",
            "Epoch: 1462, Accuracy: 0.9000\n",
            "Epoch: 1463, Accuracy: 0.9000\n",
            "Epoch: 1464, Accuracy: 0.9000\n",
            "Epoch: 1465, Accuracy: 0.9000\n",
            "Epoch: 1466, Accuracy: 0.9000\n",
            "Epoch: 1467, Accuracy: 0.9000\n",
            "Epoch: 1468, Accuracy: 0.9000\n",
            "Epoch: 1469, Accuracy: 0.9000\n",
            "Epoch: 1470, Accuracy: 0.9000\n",
            "Epoch: 1471, Accuracy: 0.9000\n",
            "Epoch: 1472, Accuracy: 0.9000\n",
            "Epoch: 1473, Accuracy: 0.9000\n",
            "Epoch: 1474, Accuracy: 0.9000\n",
            "Epoch: 1475, Accuracy: 0.9000\n",
            "Epoch: 1476, Accuracy: 0.9000\n",
            "Epoch: 1477, Accuracy: 0.9000\n",
            "Epoch: 1478, Accuracy: 0.9000\n",
            "Epoch: 1479, Accuracy: 0.9200\n",
            "Epoch: 1480, Accuracy: 0.9000\n",
            "Epoch: 1481, Accuracy: 0.9000\n",
            "Epoch: 1482, Accuracy: 0.9000\n",
            "Epoch: 1483, Accuracy: 0.9000\n",
            "Epoch: 1484, Accuracy: 0.8800\n",
            "Epoch: 1485, Accuracy: 0.8800\n",
            "Epoch: 1486, Accuracy: 0.9000\n",
            "Epoch: 1487, Accuracy: 0.9000\n",
            "Epoch: 1488, Accuracy: 0.9000\n",
            "Epoch: 1489, Accuracy: 0.9000\n",
            "Epoch: 1490, Accuracy: 0.8800\n",
            "Epoch: 1491, Accuracy: 0.9200\n",
            "Epoch: 1492, Accuracy: 0.8600\n",
            "Epoch: 1493, Accuracy: 0.8800\n",
            "Epoch: 1494, Accuracy: 0.9000\n",
            "Epoch: 1495, Accuracy: 0.8800\n",
            "Epoch: 1496, Accuracy: 0.9000\n",
            "Epoch: 1497, Accuracy: 0.9000\n",
            "Epoch: 1498, Accuracy: 0.8800\n",
            "Epoch: 1499, Accuracy: 0.8600\n",
            "Epoch: 1500, Accuracy: 0.8800\n",
            "Epoch: 1501, Accuracy: 0.8800\n",
            "Epoch: 1502, Accuracy: 0.9200\n",
            "Epoch: 1503, Accuracy: 0.9200\n",
            "Epoch: 1504, Accuracy: 0.9000\n",
            "Epoch: 1505, Accuracy: 0.9000\n",
            "Epoch: 1506, Accuracy: 0.9200\n",
            "Epoch: 1507, Accuracy: 0.9200\n",
            "Epoch: 1508, Accuracy: 0.9200\n",
            "Epoch: 1509, Accuracy: 0.8800\n",
            "Epoch: 1510, Accuracy: 0.9000\n",
            "Epoch: 1511, Accuracy: 0.9200\n",
            "Epoch: 1512, Accuracy: 0.9200\n",
            "Epoch: 1513, Accuracy: 0.9200\n",
            "Epoch: 1514, Accuracy: 0.9200\n",
            "Epoch: 1515, Accuracy: 0.9000\n",
            "Epoch: 1516, Accuracy: 0.9000\n",
            "Epoch: 1517, Accuracy: 0.9200\n",
            "Epoch: 1518, Accuracy: 0.9000\n",
            "Epoch: 1519, Accuracy: 0.9200\n",
            "Epoch: 1520, Accuracy: 0.9200\n",
            "Epoch: 1521, Accuracy: 0.9200\n",
            "Epoch: 1522, Accuracy: 0.9200\n",
            "Epoch: 1523, Accuracy: 0.9200\n",
            "Epoch: 1524, Accuracy: 0.9200\n",
            "Epoch: 1525, Accuracy: 0.9200\n",
            "Epoch: 1526, Accuracy: 0.9200\n",
            "Epoch: 1527, Accuracy: 0.8400\n",
            "Epoch: 1528, Accuracy: 0.9200\n",
            "Epoch: 1529, Accuracy: 0.9000\n",
            "Epoch: 1530, Accuracy: 0.9000\n",
            "Epoch: 1531, Accuracy: 0.9000\n",
            "Epoch: 1532, Accuracy: 0.9200\n",
            "Epoch: 1533, Accuracy: 0.8800\n",
            "Epoch: 1534, Accuracy: 0.8800\n",
            "Epoch: 1535, Accuracy: 0.9200\n",
            "Epoch: 1536, Accuracy: 0.9200\n",
            "Epoch: 1537, Accuracy: 0.9200\n",
            "Epoch: 1538, Accuracy: 0.9200\n",
            "Epoch: 1539, Accuracy: 0.9200\n",
            "Epoch: 1540, Accuracy: 0.9200\n",
            "Epoch: 1541, Accuracy: 0.9200\n",
            "Epoch: 1542, Accuracy: 0.9000\n",
            "Epoch: 1543, Accuracy: 0.9200\n",
            "Epoch: 1544, Accuracy: 0.9200\n",
            "Epoch: 1545, Accuracy: 0.9200\n",
            "Epoch: 1546, Accuracy: 0.9200\n",
            "Epoch: 1547, Accuracy: 0.9200\n",
            "Epoch: 1548, Accuracy: 0.9200\n",
            "Epoch: 1549, Accuracy: 0.9200\n",
            "Epoch: 1550, Accuracy: 0.9200\n",
            "Epoch: 1551, Accuracy: 0.9200\n",
            "Epoch: 1552, Accuracy: 0.9200\n",
            "Epoch: 1553, Accuracy: 0.9200\n",
            "Epoch: 1554, Accuracy: 0.9200\n",
            "Epoch: 1555, Accuracy: 0.9200\n",
            "Epoch: 1556, Accuracy: 0.9200\n",
            "Epoch: 1557, Accuracy: 0.9200\n",
            "Epoch: 1558, Accuracy: 0.9200\n",
            "Epoch: 1559, Accuracy: 0.9200\n",
            "Epoch: 1560, Accuracy: 0.9400\n",
            "Epoch: 1561, Accuracy: 0.9200\n",
            "Epoch: 1562, Accuracy: 0.9200\n",
            "Epoch: 1563, Accuracy: 0.9200\n",
            "Epoch: 1564, Accuracy: 0.9200\n",
            "Epoch: 1565, Accuracy: 0.9200\n",
            "Epoch: 1566, Accuracy: 0.9200\n",
            "Epoch: 1567, Accuracy: 0.9200\n",
            "Epoch: 1568, Accuracy: 0.9400\n",
            "Epoch: 1569, Accuracy: 0.9200\n",
            "Epoch: 1570, Accuracy: 0.9200\n",
            "Epoch: 1571, Accuracy: 0.9200\n",
            "Epoch: 1572, Accuracy: 0.9200\n",
            "Epoch: 1573, Accuracy: 0.9200\n",
            "Epoch: 1574, Accuracy: 0.9200\n",
            "Epoch: 1575, Accuracy: 0.9200\n",
            "Epoch: 1576, Accuracy: 0.9200\n",
            "Epoch: 1577, Accuracy: 0.9200\n",
            "Epoch: 1578, Accuracy: 0.9200\n",
            "Epoch: 1579, Accuracy: 0.9200\n",
            "Epoch: 1580, Accuracy: 0.9200\n",
            "Epoch: 1581, Accuracy: 0.9200\n",
            "Epoch: 1582, Accuracy: 0.9200\n",
            "Epoch: 1583, Accuracy: 0.9200\n",
            "Epoch: 1584, Accuracy: 0.9200\n",
            "Epoch: 1585, Accuracy: 0.9200\n",
            "Epoch: 1586, Accuracy: 0.9200\n",
            "Epoch: 1587, Accuracy: 0.9200\n",
            "Epoch: 1588, Accuracy: 0.9200\n",
            "Epoch: 1589, Accuracy: 0.9200\n",
            "Epoch: 1590, Accuracy: 0.9200\n",
            "Epoch: 1591, Accuracy: 0.9200\n",
            "Epoch: 1592, Accuracy: 0.9200\n",
            "Epoch: 1593, Accuracy: 0.9200\n",
            "Epoch: 1594, Accuracy: 0.9200\n",
            "Epoch: 1595, Accuracy: 0.9200\n",
            "Epoch: 1596, Accuracy: 0.9200\n",
            "Epoch: 1597, Accuracy: 0.9200\n",
            "Epoch: 1598, Accuracy: 0.9200\n",
            "Epoch: 1599, Accuracy: 0.9200\n",
            "Epoch: 1600, Accuracy: 0.9200\n",
            "Epoch: 1601, Accuracy: 0.9000\n",
            "Epoch: 1602, Accuracy: 0.9000\n",
            "Epoch: 1603, Accuracy: 0.9000\n",
            "Epoch: 1604, Accuracy: 0.9000\n",
            "Epoch: 1605, Accuracy: 0.9000\n",
            "Epoch: 1606, Accuracy: 0.9000\n",
            "Epoch: 1607, Accuracy: 0.9000\n",
            "Epoch: 1608, Accuracy: 0.9000\n",
            "Epoch: 1609, Accuracy: 0.9000\n",
            "Epoch: 1610, Accuracy: 0.9000\n",
            "Epoch: 1611, Accuracy: 0.9000\n",
            "Epoch: 1612, Accuracy: 0.9000\n",
            "Epoch: 1613, Accuracy: 0.9000\n",
            "Epoch: 1614, Accuracy: 0.9000\n",
            "Epoch: 1615, Accuracy: 0.9000\n",
            "Epoch: 1616, Accuracy: 0.9000\n",
            "Epoch: 1617, Accuracy: 0.9000\n",
            "Epoch: 1618, Accuracy: 0.9000\n",
            "Epoch: 1619, Accuracy: 0.9000\n",
            "Epoch: 1620, Accuracy: 0.9000\n",
            "Epoch: 1621, Accuracy: 0.9000\n",
            "Epoch: 1622, Accuracy: 0.9000\n",
            "Epoch: 1623, Accuracy: 0.9000\n",
            "Epoch: 1624, Accuracy: 0.9000\n",
            "Epoch: 1625, Accuracy: 0.9000\n",
            "Epoch: 1626, Accuracy: 0.9000\n",
            "Epoch: 1627, Accuracy: 0.9000\n",
            "Epoch: 1628, Accuracy: 0.9000\n",
            "Epoch: 1629, Accuracy: 0.9000\n",
            "Epoch: 1630, Accuracy: 0.9000\n",
            "Epoch: 1631, Accuracy: 0.9000\n",
            "Epoch: 1632, Accuracy: 0.9000\n",
            "Epoch: 1633, Accuracy: 0.9000\n",
            "Epoch: 1634, Accuracy: 0.9000\n",
            "Epoch: 1635, Accuracy: 0.9000\n",
            "Epoch: 1636, Accuracy: 0.9000\n",
            "Epoch: 1637, Accuracy: 0.9000\n",
            "Epoch: 1638, Accuracy: 0.9000\n",
            "Epoch: 1639, Accuracy: 0.9000\n",
            "Epoch: 1640, Accuracy: 0.9000\n",
            "Epoch: 1641, Accuracy: 0.9000\n",
            "Epoch: 1642, Accuracy: 0.9000\n",
            "Epoch: 1643, Accuracy: 0.9000\n",
            "Epoch: 1644, Accuracy: 0.9200\n",
            "Epoch: 1645, Accuracy: 0.9000\n",
            "Epoch: 1646, Accuracy: 0.9200\n",
            "Epoch: 1647, Accuracy: 0.9200\n",
            "Epoch: 1648, Accuracy: 0.9200\n",
            "Epoch: 1649, Accuracy: 0.9000\n",
            "Epoch: 1650, Accuracy: 0.9000\n",
            "Epoch: 1651, Accuracy: 0.9200\n",
            "Epoch: 1652, Accuracy: 0.9200\n",
            "Epoch: 1653, Accuracy: 0.9200\n",
            "Epoch: 1654, Accuracy: 0.9200\n",
            "Epoch: 1655, Accuracy: 0.9200\n",
            "Epoch: 1656, Accuracy: 0.9200\n",
            "Epoch: 1657, Accuracy: 0.9200\n",
            "Epoch: 1658, Accuracy: 0.9200\n",
            "Epoch: 1659, Accuracy: 0.9200\n",
            "Epoch: 1660, Accuracy: 0.9200\n",
            "Epoch: 1661, Accuracy: 0.9200\n",
            "Epoch: 1662, Accuracy: 0.9200\n",
            "Epoch: 1663, Accuracy: 0.9200\n",
            "Epoch: 1664, Accuracy: 0.9200\n",
            "Epoch: 1665, Accuracy: 0.9200\n",
            "Epoch: 1666, Accuracy: 0.9200\n",
            "Epoch: 1667, Accuracy: 0.9200\n",
            "Epoch: 1668, Accuracy: 0.9200\n",
            "Epoch: 1669, Accuracy: 0.9200\n",
            "Epoch: 1670, Accuracy: 0.9200\n",
            "Epoch: 1671, Accuracy: 0.9200\n",
            "Epoch: 1672, Accuracy: 0.9200\n",
            "Epoch: 1673, Accuracy: 0.9200\n",
            "Epoch: 1674, Accuracy: 0.9200\n",
            "Epoch: 1675, Accuracy: 0.9000\n",
            "Epoch: 1676, Accuracy: 0.9200\n",
            "Epoch: 1677, Accuracy: 0.9200\n",
            "Epoch: 1678, Accuracy: 0.9200\n",
            "Epoch: 1679, Accuracy: 0.9200\n",
            "Epoch: 1680, Accuracy: 0.9000\n",
            "Epoch: 1681, Accuracy: 0.9000\n",
            "Epoch: 1682, Accuracy: 0.9200\n",
            "Epoch: 1683, Accuracy: 0.9200\n",
            "Epoch: 1684, Accuracy: 0.9200\n",
            "Epoch: 1685, Accuracy: 0.9200\n",
            "Epoch: 1686, Accuracy: 0.9000\n",
            "Epoch: 1687, Accuracy: 0.9000\n",
            "Epoch: 1688, Accuracy: 0.9000\n",
            "Epoch: 1689, Accuracy: 0.9200\n",
            "Epoch: 1690, Accuracy: 0.9200\n",
            "Epoch: 1691, Accuracy: 0.9200\n",
            "Epoch: 1692, Accuracy: 0.9200\n",
            "Epoch: 1693, Accuracy: 0.9000\n",
            "Epoch: 1694, Accuracy: 0.9000\n",
            "Epoch: 1695, Accuracy: 0.9000\n",
            "Epoch: 1696, Accuracy: 0.9000\n",
            "Epoch: 1697, Accuracy: 0.9000\n",
            "Epoch: 1698, Accuracy: 0.9200\n",
            "Epoch: 1699, Accuracy: 0.9000\n",
            "Epoch: 1700, Accuracy: 0.9000\n",
            "Epoch: 1701, Accuracy: 0.9000\n",
            "Epoch: 1702, Accuracy: 0.9000\n",
            "Epoch: 1703, Accuracy: 0.9000\n",
            "Epoch: 1704, Accuracy: 0.9000\n",
            "Epoch: 1705, Accuracy: 0.9000\n",
            "Epoch: 1706, Accuracy: 0.9000\n",
            "Epoch: 1707, Accuracy: 0.9000\n",
            "Epoch: 1708, Accuracy: 0.9000\n",
            "Epoch: 1709, Accuracy: 0.9000\n",
            "Epoch: 1710, Accuracy: 0.9000\n",
            "Epoch: 1711, Accuracy: 0.9000\n",
            "Epoch: 1712, Accuracy: 0.9000\n",
            "Epoch: 1713, Accuracy: 0.9000\n",
            "Epoch: 1714, Accuracy: 0.9000\n",
            "Epoch: 1715, Accuracy: 0.9000\n",
            "Epoch: 1716, Accuracy: 0.9000\n",
            "Epoch: 1717, Accuracy: 0.9000\n",
            "Epoch: 1718, Accuracy: 0.9000\n",
            "Epoch: 1719, Accuracy: 0.9000\n",
            "Epoch: 1720, Accuracy: 0.9000\n",
            "Epoch: 1721, Accuracy: 0.9000\n",
            "Epoch: 1722, Accuracy: 0.9000\n",
            "Epoch: 1723, Accuracy: 0.9000\n",
            "Epoch: 1724, Accuracy: 0.9000\n",
            "Epoch: 1725, Accuracy: 0.9000\n",
            "Epoch: 1726, Accuracy: 0.9000\n",
            "Epoch: 1727, Accuracy: 0.9000\n",
            "Epoch: 1728, Accuracy: 0.9200\n",
            "Epoch: 1729, Accuracy: 0.8800\n",
            "Epoch: 1730, Accuracy: 0.8800\n",
            "Epoch: 1731, Accuracy: 0.8800\n",
            "Epoch: 1732, Accuracy: 0.8800\n",
            "Epoch: 1733, Accuracy: 0.8800\n",
            "Epoch: 1734, Accuracy: 0.9200\n",
            "Epoch: 1735, Accuracy: 0.9000\n",
            "Epoch: 1736, Accuracy: 0.8800\n",
            "Epoch: 1737, Accuracy: 0.9000\n",
            "Epoch: 1738, Accuracy: 0.9000\n",
            "Epoch: 1739, Accuracy: 0.9000\n",
            "Epoch: 1740, Accuracy: 0.9000\n",
            "Epoch: 1741, Accuracy: 0.9000\n",
            "Epoch: 1742, Accuracy: 0.9000\n",
            "Epoch: 1743, Accuracy: 0.9000\n",
            "Epoch: 1744, Accuracy: 0.9000\n",
            "Epoch: 1745, Accuracy: 0.9000\n",
            "Epoch: 1746, Accuracy: 0.9000\n",
            "Epoch: 1747, Accuracy: 0.9000\n",
            "Epoch: 1748, Accuracy: 0.9000\n",
            "Epoch: 1749, Accuracy: 0.9000\n",
            "Epoch: 1750, Accuracy: 0.9000\n",
            "Epoch: 1751, Accuracy: 0.8800\n",
            "Epoch: 1752, Accuracy: 0.9000\n",
            "Epoch: 1753, Accuracy: 0.8800\n",
            "Epoch: 1754, Accuracy: 0.8800\n",
            "Epoch: 1755, Accuracy: 0.8800\n",
            "Epoch: 1756, Accuracy: 0.8800\n",
            "Epoch: 1757, Accuracy: 0.8800\n",
            "Epoch: 1758, Accuracy: 0.8800\n",
            "Epoch: 1759, Accuracy: 0.8800\n",
            "Epoch: 1760, Accuracy: 0.8800\n",
            "Epoch: 1761, Accuracy: 0.8800\n",
            "Epoch: 1762, Accuracy: 0.8800\n",
            "Epoch: 1763, Accuracy: 0.8800\n",
            "Epoch: 1764, Accuracy: 0.9000\n",
            "Epoch: 1765, Accuracy: 0.8800\n",
            "Epoch: 1766, Accuracy: 0.9200\n",
            "Epoch: 1767, Accuracy: 0.9000\n",
            "Epoch: 1768, Accuracy: 0.9200\n",
            "Epoch: 1769, Accuracy: 0.9000\n",
            "Epoch: 1770, Accuracy: 0.9000\n",
            "Epoch: 1771, Accuracy: 0.9000\n",
            "Epoch: 1772, Accuracy: 0.9000\n",
            "Epoch: 1773, Accuracy: 0.9000\n",
            "Epoch: 1774, Accuracy: 0.9000\n",
            "Epoch: 1775, Accuracy: 0.9000\n",
            "Epoch: 1776, Accuracy: 0.9000\n",
            "Epoch: 1777, Accuracy: 0.9000\n",
            "Epoch: 1778, Accuracy: 0.9000\n",
            "Epoch: 1779, Accuracy: 0.9000\n",
            "Epoch: 1780, Accuracy: 0.9000\n",
            "Epoch: 1781, Accuracy: 0.9000\n",
            "Epoch: 1782, Accuracy: 0.9000\n",
            "Epoch: 1783, Accuracy: 0.9000\n",
            "Epoch: 1784, Accuracy: 0.9000\n",
            "Epoch: 1785, Accuracy: 0.9000\n",
            "Epoch: 1786, Accuracy: 0.9000\n",
            "Epoch: 1787, Accuracy: 0.9000\n",
            "Epoch: 1788, Accuracy: 0.9000\n",
            "Epoch: 1789, Accuracy: 0.9000\n",
            "Epoch: 1790, Accuracy: 0.9000\n",
            "Epoch: 1791, Accuracy: 0.9000\n",
            "Epoch: 1792, Accuracy: 0.9000\n",
            "Epoch: 1793, Accuracy: 0.9000\n",
            "Epoch: 1794, Accuracy: 0.9000\n",
            "Epoch: 1795, Accuracy: 0.9000\n",
            "Epoch: 1796, Accuracy: 0.9000\n",
            "Epoch: 1797, Accuracy: 0.9000\n",
            "Epoch: 1798, Accuracy: 0.9000\n",
            "Epoch: 1799, Accuracy: 0.9000\n",
            "Epoch: 1800, Accuracy: 0.9000\n",
            "Epoch: 1801, Accuracy: 0.1000\n",
            "Epoch: 1802, Accuracy: 0.1000\n",
            "Epoch: 1803, Accuracy: 0.1000\n",
            "Epoch: 1804, Accuracy: 0.1000\n",
            "Epoch: 1805, Accuracy: 0.1000\n",
            "Epoch: 1806, Accuracy: 0.1000\n",
            "Epoch: 1807, Accuracy: 0.1000\n",
            "Epoch: 1808, Accuracy: 0.1000\n",
            "Epoch: 1809, Accuracy: 0.1200\n",
            "Epoch: 1810, Accuracy: 0.7800\n",
            "Epoch: 1811, Accuracy: 0.9000\n",
            "Epoch: 1812, Accuracy: 0.9000\n",
            "Epoch: 1813, Accuracy: 0.9000\n",
            "Epoch: 1814, Accuracy: 0.9000\n",
            "Epoch: 1815, Accuracy: 0.9000\n",
            "Epoch: 1816, Accuracy: 0.9000\n",
            "Epoch: 1817, Accuracy: 0.9000\n",
            "Epoch: 1818, Accuracy: 0.9000\n",
            "Epoch: 1819, Accuracy: 0.9000\n",
            "Epoch: 1820, Accuracy: 0.9000\n",
            "Epoch: 1821, Accuracy: 0.9000\n",
            "Epoch: 1822, Accuracy: 0.9000\n",
            "Epoch: 1823, Accuracy: 0.9000\n",
            "Epoch: 1824, Accuracy: 0.9000\n",
            "Epoch: 1825, Accuracy: 0.9000\n",
            "Epoch: 1826, Accuracy: 0.9000\n",
            "Epoch: 1827, Accuracy: 0.9000\n",
            "Epoch: 1828, Accuracy: 0.9000\n",
            "Epoch: 1829, Accuracy: 0.9000\n",
            "Epoch: 1830, Accuracy: 0.9000\n",
            "Epoch: 1831, Accuracy: 0.9000\n",
            "Epoch: 1832, Accuracy: 0.9000\n",
            "Epoch: 1833, Accuracy: 0.9000\n",
            "Epoch: 1834, Accuracy: 0.9000\n",
            "Epoch: 1835, Accuracy: 0.9000\n",
            "Epoch: 1836, Accuracy: 0.9000\n",
            "Epoch: 1837, Accuracy: 0.9000\n",
            "Epoch: 1838, Accuracy: 0.9000\n",
            "Epoch: 1839, Accuracy: 0.9000\n",
            "Epoch: 1840, Accuracy: 0.9000\n",
            "Epoch: 1841, Accuracy: 0.9000\n",
            "Epoch: 1842, Accuracy: 0.9000\n",
            "Epoch: 1843, Accuracy: 0.9000\n",
            "Epoch: 1844, Accuracy: 0.9000\n",
            "Epoch: 1845, Accuracy: 0.9000\n",
            "Epoch: 1846, Accuracy: 0.9000\n",
            "Epoch: 1847, Accuracy: 0.9000\n",
            "Epoch: 1848, Accuracy: 0.9000\n",
            "Epoch: 1849, Accuracy: 0.9000\n",
            "Epoch: 1850, Accuracy: 0.9000\n",
            "Epoch: 1851, Accuracy: 0.9000\n",
            "Epoch: 1852, Accuracy: 0.9000\n",
            "Epoch: 1853, Accuracy: 0.9000\n",
            "Epoch: 1854, Accuracy: 0.9000\n",
            "Epoch: 1855, Accuracy: 0.9000\n",
            "Epoch: 1856, Accuracy: 0.9000\n",
            "Epoch: 1857, Accuracy: 0.9000\n",
            "Epoch: 1858, Accuracy: 0.9000\n",
            "Epoch: 1859, Accuracy: 0.9000\n",
            "Epoch: 1860, Accuracy: 0.9000\n",
            "Epoch: 1861, Accuracy: 0.9000\n",
            "Epoch: 1862, Accuracy: 0.9000\n",
            "Epoch: 1863, Accuracy: 0.9000\n",
            "Epoch: 1864, Accuracy: 0.9000\n",
            "Epoch: 1865, Accuracy: 0.9000\n",
            "Epoch: 1866, Accuracy: 0.9000\n",
            "Epoch: 1867, Accuracy: 0.9000\n",
            "Epoch: 1868, Accuracy: 0.9000\n",
            "Epoch: 1869, Accuracy: 0.9000\n",
            "Epoch: 1870, Accuracy: 0.9000\n",
            "Epoch: 1871, Accuracy: 0.9000\n",
            "Epoch: 1872, Accuracy: 0.9000\n",
            "Epoch: 1873, Accuracy: 0.9000\n",
            "Epoch: 1874, Accuracy: 0.9000\n",
            "Epoch: 1875, Accuracy: 0.9000\n",
            "Epoch: 1876, Accuracy: 0.9000\n",
            "Epoch: 1877, Accuracy: 0.9000\n",
            "Epoch: 1878, Accuracy: 0.9000\n",
            "Epoch: 1879, Accuracy: 0.9000\n",
            "Epoch: 1880, Accuracy: 0.9000\n",
            "Epoch: 1881, Accuracy: 0.9000\n",
            "Epoch: 1882, Accuracy: 0.9000\n",
            "Epoch: 1883, Accuracy: 0.9000\n",
            "Epoch: 1884, Accuracy: 0.9000\n",
            "Epoch: 1885, Accuracy: 0.9000\n",
            "Epoch: 1886, Accuracy: 0.9000\n",
            "Epoch: 1887, Accuracy: 0.9000\n",
            "Epoch: 1888, Accuracy: 0.9000\n",
            "Epoch: 1889, Accuracy: 0.9000\n",
            "Epoch: 1890, Accuracy: 0.9000\n",
            "Epoch: 1891, Accuracy: 0.9000\n",
            "Epoch: 1892, Accuracy: 0.9000\n",
            "Epoch: 1893, Accuracy: 0.9000\n",
            "Epoch: 1894, Accuracy: 0.9000\n",
            "Epoch: 1895, Accuracy: 0.9000\n",
            "Epoch: 1896, Accuracy: 0.9000\n",
            "Epoch: 1897, Accuracy: 0.9000\n",
            "Epoch: 1898, Accuracy: 0.9000\n",
            "Epoch: 1899, Accuracy: 0.9000\n",
            "Epoch: 1900, Accuracy: 0.9000\n",
            "Epoch: 1901, Accuracy: 0.9000\n",
            "Epoch: 1902, Accuracy: 0.9000\n",
            "Epoch: 1903, Accuracy: 0.9000\n",
            "Epoch: 1904, Accuracy: 0.9000\n",
            "Epoch: 1905, Accuracy: 0.9000\n",
            "Epoch: 1906, Accuracy: 0.9000\n",
            "Epoch: 1907, Accuracy: 0.9000\n",
            "Epoch: 1908, Accuracy: 0.9000\n",
            "Epoch: 1909, Accuracy: 0.9000\n",
            "Epoch: 1910, Accuracy: 0.9000\n",
            "Epoch: 1911, Accuracy: 0.9000\n",
            "Epoch: 1912, Accuracy: 0.9000\n",
            "Epoch: 1913, Accuracy: 0.9000\n",
            "Epoch: 1914, Accuracy: 0.9000\n",
            "Epoch: 1915, Accuracy: 0.9000\n",
            "Epoch: 1916, Accuracy: 0.9000\n",
            "Epoch: 1917, Accuracy: 0.9000\n",
            "Epoch: 1918, Accuracy: 0.9000\n",
            "Epoch: 1919, Accuracy: 0.9000\n",
            "Epoch: 1920, Accuracy: 0.9000\n",
            "Epoch: 1921, Accuracy: 0.9000\n",
            "Epoch: 1922, Accuracy: 0.9000\n",
            "Epoch: 1923, Accuracy: 0.9000\n",
            "Epoch: 1924, Accuracy: 0.9000\n",
            "Epoch: 1925, Accuracy: 0.9000\n",
            "Epoch: 1926, Accuracy: 0.9000\n",
            "Epoch: 1927, Accuracy: 0.9000\n",
            "Epoch: 1928, Accuracy: 0.9000\n",
            "Epoch: 1929, Accuracy: 0.9000\n",
            "Epoch: 1930, Accuracy: 0.9000\n",
            "Epoch: 1931, Accuracy: 0.9000\n",
            "Epoch: 1932, Accuracy: 0.9000\n",
            "Epoch: 1933, Accuracy: 0.9000\n",
            "Epoch: 1934, Accuracy: 0.9000\n",
            "Epoch: 1935, Accuracy: 0.9000\n",
            "Epoch: 1936, Accuracy: 0.9000\n",
            "Epoch: 1937, Accuracy: 0.9000\n",
            "Epoch: 1938, Accuracy: 0.9000\n",
            "Epoch: 1939, Accuracy: 0.9000\n",
            "Epoch: 1940, Accuracy: 0.9000\n",
            "Epoch: 1941, Accuracy: 0.9000\n",
            "Epoch: 1942, Accuracy: 0.9000\n",
            "Epoch: 1943, Accuracy: 0.9000\n",
            "Epoch: 1944, Accuracy: 0.9000\n",
            "Epoch: 1945, Accuracy: 0.9000\n",
            "Epoch: 1946, Accuracy: 0.9000\n",
            "Epoch: 1947, Accuracy: 0.9000\n",
            "Epoch: 1948, Accuracy: 0.9000\n",
            "Epoch: 1949, Accuracy: 0.9000\n",
            "Epoch: 1950, Accuracy: 0.9000\n",
            "Epoch: 1951, Accuracy: 0.9000\n",
            "Epoch: 1952, Accuracy: 0.9000\n",
            "Epoch: 1953, Accuracy: 0.9000\n",
            "Epoch: 1954, Accuracy: 0.9000\n",
            "Epoch: 1955, Accuracy: 0.9000\n",
            "Epoch: 1956, Accuracy: 0.9000\n",
            "Epoch: 1957, Accuracy: 0.9000\n",
            "Epoch: 1958, Accuracy: 0.9000\n",
            "Epoch: 1959, Accuracy: 0.9000\n",
            "Epoch: 1960, Accuracy: 0.9000\n",
            "Epoch: 1961, Accuracy: 0.9000\n",
            "Epoch: 1962, Accuracy: 0.9000\n",
            "Epoch: 1963, Accuracy: 0.9000\n",
            "Epoch: 1964, Accuracy: 0.9000\n",
            "Epoch: 1965, Accuracy: 0.9000\n",
            "Epoch: 1966, Accuracy: 0.9000\n",
            "Epoch: 1967, Accuracy: 0.9000\n",
            "Epoch: 1968, Accuracy: 0.9000\n",
            "Epoch: 1969, Accuracy: 0.9000\n",
            "Epoch: 1970, Accuracy: 0.9000\n",
            "Epoch: 1971, Accuracy: 0.9000\n",
            "Epoch: 1972, Accuracy: 0.9000\n",
            "Epoch: 1973, Accuracy: 0.9000\n",
            "Epoch: 1974, Accuracy: 0.9000\n",
            "Epoch: 1975, Accuracy: 0.9000\n",
            "Epoch: 1976, Accuracy: 0.9000\n",
            "Epoch: 1977, Accuracy: 0.9000\n",
            "Epoch: 1978, Accuracy: 0.9000\n",
            "Epoch: 1979, Accuracy: 0.9000\n",
            "Epoch: 1980, Accuracy: 0.9000\n",
            "Epoch: 1981, Accuracy: 0.9000\n",
            "Epoch: 1982, Accuracy: 0.9000\n",
            "Epoch: 1983, Accuracy: 0.9000\n",
            "Epoch: 1984, Accuracy: 0.9000\n",
            "Epoch: 1985, Accuracy: 0.9000\n",
            "Epoch: 1986, Accuracy: 0.9000\n",
            "Epoch: 1987, Accuracy: 0.9000\n",
            "Epoch: 1988, Accuracy: 0.9000\n",
            "Epoch: 1989, Accuracy: 0.9000\n",
            "Epoch: 1990, Accuracy: 0.9000\n",
            "Epoch: 1991, Accuracy: 0.9000\n",
            "Epoch: 1992, Accuracy: 0.9000\n",
            "Epoch: 1993, Accuracy: 0.9000\n",
            "Epoch: 1994, Accuracy: 0.9000\n",
            "Epoch: 1995, Accuracy: 0.9000\n",
            "Epoch: 1996, Accuracy: 0.9000\n",
            "Epoch: 1997, Accuracy: 0.9000\n",
            "Epoch: 1998, Accuracy: 0.9000\n",
            "Epoch: 1999, Accuracy: 0.9000\n",
            "Epoch: 2000, Accuracy: 0.9000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f4e29e170f0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XjlMCJ_-BxA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "58d3f261-78f0-4a64-b24c-0e6046cfc4ee"
      },
      "source": [
        "plt.plot(np.arange(len(best_acc)), best_acc)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f4e2a042978>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3RU95Xg++/WGwmB0LuKt0FgBJTsmOA87ODEdgxBZceeuTPxzeQxa7q9evV4Xrnp2/ZkJrmLHi9Pr/FMP6azMtfT7fS403PdGU93xxJg2sE4TtqJY2JbAoGrEGDzkFSSEIKShJ617x91RMqyhEqopFNVZ3/W0qLqdx61q5C0dX7n99s/UVWMMcZ4T47bARhjjHGHJQBjjPEoSwDGGONRlgCMMcajLAEYY4xH5bkdwFxUVlbqunXr3A7DGGMyyq9+9ateVa2a2p5RCWDdunUcPXrU7TCMMSajiMgH07VbF5AxxniUJQBjjPEoSwDGGONRlgCMMcajLAEYY4xHJZUARGS3iIREpF1Enphm+1oROSwirSLymoismrJ9mYhcEJE/SWi7Q0SOOef8YxGR+b8dY4wxyZo1AYhILvBdYA9QDzwqIvVTdnsGeF5VA8A+4Okp238PeH1K2/eA3wTqnK/dc47eGGPMTUtmHsBOoF1VzwCIyAvAQ8CJhH3qgW84j48Afzu5QUTuAGqAl4EdTpsPWKaqv3CePw98ETg4nzeT7l45EeHYhX63wyA3J4dHd66melmR26GYNPOzU7388uwlt8NARHjkYytZW1HidihZLZkEsBI4n/D8AnDnlH1agEeAPwIeBkpFpAK4DPxn4J8A900554Up51w53YuLyGPAYwBr1qxJItz0pKp846/eJToyjtudXarQf22U7wS3uhuISSvjEzH+5Qvv0Dc4mhbfo2d6B/mvj97ubiBZLlUzgb8J/ImIfJ14V89FYAL4beCAql642S5+VX0WeBZgx44dGbt6TceVYaIj4zz18Da+fOdaV2N57Pmj7G/t5N/trSc3x269mLg3Tl+ib3CU//crd/DA1lpXY/m3f3OMv3n7IkOj4xQXZFTBgoySzE3gi8DqhOernLbrVLVDVR9R1duBbzlt/cAngcdF5H3i9wm+KiL/0Tl+1Y3OmW3CXVEANtWUuhwJNDb46Y6O8Nb7fW6HYtJIU0sHpYV57Nr0kZIxi64x4OPa2ASvvtftdihZLZkE8BZQJyLrRaQA+BLwUuIOIlIpIpPnehJ4DkBVv6yqa1R1HfGrhOdV9QlV7QSuisgnnNE/XwV+lJq3lJ5CEScBVLufAO7bUs2S/FyaWzvcDsWkiZHxCQ61dXH/1hqK8nPdDoc711dQVVpIc0un26FktVkTgKqOA48Dh4CTwA9VtU1E9onIg85u9wAhEQkTv+H7VBKv/dvAnwLtwGmy/AZwuCtK7bIilhfnux0KxQV53LulmoPHuhifiLkdjkkDPw33cnV4nGCD3+1QAMjNEfZu9/FqqJvo8Jjb4WStpOYBqOoBVd2kqhtU9Smn7duq+pLz+EVVrXP2+Q1VHZnmHH+uqo8nPD+qqtuccz6uWb46fbg7yqZa9//6n9QY8HNpcJSfn3F/xIdxX3NrB2XF+dy1sdLtUK4LNvgYHY/x45MRt0PJWjYTeBFMxJRTkQE21yx1O5Tr7tlcxdLCPJparBvI666NTvDKiQh7ttWSn5s+vxJuX72ClWVLaLJuoAWTPv/bWexc3xAj4zHq0uAG8KSi/Fw+X1/Dy8e7GB23biAvOxLqZnB0gsZAenT/TMrJEfYGfLwe7qF/aNTtcLKSJYBFEHJGAG1OowQAEGzwc3V4nJ+e6nE7FOOi5tYOKpcW8olbKtwO5SOCAT/jMeVQW5fboWQlSwCLIOyMAKpLoy4ggE9vrKSsON+6gTxsYGScwye72bu9Ni3nhGxbuYx1FcXWDbRALAEsgnAkypry4rSb0FKQl8PurbW8ciLC8NiE2+EYFxw+GWFkPEZjmoz+mUpEaAz4eeN0L70DHxlbYubJEsAiCEeiaTEBbDrBBj+DoxMcsQk3ntTU0oFveRF3rFnhdigzCjb4iSkcPGZXAalmCWCBjY7HONMzyKY06/6ZdOf6ciqXFtBkk8I858rQGD8J97B3u4+cNOz+mbS5tpS66qXWDbQALAEssLO9g4zHlM1pNAcgUV5uDl/Y7uPV97oZGBl3OxyziA6d6GJsQtNm8teNBBv8vPVBH51XrrkdSlaxBLDArpeASNMuIIhPChsei3HYJtx4SlNLB2vKiwmsWu52KLNqDPhQhf2tdhWQSpYAFtipSJTcHOGWqvSta75j7QpqlxXZJbaHXBoY4Y3Tl2gM+MiExfhuqVrKVv8ymi0BpJQlgAUW6oqyvrKEwjz3C2zNJCdHaAz4+Em4mytDVnfFCw4e72IilhndP5OCDX7ePd/P+b4ht0PJGpYAFlh8BFB63gBO1NjgZ2xCOXTCJtx4QVNLBxuqSrg1Te9NTWfvdh+ADVhIIUsAC+ja6AQf9A2ldf//pIZVy1ldvsQusT0gcnWYX77fR7DBnxHdP5NWlxdz+5oyKxGdQpYAFlB79wCq6VcCYjqTE27+vr2XSzbhJqvtb+1ElbSr/ZOMxoCfE51XOd0z4HYoWcESwAKaLAGRTmWgbyQY8DMRU162uitZrbm1gy2+ZWysTv+uyan2bvchgl0FpIglgAUUjkQpyMthbXmx26EkZYuvlA1VJVYbKIud7xvi7XP9BBt8bodyU2qXF7FzXTlNrR1k+RIiiyKpBCAiu0UkJCLtIvLENNvXishhEWkVkddEZFVC+9si8q6ItInIbyUc85pzznedr+rUva30EIpE2VC1lLw0qrF+I5PdQG+e7SNyddjtcMwC2O+UU2jcnnndP5MaG/y0dw9cn2Njbt6sv5lEJBf4LrAHqAceFZH6Kbs9Q3y93wCwD3jaae8EPqmqtwF3Ak+ISOJ33pdV9TbnK+uK0YS7omm1CEwygg3xCTcHrO5KVmpu7aBhdRlrKjLjqnQ6e7bFK5faler8JfOn6U6gXVXPqOoo8ALw0JR96oFXncdHJrer6mjC8pCFSb5eVrg6PEbHleGM6f+ftLG6lFtrS+2HKwud7R3k+MWrBAOZ2f0zqXJpIZ/aUEFTS6d1A81TMr+QVwLnE55fcNoStQCPOI8fBkpFpAJARFaLSKtzjt9X1cTfLN93un/+vcwwHk1EHhORoyJytKcncxYuORWJj1LIhBFAUwUb/Lx9rp8Ll23CTTZpdpL63gxPABAfsHCub4hjF6+4HUpGS9Vf5N8EdonIO8Au4CIwAaCq552uoY3A10Skxjnmy6q6Hbjb+frKdCdW1WdVdYeq7qiqqkpRuAsvnAE1gGYSdIYHWt2V7NLU2sHOdeX4li9xO5R5e2BrLfm51g00X8kkgIvA6oTnq5y261S1Q1UfUdXbgW85bf1T9wGOE/9lj6pedP6NAv+TeFdT1gh1RSkuyGVlWeb9sK2pKKZh1XKbcZlFQl1RwpEBGjN09M9Uy4vz+UxdFftbO4nFrBvoZiWTAN4C6kRkvYgUAF8CXkrcQUQqRWTyXE8Czzntq0RkifN4BXAXEBKRPBGpdNrzgUbiySFrhCNR6mpK07rO+o0EG/wcv3iVs72DbodiUqC5tYMcgT3bsiMBQPx7tOPKMG+fu+x2KBlr1gSgquPA48Ah4CTwQ1VtE5F9IvKgs9s9xH+xh4Ea4CmnfQvwpoi0AD8BnlHVY8RvCB9y7g28S/yK4r+n7m25LxzJvBFAib7g1F1ptkvsjKeqNLV08MkNFVSVFrodTsrcV19DYV6OdQPNQ1KL1KrqAeDAlLZvJzx+EXhxmuNeAQLTtA8Cd8w12ExxaWCE3oHRjOz/n+QvW8LH162gubWTf3FvndvhmHlo67jK+5eG+K1dG9wOJaWWFubxuVur2X+si28Ht6blovbpzjPDMhdT2BkBlMkJAOKX2KFIlFCXTbjJZE0tHeTlCLu31bodSsoFG/z0Dozw5plLboeSkSwBLIDJEUDpugxksvZs85Ej8f5jk5lUlebWTu6uq6SsuMDtcFLus5urKS7IpclGrN0USwALIBSJsnxJPtUZ3t9aVVrIJzdU0NxqE24y1dvn+rnYfy2jFn6ZiyUFudxfX8PB452MTcTcDifjWAJYAPESEKUZVWt9Jo0BP2d7B2nruOp2KOYmNLV0UJCXw/31NbPvnKEaA376h8b4WXuv26FkHEsAKaaq8VXAajN3BFCi3VtrycsRmxOQgSZiyoFjnXx2cxWlRfluh7NgPrOpktKiPCsRfRMsAaRY5OoIV4fHM/4G8KQVJQXcVVdJs9VdyTi/PNtHd3QkIxd+mYvCvFwe2FrL37V1MTw24XY4GcUSQIqFMrgExEyCAT8X+6/xzvn+2Xc2aaO5tYMl+bncuyXrKq1/RLDBT3RknNfDmVMvLB1YAkixcFf2JYD7t9ZQYBNuMsrYRIyDx7u4r76G4oKkpvtktE9tqKC8pMBGA82RJYAUC0eiVJUWUl6SPUPulhXlc8+meN2VCau7khHeOH2JvsFRGrOg8mcy8nNz2L2tlh+fiDA0Ou52OBnDEkCKxUtAZM9f/5OCDX66oyO89X6f26GYJDS3dFBamMeuTZlTQXe+ggE/18YmePW9rFtbasFYAkihWEwJRwaoy+AaQDO5d0s1S/JzrRsoA4yMT/ByWxf3b62hKD/X7XAWzc715VSVFtr36BxYAkihC5evcW1sIiuvAIoL8rh3SzUHj3cxbhNu0tpPw71Eh8ezdvLXTHJzhL3bfRwJ9RAdHnM7nIxgCSCFro8AyvASEDMJNvjpGxzljdNWdyWdNbV2UFacz10bK90OZdEFG/yMjsd45UTE7VAygiWAFJqsAVRXnX1dQAC7NlVRWphnl9hp7NroBK+ciLBnWy35ud778f7YmjJWli2h2UYDJcV73yELKByJsrJsSdbOuizKz+X+rTUcautiZNwm3KSjI6FuhkYnri/r6TUiQmPAx+vhHvqHRt0OJ+1ZAkihUFeUTVl4AzhRMODn6vA4Pw1b3ZV01NTSQeXSQu68pcLtUFzTGPAzHlNePt7ldihpL6kEICK7RSQkIu0i8sQ029eKyGERaRWR10RkVUL72yLyroi0ichvJRxzh4gcc875x5LhldPGJmKc6RnM2v7/SZ/eWElZcb6ViE5DAyPjvPpeN3u313p6cZRtK5exrqLYuoGSMGsCEJFc4LvAHqAeeFRE6qfs9gzwvKoGgH3A0057J/BJVb0NuBN4QkQmr02/B/wmUOd87Z7ne3HVB5cGGZ2IZeUIoEQFeTns2VbLKyciXBu1bqB08uMTEUbGY54b/TOViBBs8PPG6V56oiNuh5PWkrkC2Am0q+oZVR0FXgAemrJPPfCq8/jI5HZVHVXVyf+BwsnXExEfsExVf6HxCmPPA1+c1ztxWbasApaMxoCfwdEJjoRswk06aWrpwLe8iI+tWeF2KK5rDPiJKbx83K4CbiSZBLASOJ/w/ILTlqgFeMR5/DBQKiIVACKy2ln8/Tzw+6ra4Rx/YZZz4hz/mIgcFZGjPT3pW+gp1BUlR2Bjlo4ASvSJWyqoXFpo3UBp5MrQGK+f6qEx4CPHw90/kzbXlrKpZilNViL6hlJ1E/ibwC4ReQfYBVwEJgBU9bzTNbQR+JqIzGllClV9VlV3qOqOqqr0ndYejkRZW1HiiZmXuTnCF7bXcvhkNwMjVnclHRxq62JsQrO+9PNcNAb8/PL9PjqvXHM7lLSVTAK4CKxOeL7KabtOVTtU9RFVvR34ltPWP3Uf4Dhwt3P8qhudM9OEItk/AihRsMHPyHiMwydtwk06aGrtYE15MYFVy90OJW1MFsLbbzeDZ5RMAngLqBOR9SJSAHwJeClxBxGpFJHJcz0JPOe0rxKRJc7jFcBdQEhVO4GrIvIJZ/TPV4EfpeQduWB4bIL3ewez/gZwojvWrMC3vMgmhaWB3oER3jh9iWCDLyuWIU2VW6qWsm3lMisRfQOzJgBVHQceBw4BJ4EfqmqbiOwTkQed3e4BQiISBmqAp5z2LcCbItIC/AR4RlWPOdt+G/hToB04DRxMzVtafGd6Bolp9paAmE6OU3flJ+EergxZ3RU3HTzexUTMun+m0xjw03K+n/N9Q26HkpaSugegqgdUdZOqblDVp5y2b6vqS87jF1W1ztnnNyZH/qjqK6oaUNUG599nE855VFW3Oed8XDN4vcHJEhBeugKAeDfQ2IRy6IRNuHFTc0sHG6uXcquH/gBJ1t7t8W4gW9N6ejYTOAVCkSj5ucK6yhK3Q1lUgVXLWVNebN1ALopcHeaX7/fRGLDun+msLi/m9jVlNhpoBpYAUiDcFeWWyqWeK741WXfljdOXuDRgE27csL+1E1Ws++cGggE/Jzuv0t494HYoacdbv7EWSCgS9VT/f6LGgJ+JmHLQ6q64oqm1gy2+ZZ6Yf3Kz9gZ8iGDzVqZhCWCeBkfGuXD5Gps9NAQ00RZfKRuqSqwbyAXn+4Z451w/wQZvrPt7s2qWFbFzXTlNLR1k8K3GBWEJYJ5OOZeVdR67ATxpsu7KL9/vI3J12O1wPGX/sXi/tldLP89FsMHP6Z5B3uuKuh1KWrEEME/hLm+OAErUGPCjahNuFltTSwcNq8tYXV7sdihpb8+2eIVU6wb6MEsA8xSKRCnKz/H0D+HG6qVs8S2zH65FdKZngLaOqwQD1v2TjIqlhXxqQwVNLZ3WDZTAEsA8hSNR6qpLPV1/HeLT7t8+ZxNuFstkrfu9lgCSFgz4Odc3ROuFK26HkjYsAcxTOBL1RAno2Uz2Q0/2S5uF1dTSwc515fiWL3E7lIzxwNZa8nOtGyiRJYB56B8aJXJ1xFNF4GaypqKYhtVl9sO1CEJdUU51D9jonzlaXpzPrk1VNLd2EotZNxBYApiX64vAeHQOwFTBgI/jF69ytnfQ7VCyWlNLBzkCu7dZApirxoCfzivDvH3ustuhpAVLAPMQ8mgNoJlM9kc325yABaOqNLd28KkNlVSVFrodTsa5r76Gwrwcm7fisAQwD+GuKKWFefiWF7kdSlrwLV/Cx9etsMJbC+j4xau8f2noeq17MzdLC/P43K3V7D8Wr6DqdZYA5iHslICwIly/FmzwE44MELIJNwuiqbWDvBxh97Zat0PJWMEGP70DI7x55pLbobjOEsBNUlVnBJDdAE60Z5uPHKu7siBiMWV/ayef2VRFWXGB2+FkrM9urqakINeuVEkyAYjIbhEJiUi7iDwxzfa1InJYRFpF5DURWeW03yYiPxeRNmfbP0445s9F5KyIvOt83Za6t7XwegZGuDw0ZkNAp6gqLeSTGyqs7soCeOf8ZS72X7Pun3laUpDLffU1HDzexdhEzO1wXDVrAhCRXOC7wB6gHnhUROqn7PYM8Lyz+Ps+4GmnfQj4qqpuBXYDfygiZQnH/Y6q3uZ8vTvP97Kowl3xEUB2A/ijggE/718aoq3jqtuhZJWmlk4K8nK4v77G7VAyXjDgp39ojJ+197odiquSuQLYCbSr6hlVHQVeAB6ask898Krz+MjkdlUNq+op53EH0A1UpSJwt02OALIhoB+1e1steTliIy1SaCKm7D/WyWc3V1FalO92OBnv7k2VlBblef57NJkEsBI4n/D8gtOWqAV4xHn8MFAqIhWJO4jITqCA+Pq/k55yuob+QESmHdMmIo+JyFEROdrT05NEuIvjVCRKRUkBlUttKN5UZcUF3F1XSXOr1V1JlV+e7aMnOkKwwSp/pkJhXi67t9bySluE4bEJt8NxTapuAn8T2CUi7wC7gIvA9U9VRHzAXwD/VFUnO92eBG4FPg6UA7873YlV9VlV3aGqO6qq0ufiIRSJUmc3gGcUbPBzsf8ab5/rdzuUrNDU2kFxQS6fu7Xa7VCyRrDBT3RknJ+E0+cPy8WWTAK4CKxOeL7KabtOVTtU9RFVvR34ltPWDyAiy4D9wLdU9RcJx3Rq3AjwfeJdTRlBVQl3Ra3//wbur6+hwCbcpMTYRIyDxzq5d0sNxQV5boeTNT61oYLykoLrhfW8KJkE8BZQJyLrRaQA+BLwUuIOIlIpIpPnehJ4zmkvAP6G+A3iF6cc43P+FeCLwPH5vJHFdLH/GoOjE9b/fwOlRfl8dnMVB4512oSbeXrj9CUuD41Z6ecUy8vNYc+2Wn58IsLQ6Ljb4bhi1gSgquPA48Ah4CTwQ1VtE5F9IvKgs9s9QEhEwkAN8JTT/o+AzwBfn2a451+KyDHgGFAJ/IdUvamFFrYSEElpDPjpjo7wy7N9boeS0ZpaOigtzGPX5vTpAs0WjQE/18YmePW9brdDcUVS15OqegA4MKXt2wmPXwRenOa4HwA/mOGcn5tTpGlksgicV5eBTNa9W6pZkp9Lc2sHn9xQMfsB5iNGxic41NbF57fWUpiX63Y4WWfn+nKqSwtpaumg0YNLa9pM4JsQ7opSu6yI5UtsON6NFBfkce+WaptwMw+vh3uJDo/TaKWfF0RujvCF7T6OhHqIDo+5Hc6iswRwE0JODSAzu2CDn77BUd44bXVXbkZTSwdlxfnctbHS7VCyVrDBz+h4jFdORNwOZdFZApijiZhyqnuAzTYENCm7NlVRWphnJaJvwrXRCX58MsKebT7yc+1HdaF8bE0ZK8uWeHLEmn1XzdEHlwYZHY9ZDaAkFeXncv/WGl5u62Jk3LsTbm7Gq+91MzQ6YaN/FpiI0Bjw8dNTvfQPjbodzqKyBDBHkzeAN1sXUNKCDX6iw+P8NOztuitz1dzaQeXSQu68xW6gL7Rgg5/xmPLy8S63Q1lUlgDmaHII6MZq6wJK1l0bKykrzrfyu3MQHR7j1fe62bu9ltwcW29ioW31L2NdRbHnvkctAcxRKBJlTXmxzcicg3xnws0rJyJcG7VuoGT8+GSEkfGY1f5ZJCJCsMHPz09foic64nY4i8YSwByFu6LW/38TggE/Q6MTHAl5c8LNXDW3dOJfXsTH1qxwOxTPCDb4iSkcPO6d0hCWAOZgdDzG2d5BNtda989c3XlLBZVLCz050mKurgyN8fqpHvYGfORY98+i2VRTyqaapTS3WAIw0zjbO8h4TO0K4Cbk5gh7t9fy6nvdDIx4s+5Ksg61dTE2odb944JgwM8v3++j88o1t0NZFJYA5uD6IjCWAG5KY4OfkfEYP/bghJu5aGrtYE15MdtXLnc7FM9pdJLufo9UCLUEMAfhrii5OcItVSVuh5KR7lizAt/yIusGuoHegRH+vr2XYIOPeKFcs5jWV5awbeUymiwBmKlCkSjrK0usKNdNysmJT7h5/VQPV4a8V3clGQePdxFTrPvHRcGAn5bz/Zy7NOR2KAvOEsAchCO2CMx8NQb8jE0oh9q8NeEmWU0tHWysXmrfZy7a68y8bj6W/VeqlgCSdG10gnN9Q7YM5DwFVi1nTbn3Jtwko+vKMG+930cw4LfuHxetWlHMx9aU0eSB0UCWAJLU3j2Aqi0CM1+TdVfeOH2J3gHvTLhJxv5jnahipZ/TQGPAz8nOq7R3D7gdyoJKKgGIyG4RCYlIu4g8Mc32tSJyWERaReQ1EVnltN8mIj8XkTZn2z9OOGa9iLzpnPOvnOUj09b1EUBWA2jegg1+JmLKQY/VXZlNU0sH9b5lbKiyq0y37Q34EInXY8pmsyYAEckFvgvsAeqBR0WkfspuzxBf9zcA7AOedtqHgK+q6lZgN/CHIlLmbPt94A9UdSNwGfhn830zCykciVKQl8Pa8mK3Q8l4t9aWsrF6qZWITnC+b4h3z/fbzd80UbOsiDvXl9PU0oFq9q5pnUxBm51Au6qeARCRF4CHgBMJ+9QD33AeHwH+FkBVw5M7qGqHiHQDVSJyBfgc8H86m/8H8P8A37vpd7LAQl1RNlYtJc/qss/bZDfQHx0+xW/9xa+w7m7o6I9PPGq00s9pozHg59/97XFOdkap9y9zO5wFkUwCWAmcT3h+Abhzyj4twCPAHwEPA6UiUqGq15eBEpGdQAFwGqgA+p0F5yfPuXK6FxeRx4DHANasWZNEuAvjVCTKzvXlrr1+tvk/dqzmyHvdnOnN7j7WuXh052pW2xVm2tizrZbvvNRGc2uHpxNAMr4J/ImIfB14HbgIXC/7KCI+4C+Ar6lqbC4jHFT1WeBZgB07drhyLXZ1eIyOK8PW/59CK8uW8KPH73I7DGNmVLG0kE9tqKC5tZPfeWBzVo7MSqY/4yKwOuH5KqftOlXtUNVHVPV24FtOWz+AiCwD9gPfUtVfOIdcAspEJG+mc6aTU84NYBsBZIy3BBv8nOsbovXCFbdDWRDJJIC3gDpn1E4B8CXgpcQdRKRSRCbP9STwnNNeAPwN8RvEL07ur/G7KkeAf+g0fQ340XzeyEIKdcW7KawGkDHe8kB9Lfm5krXlS2ZNAE4//ePAIeAk8ENVbRORfSLyoLPbPUBIRMJADfCU0/6PgM8AXxeRd52v25xtvwt8Q0Taid8T+LNUvalUC0eilBTksrJsiduhGGMW0fLifHZtqmL/sU5isewbDZTUPQBVPQAcmNL27YTHLwIvTnPcD4AfzHDOM8RHGKW9cCTKxppSq81ujAcFG/z8+GQ3vzp3mY+vy66BIDamMQnxGkA2OccYL7p3Sw2FeTlZOW/FEsAsegdG6B0Ytf5/YzxqaWEe926pZv+xTsYnYm6Hk1KWAGYRnhwBZENAjfGsYMBP78Aob57tczuUlLIEMItwlw0BNcbrPntrNSUFuVlXG8gSwCzC3QMsX5JPVWmh26EYY1xSlJ/L/fU1HDzexVgWdQNZAphFuCu+CEw2zgI0xiSvMeCnf2iMn7X3uh1KylgCuAFVJRSJsqnWRgAZ43V3b6pkWVFeVk0KswRwA11Xh4kOj1v/vzGGwrxcdm+r5e/aIgyPTcx+QAawBHADIecGsA0BNcZAvBtoYGScn4R73A4lJSwB3MCpiNUAMsb82qc2VFBeUpA13UCWAG4gFIlSVVrIipK0Xq3SGLNI8nJz2LOtlsMnuxkaHZ/9gDRnCeAG4iUg7K9/Y8yvBRv8XBub4PDJbrdDmTdLADOIxZRwJGrdPy8lHo4AABHXSURBVMaYD/n4unJqlhVmRTeQJYAZnL88xPBYjM02BNQYkyA3R/jCdh+vhXu4OjzmdjjzYglgBmHnBnCdXQEYY6YINvgZHY/xSlvE7VDmxRLADCaLwNVV2xWAMebDbl9dxsqyJRlfGyipBCAiu0UkJCLtIvLENNvXishhEWkVkddEZFXCtpdFpF9Emqcc8+cicnaalcLSQqgrysqyJZQW5bsdijEmzYgIjQ0+fnqql8uDo26Hc9NmTQAikgt8F9gD1AOPikj9lN2eIb7ubwDYBzydsO0/AV+Z4fS/o6q3OV/vzjn6BRSORK0EtDFmRsGAn/GY8nJbl9uh3LRkrgB2Au2qekZVR4EXgIem7FMPvOo8PpK4XVUPA9EUxLpoxiZinO4ZsBFAxpgZbfUvY31lSUZ3AyWTAFYC5xOeX3DaErUAjziPHwZKRaQiiXM/5XQb/YGITFtvWUQeE5GjInK0p2dxpl9/cGmQsQllky0DaYyZgYgQDPj4+elL9ERH3A7npqTqJvA3gV0i8g6wC7gIzFYt6UngVuDjQDnwu9PtpKrPquoOVd1RVVWVonBvLNRlJSCMMbNrbPATUzh4vNPtUG5KMgngIrA64fkqp+06Ve1Q1UdU9XbgW05b/41OqqqdGjcCfJ94V1NaCEWi5AhstBFAxpgb2FRTyuaa0oydFJZMAngLqBOR9SJSAHwJeClxBxGpFJHJcz0JPDfbSUXE5/wrwBeB43MJfCGFu6KsqyihKD/X7VCMMWku2ODjrfcv09F/ze1Q5mzWBKCq48DjwCHgJPBDVW0TkX0i8qCz2z1ASETCQA3w1OTxIvJT4H8B94rIBRF5wNn0lyJyDDgGVAL/IUXvad7CkSh11v9vjElCY8APwIFjmdcNlJfMTqp6ADgwpe3bCY9fBF6c4di7Z2j/XPJhLp7hsQnevzRIY8DndijGmAywrrKE7SuX09TSwW/cfYvb4cyJzQSe4nTPADGFTTYHwBiTpMaAj5YLVzh3acjtUObEEsAUkyUgrAy0MSZZe50eg6YMmxNgCWCKUNcA+bnCusoSt0MxxmSIVSuKuWPtiowbDWQJYIpwJMotlUvJz7WPxhiTvMaAj/e6orR3Z07hA/stN0U4ErX+f2PMnO3d7kMEmloyZzSQJYAEAyPjXLh8jc02BNQYM0fVy4q4c305za0dqKrb4STFEkCCU84NYCsBYYy5GcEGP6d7BjnZmRndQJYAElwfAWRdQMaYm7Bnm4/cHMmY0UCWABKEugYoys9h9Ypit0MxxmSg8pICPr2xMmO6gSwBJDjVHaWuupScHHE7FGNMhgoGfJzvu0bLhStuhzIrSwAJQl1R6/83xszL57fWUpCbQ3MGzAmwBOC4PDhKd3SEzbU2AsgYc/OWL8nnM5uqaG7tJBZL724gSwCOsI0AMsakSLDBR9fVYY5+cNntUG7IEoDDEoAxJlXu21JDUX5O2q8XbAnAEY4MUFqYh295kduhGGMyXElhHvfeWsOBY52MT8TcDmdGSSUAEdktIiERaReRJ6bZvlZEDjsLvL8mIqsStr0sIv0i0jzlmPUi8qZzzr9yVhtzTcgpARFfoMwYY+anMeCjd2CUN8/2uR3KjGZNACKSC3wX2APUA4+KSP2U3Z4BnlfVALAPeDph238CvjLNqX8f+ANV3QhcBv7Z3MNPDVWN1wCy7h9jTIp89tZqSgpy07pCaDJXADuBdlU9o6qjwAvAQ1P2qQdedR4fSdyuqoeBD82LdtYB/hy/XkXsfxBfF9gVPdER+ofGrAaQMSZlivJz+fzWWg4e72J0PD27gZJJACuB8wnPLzhtiVqAR5zHDwOlIlJxg3NWAP3OesMznRMAEXlMRI6KyNGenp4kwp27cGQAsBvAxpjUagz4uHJtjL9v73U7lGml6ibwN4FdIvIOsAu4CEyk4sSq+qyq7lDVHVVVVak45UeEJkcAWQ0gY0wK3V1XxbKivLTtBkpmUfiLwOqE56uctutUtQPnCkBElgL/QFX7b3DOS0CZiOQ5VwEfOediCndFqSgpoHJpoVshGGOyUEFeDru31XLgWBfDYxMU5ee6HdKHJHMF8BZQ54zaKQC+BLyUuIOIVIrI5LmeBJ670Qk1XiXpCPAPnaavAT+aS+CpFLIbwMaYBRJs8DMwMs5roYXpwp6PWROA8xf648Ah4CTwQ1VtE5F9IvKgs9s9QEhEwkAN8NTk8SLyU+B/AfeKyAURecDZ9LvAN0Sknfg9gT9L0XuaE1XlVCRqJaCNMQvik7dUUFFSkJYlopPpAkJVDwAHprR9O+Hxi/x6RM/UY++eof0M8RFGrrrYf43B0QnqbASQMWYB5OXmsGd7Lf/7VxcZGh2nuCCpX7uLwvMzga8vAmNdQMaYBRIM+Lk2NsGPT3a7HcqHeD4BhLriQ0DrLAEYYxbIx9eVU7OsMO1KRHs+AYQjUXzLi1i+JN/tUIwxWSonR9i73c9roR6uDo+5Hc51nk8AtgiMMWYxBBt8jE7EeKUt4nYo13k6AUzElPaeATbZDWBjzAK7bXUZq1YsSavRQJ5OAB9cGmR0PGZXAMaYBSciNAb8/OxUL5cHR90OB/B4Arg+AsjmABhjFkFjwMd4THm5rcvtUACPJ4BQ1wAisLHauoCMMQtvq38Zt1SWpE1tIE8ngHAkyuoVxWk1McMYk71EhMYGP784c4nu6LDb4VgCsP5/Y8xiCgZ8xBQOHnO/G8izCWBkfIKzvYNsrrXuH2PM4qmrKeXW2tK06AbybAI42zvIeEztCsAYs+gaAz6OfnCZjv5rrsbh2QQQ6rIRQMYYdzQG/ADsb+10NQ7PJoBwJEpujrC+ssTtUIwxHrOusoTAquU0uzwpzMMJYID1lSUU5qXXCj3GGG9oDPhouXCFDy4NuhaDhxNA1EpAG2Ncs9fpBmp2sRsoqQQgIrtFJCQi7SLyxDTb14rIYRFpFZHXRGRVwravicgp5+trCe2vOed81/mqTs1bmt3Q6Djn+obsBrAxxjUry5Zwx9oVro4GmjUBiEgu8F1gD1APPCoi9VN2ewZ4XlUDwD7gaefYcuA7wJ3EV//6joisSDjuy6p6m/O1aCsltHcPoIoNATXGuCoY8PFeV5T27qgrr5/MFcBOoF1Vz6jqKPAC8NCUfeqBV53HRxK2PwC8oqp9qnoZeAXYPf+w52dyBJAtAmOMcdMXAj5yBJpa3OkGSiYBrATOJzy/4LQlagEecR4/DJSKSEUSx37f6f759yIi0724iDwmIkdF5GhPT08S4c7uVPcABXk5rC0vTsn5jDHmZlSXFnHn+gqaWjtQ1UV//VTdBP4msEtE3gF2AReBiVmO+bKqbgfudr6+Mt1Oqvqsqu5Q1R1VVVUpCTbUFWVj1VLycj17D9wYkyaCDX7O9AxyovPqor92Mr8BLwKrE56vctquU9UOVX1EVW8HvuW09d/oWFWd/DcK/E/iXU2LIhyJ2gQwY0xa2L2tltwccWU0UDIJ4C2gTkTWi0gB8CXgpcQdRKRSRCbP9STwnPP4EPB5EVnh3Pz9PHBIRPJEpNI5Nh9oBI7P/+3M7sq1MTqvDNsIIGNMWigvKeCujZU0tSx+N9CsCUBVx4HHif8yPwn8UFXbRGSfiDzo7HYPEBKRMFADPOUc2wf8HvEk8hawz2krJJ4IWoF3iV8V/PdUvrGZnHIWgbFlII0x6SLY4OfC5Wu0XLiyqK+bVCF8VT0AHJjS9u2Exy8CL85w7HP8+opgsm0QuGOuwaZCODIAYFcAxpi08fmtNRT8dQ5NLR3ctrps0V7Xc3dBw5EoJQW5rCxb4nYoxhgDwLKifHZtrmJ/ayex2OJ1A3kuAYS6otTVlJKTM+2oU2OMcUVjwEfX1WGOfnB50V7TcwnAagAZY9LRfVtqKMrPWdTSEJ5KAL0DI1waHKXObgAbY9JMSWEe926p4eDxTsYnYovymp5KAOGILQJjjElfwYCP3oFRfnGmb1Fez1sJYHIVMOsCMsakoXs2V7O0MG/RuoE8lQBCkQHKivOpKi10OxRjjPmIovxc7q+v4eW2LkbHF74byFMJIByJsqmmlBnqzhljjOuCDT6uXBvjZ+2pKX55I55JAKpKuCtqM4CNMWntro1VLF+ST/MilIj2TALoujpMdGTc+v+NMWmtIC+H3Vtr+bsTEYbHZiuqPD+eSQCTi8BYCQhjTLoLNvgZGBnntdDCLpTomQQQjlgCMMZkhk/cUk5FSQFNC1wi2jMJINQ1QHVpIStKCtwOxRhjbigvN4cvbPdx+GSEwZHxBXsdzySAyRFAxhiTCYINfobHYhx+b+G6gTyRAGIx5VS3JQBjTObYsXYFtcuKFnRSmCcSwPnLQwyPxdhca0NAjTGZISdH2Bvw8ZNQD1eujS3MaySzk4jsFpGQiLSLyBPTbF8rIodFpFVEXhORVQnbviYip5yvryW03yEix5xz/rEs4OwsGwFkjMlEjQEfoxMxXjkRWZDzz5oARCQX+C6wB6gHHhWR+im7PQM8r6oBYB/wtHNsOfAd4E7ii75/x1kbGOB7wG8Cdc7X7nm/mxlMjgCqswRgjMkgt60uY9WKJQvWDZTMFcBOoF1Vz6jqKPAC8NCUfeqBV53HRxK2PwC8oqp9qnoZeAXYLSI+YJmq/kLjqyA/D3xxnu9lRqHIACvLlrC0MKkVMI0xJi2ICMEGP3/f3kvf4GjKz59MAlgJnE94fsFpS9QCPOI8fhgoFZGKGxy70nl8o3MCICKPichRETna03NztTFurS0l2OC/qWONMcZNDzb4uW9LDdHh1N8HSNWfxN8E/kREvg68DlwEUjKHWVWfBZ4F2LFjx00tlvnPP7sxFaEYY8yi2+Jbxn/7yh0Lcu5kEsBFYHXC81VO23Wq2oFzBSAiS4F/oKr9InIRuGfKsa85x6+a0v6hcxpjjFlYyXQBvQXUich6ESkAvgS8lLiDiFSKyOS5ngSecx4fAj4vIiucm7+fBw6paidwVUQ+4Yz++SrwoxS8H2OMMUmaNQGo6jjwOPFf5ieBH6pqm4jsE5EHnd3uAUIiEgZqgKecY/uA3yOeRN4C9jltAL8N/CnQDpwGDqbqTRljjJmdxAfhZIYdO3bo0aNH3Q7DGGMyioj8SlV3TG33xExgY4wxH2UJwBhjPMoSgDHGeJQlAGOM8aiMugksIj3ABzd5eCXQm8JwMp19Hr9mn8WH2efxYdnweaxV1aqpjRmVAOZDRI5Odxfcq+zz+DX7LD7MPo8Py+bPw7qAjDHGoywBGGOMR3kpATzrdgBpxj6PX7PP4sPs8/iwrP08PHMPwBhjzId56QrAGGNMAksAxhjjUZ5IALMtau8VIrJaRI6IyAkRaRORf+V2TOlARHJF5B0RaXY7FreJSJmIvCgi74nISRH5pNsxuUVE/o3zc3JcRP4/ESlyO6ZUy/oEkOSi9l4xDvxfqloPfAL45x7+LBL9K+Klzg38EfCyqt4KNODRz0VEVgL/EtihqtuAXOJroWSVrE8AJLeovSeoaqeqvu08jhL/4Z52LWavEJFVwF7ia1N4mogsBz4D/BmAqo6qar+7UbkqD1giInlAMdDhcjwp54UEkMyi9p4jIuuA24E33Y3EdX8I/N9AzO1A0sB6oAf4vtMl9qciUuJ2UG5Q1YvAM8A5oBO4oqp/525UqeeFBGCmcNZt/t/Av1bVq27H4xYRaQS6VfVXbseSJvKAjwHfU9XbgUHAk/fMnCVsHyKeFP1AiYj8E3ejSj0vJIBZF7X3EhHJJ/7L/y9V9a/djsdlnwYeFJH3iXcNfk5EfuBuSK66AFxQ1cmrwheJJwQvug84q6o9qjoG/DXwKZdjSjkvJIBZF7X3ChER4v27J1X1v7gdj9tU9UlVXaWq64h/X7yqqln3V16yVLULOC8im52me4ETLobkpnPAJ0Sk2Pm5uZcsvCGe53YAC01Vx0VkclH7XOA5VW1zOSy3fBr4CnBMRN512v6tqh5wMSaTXv4F8JfOH0tngH/qcjyuUNU3ReRF4G3io+feIQtLQlgpCGOM8SgvdAEZY4yZhiUAY4zxKEsAxhjjUZYAjDHGoywBGGOMR1kCMMYYj7IEYIwxHvX/AwC9Y2XoB9cNAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7koxeu9q4Gn",
        "colab_type": "text"
      },
      "source": [
        "Load MuonE data and reconstruct tracks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vk18NYQOrfzB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "bf50d8c7-227a-45e9-e82e-4422a4e6f4c6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgK9yW_ErILc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "c8941d75-50e5-4d45-c66d-3e2eb7dea5e5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def readMuonE():\n",
        "\n",
        "   event_file = \"/content/drive/My Drive/MuonE_tracking/hitFile.csv\"\n",
        "\n",
        "   data = pd.read_csv(event_file) \n",
        "   # Preview the first 5 lines of the loaded data \n",
        "   data.head()\n",
        "\n",
        "\n",
        "   X = np.array([[0,0,0]])\n",
        "   print(X)\n",
        "   for i, row in data.iterrows():\n",
        "       \n",
        "  #     X = np.append(X,np.array([[1,1,1]]),axis=0)\n",
        "   #print(X)\n",
        "\n",
        "\n",
        "'''\n",
        " [  0.81616245   0.           2.        ]\n",
        " [  1.22235711   0.           3.        ]\n",
        " [  1.62729628   0.           4.        ]\n",
        " [  2.0309816    0.           5.        ]\n",
        " [  2.43341469   0.           6.        ]\n",
        "\n",
        "\n",
        "\n",
        "   for i, row in data.iterrows():\n",
        "      print(row)\n",
        "      #for j, column in row.iteritems():\n",
        "        #print(column)\n",
        "\n",
        "\n",
        "    \n",
        "    \"\"\" Feed params into randomiser \"\"\"\n",
        "    while True:\n",
        "        radii, dirs, signs, event_size = rand_pars(event_size_min, event_size_max, curve_max, curve_min)\n",
        "        xys = []\n",
        "        X = np.empty([3,1])\n",
        "        x = np.arange(0 + height/num_layers,height + height/num_layers, height/num_layers)\n",
        "        i = 0\n",
        "        for r, d, s in zip(radii, dirs, signs):\n",
        "            y1test = y1(x, r, d, s)\n",
        "            y2test = y2(x, r, d, s)\n",
        "            #print(y1test,y2test, x)\n",
        "            if -2.5 < y1test[0] < 2.5 and not any(np.isnan(y1test)):\n",
        "                X = np.append(X, np.vstack((y1test, np.array([i]*len(y1test)), x )), axis=1)\n",
        "                i += 1\n",
        "            if -2.5 < y2test[0] < 2.5 and not any(np.isnan(y2test)):\n",
        "                X = np.append(X, np.vstack((y2test, np.array([i]*len(y2test)), x )), axis=1)\n",
        "                i += 1\n",
        "        print(\" X \",X)        \n",
        "        X = X[:,1:].T\n",
        "        print(\"X.T \",X)\n",
        "        np.random.shuffle(X)\n",
        "\n",
        "        e = np.array([[i,j] for layer in np.arange(num_layers-1) for i in np.argwhere(X[:,2] == layer+1) for j in np.argwhere(X[:,2] == (layer+2)) if (X[i, 0] - np.tan(max_angle/2) < X[j, 0] < X[i, 0] + np.tan(max_angle/2))]).T.squeeze()\n",
        "        \n",
        "        # This handles when no edges were constructed. In that case, the randomisation is a do-over\n",
        "        try:\n",
        "            y = np.array([int(i[1] == j[1]) for i,j in zip(X[e[0]], X[e[1]])])    \n",
        "            break\n",
        "        except:\n",
        "            pass\n",
        "    if iter is not None and num_samples is not None:\n",
        "        out.update(progress(iter, num_samples))    \n",
        "    \n",
        "    X = np.array([X[:,2], X[:,0]]).T / feature_scale\n",
        "\n",
        "    data = Data(x = torch.from_numpy(X).float(), edge_index = torch.from_numpy(e), y = torch.from_numpy(y), pid = torch.from_numpy(X[:,1]))\n",
        "    return data\n",
        "'''\n",
        "\n",
        "readMuonE()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0]]\n",
            "[[0 0 0]\n",
            " [1 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbX8ha5E5K-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# stop execution\n",
        "\n",
        "print('Stopped here')\n",
        "exit()\n",
        "quit()\n",
        "\n",
        "value = input(\"Stopping here:\\n\")\n",
        " \n",
        "print(f'You entered {value}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rWAb46MR76f",
        "colab_type": "text"
      },
      "source": [
        "## Moving to \"Real\" Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0PH4JO3R72K",
        "colab_type": "text"
      },
      "source": [
        "### Real Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mISgZbDfR72N",
        "colab_type": "text"
      },
      "source": [
        "The data from a (good) simulation is much more busy. Let's load a TrackML event and visualise it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9oR4BwnVi7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/LAL/trackml-library"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRwpiDR2R72Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import trackml.dataset\n",
        "\n",
        "def calc_dphi(phi1, phi2):\n",
        "    \"\"\"Computes phi2-phi1 given in range [-pi,pi]\"\"\"\n",
        "    dphi = phi2 - phi1\n",
        "    dphi[dphi > np.pi] -= 2*np.pi\n",
        "    dphi[dphi < -np.pi] += 2*np.pi\n",
        "    return dphi\n",
        "\n",
        "def select_hits(hits, truth, particles, pt_min=0):\n",
        "    # Barrel volume and layer ids\n",
        "    vlids = [(8,2), (8,4), (8,6), (8,8),\n",
        "             (13,2), (13,4), (13,6), (13,8),\n",
        "             (17,2), (17,4)]\n",
        "    n_det_layers = len(vlids)\n",
        "    # Select barrel layers and assign convenient layer number [0-9]\n",
        "    vlid_groups = hits.groupby(['volume_id', 'layer_id'])\n",
        "    hits = pd.concat([vlid_groups.get_group(vlids[i]).assign(layer=i)\n",
        "                      for i in range(n_det_layers)])\n",
        "    # Calculate particle transverse momentum\n",
        "    pt = np.sqrt(particles.px**2 + particles.py**2)\n",
        "    # True particle selection.\n",
        "    # Applies pt cut, removes all noise hits.\n",
        "    particles = particles[pt > pt_min]\n",
        "    truth = (truth[['hit_id', 'particle_id']]\n",
        "             .merge(particles[['particle_id']], on='particle_id'))\n",
        "    # Calculate derived hits variables\n",
        "    r = np.sqrt(hits.x**2 + hits.y**2)\n",
        "    phi = np.arctan2(hits.y, hits.x)\n",
        "    # Select the data columns we need\n",
        "    hits = (hits[['hit_id', 'z', 'layer']]\n",
        "            .assign(r=r, phi=phi)\n",
        "            .merge(truth[['hit_id', 'particle_id']], on='hit_id'))\n",
        "    # Remove duplicate hits\n",
        "    hits = hits.loc[\n",
        "        hits.groupby(['particle_id', 'layer'], as_index=False).r.idxmin()\n",
        "    ]\n",
        "    return hits    \n",
        "\n",
        "def select_segments(hits1, hits2, phi_slope_max, z0_max):\n",
        "    \"\"\"\n",
        "    Construct a list of selected segments from the pairings\n",
        "    between hits1 and hits2, filtered with the specified\n",
        "    phi slope and z0 criteria.\n",
        "    Returns: pd DataFrame of (index_1, index_2), corresponding to the\n",
        "    DataFrame hit label-indices in hits1 and hits2, respectively.\n",
        "    \"\"\"\n",
        "    # Start with all possible pairs of hits\n",
        "    keys = ['evtid', 'r', 'phi', 'z']\n",
        "    hit_pairs = hits1[keys].reset_index().merge(\n",
        "        hits2[keys].reset_index(), on='evtid', suffixes=('_1', '_2'))\n",
        "    # Compute line through the points\n",
        "    dphi = calc_dphi(hit_pairs.phi_1, hit_pairs.phi_2)\n",
        "    dz = hit_pairs.z_2 - hit_pairs.z_1\n",
        "    dr = hit_pairs.r_2 - hit_pairs.r_1\n",
        "    phi_slope = dphi / dr\n",
        "    z0 = hit_pairs.z_1 - hit_pairs.r_1 * dz / dr\n",
        "    # Filter segments according to criteria\n",
        "    good_seg_mask = (phi_slope.abs() < phi_slope_max) & (z0.abs() < z0_max)\n",
        "    return hit_pairs[['index_1', 'index_2']][good_seg_mask]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeoojN9-R72Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Geometric and physics cuts\n",
        "pt_min = 1\n",
        "phi_slope_max = .001\n",
        "z0_max = 200\n",
        "\n",
        "# Graph features and scale\n",
        "feature_names = ['r', 'phi', 'z']\n",
        "feature_scale = np.array([1000., np.pi, 1000.])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcgJUDa2R72l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_event(event_file, pt_min, phi_slope_max, z0_max, feature_names, feature_scale):\n",
        "    hits, particles, truth = trackml.dataset.load_event(\n",
        "        event_file, parts=['hits', 'particles', 'truth'])\n",
        "    hits = select_hits(hits, truth, particles, pt_min=pt_min).assign(evtid=int(event_file[-9:]))\n",
        "    \n",
        "    # Define adjacent layers\n",
        "    n_det_layers = 10\n",
        "    l = np.arange(n_det_layers)\n",
        "    layer_pairs = np.stack([l[:-1], l[1:]], axis=1)\n",
        "    \n",
        "    layer_groups = hits.groupby('layer')\n",
        "    segments = []\n",
        "    for (layer1, layer2) in layer_pairs:\n",
        "        # Find and join all hit pairs\n",
        "        try:\n",
        "            hits1 = layer_groups.get_group(layer1)\n",
        "            hits2 = layer_groups.get_group(layer2)\n",
        "        # If an event has no hits on a layer, we get a KeyError.\n",
        "        # In that case we just skip to the next layer pair\n",
        "        except KeyError as e:\n",
        "            logging.info('skipping empty layer: %s' % e)\n",
        "            continue\n",
        "        # Construct the segments\n",
        "        segments.append(select_segments(hits1, hits2, phi_slope_max, z0_max))\n",
        "        # Combine segments from all layer pairs\n",
        "    segments = pd.concat(segments)\n",
        "    \n",
        "    X = (hits[feature_names].values / feature_scale).astype(np.float32)\n",
        "    n_edges = len(segments)\n",
        "    n_hits = len(hits)\n",
        "    \n",
        "    pid1 = hits.particle_id.loc[segments.index_1].values\n",
        "    pid2 = hits.particle_id.loc[segments.index_2].values\n",
        "    y = np.zeros(n_edges, dtype=np.float32)\n",
        "    y[:] = (pid1 == pid2)\n",
        "    \n",
        "    hit_idx = pd.Series(np.arange(n_hits), index=hits.index)\n",
        "    seg_start = hit_idx.loc[segments.index_1].values\n",
        "    seg_end = hit_idx.loc[segments.index_2].values\n",
        "    \n",
        "    e = np.vstack([seg_start, seg_end])\n",
        "    \n",
        "    return X, e, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4Z6oRqGx8AR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyIA-E7YR73B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "event_file = \"/content/drive/My Drive/GNN Tutorial/train_100_events/event000001000\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N58kXJW9R73L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, e, y = build_event(event_file, pt_min, phi_slope_max, z0_max, feature_names, feature_scale)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fMEFd7hR73S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_trackml_graph(X, e, feature_scale):\n",
        "    X = X * feature_scale\n",
        "    x = X[:,0] * np.cos(X[:,1])\n",
        "    y = X[:,0] * np.sin(X[:,1])\n",
        "    plt.figure(figsize=(15,15))\n",
        "    plt.plot([x[e[0,:]], x[e[1,:]]], [y[e[0,:]], y[e[1,:]]], c='b')\n",
        "    plt.scatter(x, y, c='k')\n",
        "#     plt.ylim(-10,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W6-avtMR73d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "plot_trackml_graph(X, e, feature_scale)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Jr5e14jR76j",
        "colab_type": "text"
      },
      "source": [
        "We need to create a dataset from the TrackML data, which may take a little while..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYpt8aFrR76l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_event(event_file, pt_min, phi_slope_max, z0_max, feature_names, feature_scale, iter=None, num_samples=None):\n",
        "    print(\"Preparing\",event_file)\n",
        "    X, e, y = build_event(event_file, pt_min, phi_slope_max, z0_max, feature_names, feature_scale)\n",
        "    data = Data(x = torch.from_numpy(X).float(), edge_index = torch.from_numpy(e), y = torch.from_numpy(y))\n",
        "\n",
        "    if iter is not None and num_samples is not None:\n",
        "        out.update(progress(iter, num_samples))    \n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yol5mXgrR764",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dir = \"/content/drive/My Drive/GNN Tutorial/train_100_events/\"\n",
        "all_events = os.listdir(input_dir)\n",
        "all_events = [input_dir + event[:14] for event in all_events]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-6GUcQyR76_",
        "colab_type": "text"
      },
      "source": [
        "Build the graphs. This will take less than 5 minutes. I hope."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cInnb2NhR77B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_size, test_size = 95, 5\n",
        "out = display(progress(0, train_size), display_id=True)\n",
        "train_dataset = [prepare_event(event_file, pt_min, phi_slope_max, z0_max, feature_names, feature_scale, iter, train_size) for (event_file, iter) in zip(all_events[:train_size], range(train_size))]\n",
        "out = display(progress(0, test_size), display_id=True)\n",
        "test_dataset = [prepare_event(event_file, pt_min, phi_slope_max, z0_max, feature_names, feature_scale, iter, test_size) for (event_file, iter) in zip(all_events[-test_size:], range(test_size))]\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4_37OYMR77H",
        "colab_type": "text"
      },
      "source": [
        "### MPGNN on TrackML Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_bZC1p9R77J",
        "colab_type": "text"
      },
      "source": [
        "To get the weight, we should again look at the fake:true ratio:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTPEfxTnR77R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Fake / True = \", (len(train_dataset[0].y) - train_dataset[0].y.sum().item()) / train_dataset[0].y.sum().item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUHuO7OsR776",
        "colab_type": "text"
      },
      "source": [
        "It's actually pretty close! This is because we put a momentum cut that removed many edges that may have involved high curvature. Thus we have few fake edges. Try making the graphs with a lower pT cut and we will return many fake edges. We can now train the MPNN on the high-pT events."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEl3Pa62R778",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_loss_v = []\n",
        "t_acc_v = []\n",
        "v_loss_v = []\n",
        "v_acc_v = []\n",
        "ep = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAVq7hbTR78B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight = 2\n",
        "m_configs = {\"input_dim\": 3, \"hidden_node_dim\": 64, \"hidden_edge_dim\": 64, \"in_layers\": 2, \"node_layers\": 4, \"edge_layers\": 4, \"n_graph_iters\": 8, \"layer_norm\": True}\n",
        "model = MPNN_Network(**m_configs).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3, amsgrad=True)\n",
        "for epoch in range(200):\n",
        "    ep += 1  \n",
        "    model.train()\n",
        "    acc, total_loss = train(model, train_loader, optimizer)\n",
        "    t_loss_v.append(total_loss)\n",
        "    t_acc_v.append(acc)\n",
        "\n",
        "    model.eval()\n",
        "    acc, total_loss = evaluate(model, test_loader)\n",
        "    v_loss_v.append(total_loss)\n",
        "    v_acc_v.append(acc)\n",
        "\n",
        "    print('Epoch: {}, Accuracy: {:.4f}'.format(ep, acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJu3J1wiR78H",
        "colab_type": "text"
      },
      "source": [
        "The same configuration (16 dims, 1 iteration, 2 hidden layers in each network) that gave us 90% on toy data actually works pretty well still (after many epochs). That's because the pT cut gives us quite clean graphs. However, putting up the graph iterations doesn't help with a small number of hidden dimensions. Essentially, the message passing is washing out the information contained in only 16 dimensions. We should increase them to harness the power of the message passing. With 64 dimensions, and 8 iterations, I get around 96% accuracy after 50 epochs. For real tracking data, this should be quite impressive. And remember we can tweak the efficiency and purity by altering the weight."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bma6hUtqR78J",
        "colab_type": "text"
      },
      "source": [
        "Visualising the model..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5xqmPPhR78K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_trackml_classified(event, preds, feature_scale, cut=0.5):\n",
        "    X = event.x.numpy() * feature_scale\n",
        "    x = X[:,0] * np.cos(X[:,1])\n",
        "    y = X[:,0] * np.sin(X[:,1])\n",
        "    \n",
        "    plt.figure(figsize=(20,20))\n",
        "    edges = event.edge_index.numpy()\n",
        "    labels = event.y\n",
        "    plt.scatter(x, y, c='k')\n",
        "    \n",
        "    preds = preds.detach().numpy()\n",
        "    \n",
        "    out = display(progress(0, test_size), display_id=True)\n",
        "\n",
        "    for j in range(len(labels)):\n",
        "        if j%1000 == 0: \n",
        "            out.update(progress(j, len(labels))) \n",
        "        # False negatives\n",
        "        if preds[j] < cut and labels[j].item() > cut:\n",
        "            plt.plot([x[edges[0,j]], x[edges[1,j]]],\n",
        "                     [y[edges[0,j]], y[edges[1,j]]],\n",
        "                     '--', c='b')\n",
        "\n",
        "        # False positives\n",
        "        if preds[j] > cut and labels[j].item() < cut:\n",
        "            plt.plot([x[edges[0,j]], x[edges[1,j]]],\n",
        "                     [y[edges[0,j]], y[edges[1,j]]],\n",
        "                     '-', c='r', alpha=preds[j])\n",
        "\n",
        "        # True positives\n",
        "        if preds[j] > cut and labels[j].item() > cut:\n",
        "            plt.plot([x[edges[0,j]], x[edges[1,j]]],\n",
        "                     [y[edges[0,j]], y[edges[1,j]]],\n",
        "                     '-', c='k', alpha=preds[j])\n",
        "                \n",
        "        # True negatives\n",
        "        if preds[j] < cut and labels[j].item() < cut:\n",
        "            plt.plot([x[edges[0,j]], x[edges[1,j]]],\n",
        "                     [y[edges[0,j]], y[edges[1,j]]],\n",
        "                     '-', c='k', alpha=preds[j])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zGk2CAuR78P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = test_loader.dataset[0].to(device)\n",
        "preds = torch.sigmoid(model(data)).to('cpu')\n",
        "plot_trackml_classified(data.to('cpu'), preds, feature_scale, cut = 0.6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbT3RpylR78b",
        "colab_type": "text"
      },
      "source": [
        "### The Attention mechanism in a GNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdIkjHlZR78d",
        "colab_type": "text"
      },
      "source": [
        "The MPNN is working pretty well. Let's try and do better with a more sophisticated convolution function. Essentially, we run the classification network in every iteration, and use the score to weight how the node features are aggregated. In this way, the graph should learn what \"attention\" it gives to each edge connected to each node. We thus call it an Attention GNN (AGNN). Its model is given by"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7WNC_BdR78x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EdgeNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    A module which computes weights for edges of the graph.\n",
        "    For each edge, it selects the associated nodes' features\n",
        "    and applies some fully-connected network layers with a final\n",
        "    sigmoid activation.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim=8, hidden_activation='Tanh',\n",
        "                 layer_norm=True):\n",
        "        super(EdgeNetwork, self).__init__()\n",
        "        self.network = make_mlp(input_dim*2,\n",
        "                                [hidden_dim, hidden_dim, hidden_dim, 1],\n",
        "                                hidden_activation=hidden_activation,\n",
        "                                output_activation=None,\n",
        "                                layer_norm=layer_norm)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # Select the features of the associated nodes\n",
        "        start, end = edge_index\n",
        "        x1, x2 = x[start], x[end]\n",
        "        edge_inputs = torch.cat([x[start], x[end]], dim=1)\n",
        "        return self.network(edge_inputs).squeeze(-1)\n",
        "\n",
        "class NodeNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    A module which computes new node features on the graph.\n",
        "    For each node, it aggregates the neighbor node features\n",
        "    (separately on the input and output side), and combines\n",
        "    them with the node's previous features in a fully-connected\n",
        "    network to compute the new features.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim=8, hidden_activation='Tanh',\n",
        "                 layer_norm=True):\n",
        "        super(NodeNetwork, self).__init__()\n",
        "        self.network = make_mlp(input_dim*3, [output_dim]*4,\n",
        "                                hidden_activation=hidden_activation,\n",
        "                                output_activation=hidden_activation,\n",
        "                                layer_norm=layer_norm)\n",
        "\n",
        "    def forward(self, x, e, edge_index):\n",
        "        start, end = edge_index\n",
        "        # Aggregate edge-weighted incoming/outgoing features\n",
        "        mi = scatter_add(e[:, None] * x[start], end, dim=0, dim_size=x.shape[0])\n",
        "        mo = scatter_add(e[:, None] * x[end], start, dim=0, dim_size=x.shape[0])\n",
        "        node_inputs = torch.cat([mi, mo, x], dim=1)\n",
        "        return self.network(node_inputs)\n",
        "\n",
        "class AGNN_Network(nn.Module):\n",
        "    \"\"\"\n",
        "    Segment classification graph neural network model.\n",
        "    Consists of an input network, an edge network, and a node network.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=3, hidden_dim=8, n_graph_iters=3,\n",
        "                 hidden_activation='Tanh', layer_norm=True):\n",
        "        super(AGNN_Network, self).__init__()\n",
        "        self.n_graph_iters = n_graph_iters\n",
        "        # Setup the input network\n",
        "        self.input_network = make_mlp(input_dim, [hidden_dim],\n",
        "                                      output_activation=hidden_activation,\n",
        "                                      layer_norm=layer_norm)\n",
        "        # Setup the edge network\n",
        "        self.edge_network = EdgeNetwork(hidden_dim, hidden_dim,\n",
        "                                        hidden_activation, layer_norm=layer_norm)\n",
        "        # Setup the node layers\n",
        "        self.node_network = NodeNetwork(hidden_dim, hidden_dim,\n",
        "                                        hidden_activation, layer_norm=layer_norm)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Apply forward pass of the model\"\"\"\n",
        "        # Apply input network to get hidden representation\n",
        "        x = self.input_network(inputs.x)\n",
        "        # Shortcut connect the inputs onto the hidden representation\n",
        "        # Loop over iterations of edge and node networks\n",
        "        for i in range(self.n_graph_iters):\n",
        "            x0 = x\n",
        "            # Apply edge network\n",
        "            e = torch.sigmoid(self.edge_network(x, inputs.edge_index))\n",
        "            # Apply node network\n",
        "            x = self.node_network(x, e, inputs.edge_index)\n",
        "            # Add the residual  between iterations\n",
        "            x = x + x0\n",
        "        # Apply final edge network\n",
        "        return self.edge_network(x, inputs.edge_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSu9BbPnR787",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_loss_v = []\n",
        "t_acc_v = []\n",
        "v_loss_v = []\n",
        "v_acc_v = []\n",
        "ep = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdhHCqD-R79A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight = 2\n",
        "m_configs = {\"input_dim\": 3, \"hidden_dim\": 64, \"n_graph_iters\": 8, \"layer_norm\": True}\n",
        "model = AGNN_Network(**m_configs).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3, amsgrad=True)\n",
        "for epoch in range(200):\n",
        "    ep += 1  \n",
        "    model.train()\n",
        "    acc, total_loss = train(model, train_loader, optimizer)\n",
        "    t_loss_v.append(total_loss)\n",
        "    t_acc_v.append(acc)\n",
        "\n",
        "    model.eval()\n",
        "    acc, total_loss = evaluate(model, test_loader)\n",
        "    v_loss_v.append(total_loss)\n",
        "    v_acc_v.append(acc)\n",
        "\n",
        "    print('Epoch: {}, Accuracy: {:.4f}'.format(ep, acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_K1hQ5WFN9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axs = plt.subplots(1, 2, constrained_layout=True, figsize=(10, 5))\n",
        "axs[0].plot(np.arange(len(t_loss_v)), t_loss_v, np.arange(len(t_acc_v)), t_acc_v)\n",
        "axs[0].set_title(\"Training loss and accuracy\")\n",
        "axs[0].set_yscale(\"log\")\n",
        "axs[1].plot(np.arange(len(v_loss_v)), v_loss_v, np.arange(len(v_acc_v)), v_acc_v)\n",
        "axs[1].set_title(\"Validation loss and accuracy\")\n",
        "axs[1].set_yscale(\"log\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4XClSGZR79S",
        "colab_type": "text"
      },
      "source": [
        "### Did the AGNN help?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QV8t9lnF10A",
        "colab_type": "text"
      },
      "source": [
        "The AGNN is slower to train and infer than the MPNN, but is potentially more accurate. After about 40 epochs with the AGNN, I get over 97% accuracy. With some hyperparameter optimisation, I can get better performance with the AGNN, but if you can make the MPNN out-do the AGNN, let me know!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caBTYORcMXXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = test_loader.dataset[0].to(device)\n",
        "preds = torch.sigmoid(model(data)).to('cpu')\n",
        "plot_trackml_classified(data.to('cpu'), preds, feature_scale, cut = 0.6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6o5QV1b3YwaY",
        "colab_type": "text"
      },
      "source": [
        "# **That's all**"
      ]
    }
  ]
}